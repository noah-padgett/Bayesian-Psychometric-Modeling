[["model-evaluation.html", "Chapter 10 Model Evaluation", " Chapter 10 Model Evaluation The goal of evaluating the model is to determine if the inferences suggested by the model are reasonable based ones content area knowledge. The text and (BDA3) say that this evaluation of reasonableness is really a form of prior, that is Gelman et al. (2013) argued tha when analysts deem that the posterior distribution and inferences are unreasonable, what they are really expressing is that there is additional information available that was not included in the analysis. Having some knowledge about what the posterior distribution should look like can be extremely helpful in developing how the model takes shape (e.g., setting reasonable boundaries on parameters). Three aspects/questions we aim to address as part of the logic of model checking: What is going on in our data, possibly in relation to the model? What the model has to say about what should be going on? How what is going on in the data compared to what the model says should be going on? Model evaluation is accomplished through Residual analysis, Posterior predictive distributions, and Model comparisons. "],["residual-analysis.html", "10.1 Residual Analysis", " 10.1 Residual Analysis A residual is a key component of analysis in many statistical procedures. Here, we describe how this important feature can be used in Bayesian psychometric analysis. At a fundamental level, a residual is one component in the data-model relationship, that is \\[DATA = MODEL + RESIDUALS.\\] This view highlights the three major pieces we are going to use in the model evaluation process. We can rewrite the above to focus on the residuals as well. In factor analysis (CFA in particular), we tend to be interested in the residuals of the covariances among variables because our factor model is a hypothesis for how the observed variables are interrelated. In the observed data, we capture this relationship with the observed sample covariance matrix \\(\\mathbf{S}\\), which for variable \\(j\\) and \\(k\\) is usually computed as \\[s_{jk} = \\frac{\\sum_{\\forall i (x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)}}{n-1}.\\] And, the whole covariance matrix is \\(\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\prime}\\mathbf{X}\\) where \\(\\mathbf{X}\\) is in centered form. In CFA, we can get the model implied covariance matrix by using the estimated parameters to get \\(\\Sigma(\\theta)\\), that is \\[\\Sigma(\\theta) = \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi.\\] Then, to get the residual matrix we simple find the difference scores (\\(\\mathbf{E}\\)) between these matrices \\[\\mathbf{E} = \\mathbf{S} - \\Sigma(\\theta).\\] Although, we would probably want to rescale \\(\\mathbf{S}\\) and \\(\\Sigma(\\theta)\\) to be correlation matrices because the scale of the covariances and variances are likely not of primary interest. This means we can use the residuals correlations instead. Generating the residual correlations is accomplished using the posterior predictive distribution. "],["posterior-predictive-distributions.html", "10.2 Posterior Predictive Distributions", " 10.2 Posterior Predictive Distributions The posterior predictive distribution is used heavily in model evaluation. (look at Bayes notes) Basically, the posterior predictive distribution is the what values of the observed data (\\(Y\\)) are mostly likely given the posterior distribution. 10.2.1 Example of posterior predictive distribution of correlations In this example, we use the correlations of the observed variable as the function of interest. # model code jags.model.cfa &lt;- function(){ ######################################## # Specify the factor analysis measurement # model for the observables ######################################## for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, parameters.to.save = param_save, n.iter=15000, n.burnin = 5000, n.chains = 1, # for simplicity n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpoJYxxc/model8a8836155848.txt&quot;, fit using jags, ## 1 chains, each with 15000 iterations (first 5000 discarded) ## n.sims = 10000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.727 0.045 0.641 0.697 0.726 0.757 0.820 ## lambda[3] 0.420 0.037 0.348 0.394 0.419 0.444 0.495 ## lambda[4] 1.049 0.066 0.925 1.003 1.047 1.093 1.182 ## lambda[5] 0.983 0.059 0.871 0.943 0.982 1.022 1.104 ## phi 0.436 0.043 0.357 0.406 0.435 0.464 0.525 ## psi[1] 0.373 0.028 0.321 0.353 0.372 0.391 0.431 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.192 0.212 ## psi[3] 0.180 0.012 0.157 0.171 0.179 0.187 0.204 ## psi[4] 0.378 0.030 0.322 0.357 0.377 0.397 0.441 ## psi[5] 0.266 0.022 0.226 0.251 0.265 0.280 0.311 ## tau[1] 3.333 0.041 3.253 3.306 3.333 3.361 3.411 ## tau[2] 3.898 0.029 3.841 3.878 3.897 3.917 3.955 ## tau[3] 4.596 0.023 4.552 4.580 4.596 4.612 4.640 ## tau[4] 3.033 0.041 2.953 3.006 3.034 3.061 3.114 ## tau[5] 3.712 0.037 3.638 3.687 3.713 3.738 3.785 ## deviance 3381.534 42.306 3300.328 3352.318 3380.587 3409.720 3465.205 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 894.9 and DIC = 4276.4 ## DIC is an estimate of expected predictive error (lower deviance is better). plot(fit) # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # compute model implied covariance/correlations # for each iterations out.mat &lt;- matrix(ncol=10,nrow=nrow(plot.data)) colnames(out.mat) &lt;- c(&quot;r12&quot;, &quot;r13&quot;, &quot;r14&quot;, &quot;r15&quot;, &quot;r23&quot;, &quot;r24&quot;, &quot;r25&quot;, &quot;r34&quot;,&quot;r35&quot;, &quot;r45&quot;) plot.data1 &lt;- cbind(plot.data,out.mat) # compute the model implied correlations for each iterations i &lt;- 1 for(i in 1:nrow(plot.data1)){ x &lt;- plot.data1[i,] x &lt;- unlist(x) lambda &lt;- matrix(x[4:8], ncol=1) phi &lt;- matrix(x[9], ncol=1) psi &lt;- diag(x[10:14], ncol=5, nrow=5) micov &lt;- lambda%*%phi%*%t(lambda)+psi D &lt;- diag(sqrt(diag(micov)), ncol=5, nrow=5) Dinv &lt;- solve(D) micor &lt;- Dinv%*%micov%*%Dinv outr &lt;- micor[lower.tri(micor)] # combine plot.data1[i,20:29] &lt;- outr } obs.mat &lt;- matrix(cor(dat)[lower.tri(cor(dat))],byrow=T, ncol=10,nrow=nrow(plot.data)) colnames(obs.mat) &lt;- c(&quot;ObsR12&quot;, &quot;ObsR13&quot;, &quot;ObsR14&quot;, &quot;ObsR15&quot;, &quot;ObsR23&quot;, &quot;ObsR24&quot;, &quot;ObsR25&quot;, &quot;ObsR34&quot;,&quot;ObsR35&quot;, &quot;ObsR45&quot;) plot.data1 &lt;- cbind(plot.data1,obs.mat) theme_set(theme_classic()) t1 &lt;- grid::textGrob(&#39;PI&#39;) t2 &lt;- grid::textGrob(&#39;AD&#39;) t3 &lt;- grid::textGrob(&#39;IGC&#39;) t4 &lt;- grid::textGrob(&#39;FI&#39;) t5 &lt;- grid::textGrob(&#39;FC&#39;) p12 &lt;- ggplot(plot.data1) + geom_density(aes(x=r12))+ geom_vline(aes(xintercept = ObsR12), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p13 &lt;- ggplot(plot.data1) + geom_density(aes(x=r13))+ geom_vline(aes(xintercept = ObsR13), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p14 &lt;- ggplot(plot.data1) + geom_density(aes(x=r14))+ geom_vline(aes(xintercept = ObsR14), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p15 &lt;- ggplot(plot.data1) + geom_density(aes(x=r15))+ geom_vline(aes(xintercept = ObsR15), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p23 &lt;- ggplot(plot.data1) + geom_density(aes(x=r23))+ geom_vline(aes(xintercept = ObsR23), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p24 &lt;- ggplot(plot.data1) + geom_density(aes(x=r24))+ geom_vline(aes(xintercept = ObsR24), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p25 &lt;- ggplot(plot.data1) + geom_density(aes(x=r25))+ geom_vline(aes(xintercept = ObsR25), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p34 &lt;- ggplot(plot.data1) + geom_density(aes(x=r34))+ geom_vline(aes(xintercept = ObsR34), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p35 &lt;- ggplot(plot.data1) + geom_density(aes(x=r35))+ geom_vline(aes(xintercept = ObsR35), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p45 &lt;- ggplot(plot.data1) + geom_density(aes(x=r45))+ geom_vline(aes(xintercept = ObsR45), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) layout &lt;- &#39; A#### BC### DEF## GHIJ# KLMNO &#39; wrap_plots(A=t1,C=t2,F=t3,J=t4,O=t5, B=p12,D=p13,G=p14,K=p15, E=p23,H=p24,L=p25, I=p34,M=p35, N=p45,design = layout) 10.2.2 PPD SRMR # model code jags.model.cfa &lt;- function(){ ######################################## # Specify the factor analysis measurement # model for the observables ######################################## for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # Posterior Predictive Distribution of x # needed for SRMR # set mean to 0 # x.ppd[i,j] ~ dnorm(0, inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;)# &quot;Sigma&quot;, # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, #inits=start_values, parameters.to.save = param_save, n.iter=10000, n.burnin = 5000, n.chains = 1, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpeKu3Vd/model5ca423966bf4.txt&quot;, fit using jags, ## 1 chains, each with 10000 iterations (first 5000 discarded) ## n.sims = 5000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.727 0.045 0.641 0.696 0.726 0.757 0.818 ## lambda[3] 0.419 0.037 0.348 0.393 0.418 0.444 0.492 ## lambda[4] 1.048 0.066 0.923 1.004 1.045 1.091 1.182 ## lambda[5] 0.981 0.060 0.872 0.940 0.979 1.020 1.103 ## phi 0.437 0.043 0.357 0.408 0.436 0.465 0.525 ## psi[1] 0.374 0.028 0.321 0.354 0.373 0.392 0.431 ## psi[2] 0.182 0.014 0.156 0.172 0.182 0.191 0.211 ## psi[3] 0.180 0.012 0.157 0.171 0.179 0.187 0.204 ## psi[4] 0.377 0.030 0.322 0.357 0.376 0.396 0.439 ## psi[5] 0.266 0.022 0.226 0.251 0.265 0.281 0.312 ## tau[1] 3.332 0.040 3.253 3.305 3.332 3.360 3.410 ## tau[2] 3.897 0.029 3.842 3.878 3.897 3.917 3.954 ## tau[3] 4.595 0.023 4.551 4.579 4.595 4.611 4.641 ## tau[4] 3.033 0.041 2.953 3.006 3.033 3.062 3.113 ## tau[5] 3.711 0.036 3.638 3.687 3.712 3.736 3.781 ## deviance 3380.191 41.348 3300.402 3351.851 3379.587 3407.935 3462.773 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 854.8 and DIC = 4235.0 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit)[[1]] # function for estimating SRMR SRMR.function &lt;- function(data.cov.matrix, mod.imp.cov.matrix){ J=nrow(data.cov.matrix) temp &lt;- matrix(NA, nrow=J, ncol=J) for(j in 1:J){ for(jprime in 1:j){ temp[j, jprime] &lt;- ((data.cov.matrix[j, jprime] - mod.imp.cov.matrix[j, jprime])/(data.cov.matrix[j, j] * data.cov.matrix[jprime, jprime]))^2 } } SRMR &lt;- sqrt((2*sum(temp,na.rm=TRUE))/(J*(J+1))) SRMR } # set up the parameters for # (1) model implied covariance # (2) PPD of x/covariance iter &lt;- nrow(jags.mcmc) srmr.realized &lt;- rep(NA, iter) srmr.ppd &lt;- rep(NA, iter) srmr.rpv &lt;- rep(NA, iter) jags.mcmc &lt;- cbind(jags.mcmc, srmr.realized, srmr.ppd, srmr.rpv) N &lt;- 500; J &lt;- 5; M &lt;- 1 cov.x &lt;- cov(mydata$x) i &lt;- 1 for(i in 1:iter){ # set up parameters x &lt;- jags.mcmc[i,] lambda &lt;- matrix(x[2:6], ncol=M, nrow=J) phi &lt;- matrix(x[7], ncol=M, nrow=M) psi &lt;- diag(x[8:12], ncol=J, nrow=J) # estimate model implied covariance matrix cov.imp &lt;- lambda%*%phi%*%t(lambda) + psi # get posterior predicted observed x x.ppd &lt;- mvtnorm::rmvnorm(N, mean=rep(0, J), sigma=cov.imp) # compute posterior predictied covariance matrix cov.ppd &lt;- cov(x.ppd) # estimate SRMR values jags.mcmc[i,18] &lt;- SRMR.function(cov.x, cov.imp)# srmr realized jags.mcmc[i,19] &lt;- SRMR.function(cov.ppd, cov.imp)# srmr ppd # posterior predicted p-value of realized SRMR being &lt;= 0.08. jags.mcmc[i,20] &lt;- ifelse(jags.mcmc[i,18]&lt;=0.08, 1, 0) } plot.dat &lt;- as.data.frame(jags.mcmc) p1 &lt;- ggplot(plot.dat, aes(x=srmr.realized, y=srmr.ppd))+ geom_point()+ geom_abline(slope=1, intercept = 0)+ lims(x=c(0,1),y=c(0,1))+ labs(x=&quot;Realized SRMR&quot;,y=&quot;Posterior Predicted SRMR&quot;) + theme_bw()+theme(panel.grid = element_blank()) p2 &lt;- ggplot(plot.dat, aes(x=srmr.realized))+ geom_density()+ lims(x=c(0,1))+ labs(x=&quot;Realized SRMR&quot;, y=NULL) + annotate(&quot;text&quot;, x = 0.75, y = 3, label = paste0(&quot;Pr(SRMR &lt;= 0.08)= &quot;, round(mean(plot.dat$srmr.rpv), 2))) + theme_bw()+theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) p1 + p2 "],["model-comparison.html", "10.3 Model Comparison", " 10.3 Model Comparison Bayes Factors (BF) Conditional predictive ordinate (CPO) Information criteria Entropy "]]
