<<<<<<< Updated upstream
[["index.html", "Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy Chapter 1 Getting Started/Overview Chapter", " Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy R. Noah Padgett 2021-04-05 Chapter 1 Getting Started/Overview Chapter This online book is meant to serve a computation reference for the text Bayesian Psychometric Modeling by Roy Levy and Robert Mislevy. I hope that having a detailed computation guide using Stan is of interest to someone. To reference the book, I will frequently use BPM as a shorthand for the title of the text. Throughout this book, I have incorporated elaborations on the code to help set up the examples and provide more details when possible. The authors provided an accompanying website, where the examples are shown using WinBUGS and Netica. The later is used in Chapter 14 for estimation of Bayesian networks. I wanted to deeply dive into this text so rerunning all the examples in a different language provides an excellent excuse to do so. Also, since I can go into more detail in this format than they could, I have not restricted myself by cutting short of the analysis. Meaning that I have done my best to fully (or sufficient for the example) analyze the posteriors to see what issues may pop up and how I resolved them. "],["software.html", "1.1 Software", " 1.1 Software We have used R to call Stan. So we have relied on rstan heavily. NEED TO UPDATE - NEEDS A LOT MORE INFORMATION "],["overview-of-assessment-and-psychometric-modeling.html", "1.2 Overview of Assessment and Psychometric Modeling", " 1.2 Overview of Assessment and Psychometric Modeling Chapter 1 of BPM provided an excellent overview of topics related to psychometric modeling. I wanted to highlight some particularly important pieces to keep in mind while modeling. [We] view assessment as an instance of evidentiary reasoning. (p. 3) This idea forms the basis for this text. Providing evidence in support of inferences, claims, decisions, etc. is a major stance of the probabilistic modeling used by Levy and MisLevy. Observed data provide some evidence, but not all data is evidence. This gets to their point that data are grounds to help provide evidence, but they also recognize that evidence does not depend solely on data. What these data represents is a major factor in helping to decide whether evidence has been gathered. Inferences depends on the data and the claim being made. They used excellent examples of the relationships between data, claims, and potential alternative explanations. These examples on pages 5-9 should be read. In summary, the idea is to use our claim to make predictions about what data we should observe. However, we have to use data to make inferences about our claims. The reversal is in line with the relationship between deductive reasoning and inductive reasoning. A models is a simplified version of a real world scenario, wherein [relevant] features of the problem at hand are represented, while [less relevant features] are suppressed. They took that stance that they acknowledge that any model they come up with is wrong, but they aim to develop a useful model to help solve a problem at hand. The models they are developing help describe the world according to the analyst. The according to analyst is important since this implies that the analyst is in control of how the model represents the world. The inclusion of the analyst as an active participant in the model building process instead of a passive observer is a realistic representation of the problem of assessment and modeling building. They highlighed 3 goals of modeling (p. 11) represent the relationships among the relevant entities; provide the machinery for making inferences about what is unknown based on what is known; and offers machanisms for effectively communicating results to take actions in the real world. Probability is interpreted as an approach to describing uncertainty about beliefs. This is called the epistemic interpretation of probability. Traditionally, probability in assessment has been interpreted in the frequency of an event. For example, if you flip a coin 100 times we could use a probability of 0.5 to represents the proportion of times the coin would land on heads. This would lead us to expect the number heads to occur to be approximately 50. In the epistemic interpretation, we could describe that we believe that the coin is fair meaning would would place equal weight to heads and tails when flipped. However, if we had a belief that the coin favors heads we could reflect this in the probability we assign to each event. This epistemic interpretation aligns with (1) above where we aim to provide evidence through assessment. Context of assessment should be incorporated into modeling whenever possible. Context (when, where, how long, how much, with whom, etc.) should be considered (at least) as part of the assessment and included in modeling/decision making. Without such details, the analyst may overlook an important consideration in making a decision. Evidence-Centered Design A framework for describing assessments and the context around the assessment. Three properties of ECD are helps us understand the argumentation behind the use of particular psychometric models; helps us through the assessment development process that might lead to such models; and does not require the use of such models. "],["looking-forward.html", "1.3 Looking Forward", " 1.3 Looking Forward The remainder of this online accompanying text to BPM is organized as follows. Chapters 2-6 round of the Foundational information which includes a introduction of Bayesian inference (Chp 2), a discussion of the conceptual issues in Bayesian inferences (chp 3), a dive into normal distribution models (chp 4), a near 50,000 ft view of estimation with markov chain Monte Carlo (MCMC, chp 5), and introducing notation for regression modeling (chp 6). Next, we turn our attention to the meat of the book with section 2 which is the remainder of the text, chapters 7-14. These chapters move from basic psychometric modeling (chp 7) to classical test theory (chp 8), factor analysis (chp 9), item response theory (chp 11), latent class analysis (chp 13), and networks (chp 14). Other modeling issues and topics are discussed such as model comparison (chp 10) and missing data (chp 12). Throughout all these chapters I will go through all the example analyses using Stan instead of WinBUGS so that potential differences can be compared and discussed. "],["chp2.html", "Chapter 2 Introduction to Bayesian Inference", " Chapter 2 Introduction to Bayesian Inference Chapter 2 is focused on introducing the fundamentals of Bayesian modeling. I will briefly reiterate some of these concepts, but Levy and Mislevy did an excellent job introducing the basic concepts so I defer to them. A few points I would like to highlight are The concept of likelihood is fundamental to Bayesian methods and frequentist methods as well. The likelihood function (denoted \\(p(\\mathbf{x} \\mid \\theta)\\) or equivalently \\(L(\\theta \\mid \\mathbf{x})\\)) is fundamental as this conditional probability describes our beliefs about the data generating process. Another way of thinking about the likelihood function is as the data model. The data model decribes how parameters of interest relates to the observed data. This key concept is used in frequentist methods (e.g., maximum likelihood estimation) to obtain point estimates of model parameters. A fundamental difference between maximum likelihood and Bayesian estimation is how we use the likelihood function to construct interval estimates of parameters (see next point). Interval estimates in Bayesian methods do not rely on the idea of repeated sampling. In frequentist analyses, the construction of interval estimates around maximum likelihood estimators is dependent on utilizing repeated sampling paradigm. The interval estimate around the MLE is referred to the sampling distribution of the parameter estimator. BPM discusses these features of maximum likelihood well on p. 26. In Bayesian methods, an interval estimate is constructed based on the distribution of the parameter and not the parameter estimator. This distinction makes Bayesian intervals based on the likely values of the parameter based on our prior beliefs and observed data. Bayes Theorem Bayes theorem is the underlying engine of all Bayesian methods. We use Bayes theorm to decompose conditional probabilities so that they can work for us. As an analyst, we interested in the plausible values of the parameters based on the observed data. This can be expressed as a conditional probability (\\(p(\\theta \\mid \\mathbf{x})\\)). Bayes theorm states that \\[\\begin{equation} \\begin{split} p(\\theta \\mid \\mathbf{x}) &amp;= \\frac{p(\\mathbf{x}, \\theta)}{p(\\mathbf{x})}\\\\ &amp;= \\frac{p(\\mathbf{x}\\mid \\theta)p(\\theta)}{p(\\mathbf{x})}.\\\\ \\end{split} \\tag{2.1} \\end{equation}\\] The distinction between frequentist and Bayesian approaches is more than treating model parameters as random. A different way to stating the difference between frequentist and Bayesian approaches is based on what is being conditioned on to make inferences. In a classic frequentist hypothesis testing scenario, the model parameters are conditioned on to calculate the probability of the observed data (i.e., \\(\\mathrm{Pr}(data \\mid \\theta)\\)). This implies that the data are treated as random variables, but this does not exclude the fact the \\(\\theta\\) can be a collection of parameters that have random components (e.g., random intercepts in HLM). However, in a Bayesian model, the model parameters are the object of interest and the data are conditioned on (i.e., \\(\\mathrm{Pr}(\\theta \\mid data)\\)). This implies that the data are treated as a fixed entity that is used to construct inferences. This is how BPM related Bayesian inference to inductive reasoning. The inductive reasoning comes from taking observations and trying to making claims about the general. "],["beta-binomial-example.html", "2.1 Beta-binomial Example", " 2.1 Beta-binomial Example Here I go through the the first example from BPM. The example is a relatively simple beta-binomial model. Which is a way of modeling the number of occurrences of a bernoulli process. For example, suppose we were interested in the number of times a coin landed on heads. Here, we have a set number of coin flips (say \\(J\\)) and we are interested in the number of times the coin landed on heads (call this outcome \\(y\\)). We can model this structure letting \\(y\\) be a binomial random variable which we can express this as \\[y\\sim\\mathrm{Binomial}(\\theta, J)\\] where \\(\\theta\\) is the probability of heads on any given coin toss. As part of the Bayesian modeling I need to specify my prior belief as to the likely values of \\(\\theta\\). The probability \\(\\theta\\) lies in the interval \\([0, 1]\\). A nice probability distribution on this range is the beta distribution. That is, I can model my belief as to the likely values of the probability of heads by saying that \\(\\theta\\) is beta distributed which can be expressed as \\[\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\]. The two parameters for the beta distribution are representative of the shape the distribution will take. When \\(\\alpha = \\beta\\) the distribution is symmetrical, and when \\(\\alpha = \\beta=1\\) the beta distribution is flat or uniform over \\([0,1]\\). When a distribution is uniform I mean that all values are equally likely over the range of possible values which can be described as having the belief that all values are equally plausible. This model can be represented in a couple different ways. One way is as a directed acyclic graph (DAG). A DAG representation is very similar to path models in general structural equation modeling. The directed nature of the diagram highlights how observed variables (e.g., \\(y\\)) are modeled by unknown parameters \\(\\theta\\). All observed or explicitly defined variables/values are in rectangles while any latent variable or model parameter are in circles. DAG representation of model for the beta-binomal model is Figure 2.1: Directed Acyclic Graph (DAG) for the beta-binomial model I have given an alternative DAG representation that includes all relevant details. In terms of a DAG, I prefer this representation as all the assumed model components are made explicit. However, in more complex models this approach will likely lead to very dense and possible unuseful representations. Figure 2.2: DAG with explicit representation for all beta-binomial model components Yet another alternative representation is what I call a model specification chart. This takes on a similar feel as a DAG in that the flow of model parameters can be shown, but with the major difference that I use the distributional notation explicitly. Figure 2.3: Model specification diagram for beta-binomial model I will stick with these last two representations as much as possible. 2.1.1 Computation using Stan Now, Im finally getting to the analysis part. I have done my best to be descriptive of what the Stan code represents and how it works (in a general how to use this sense). I highly recommend a look at the example analysis by the development team to help see their approach as well (see here Stan analysis). model_beta_binomial &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y; real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { y ~ binomial(J, theta); theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_binomial, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: db495166b911389af4867f0120ac5e81. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.39 0.52 0.59 0.66 0.79 1553 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:11:47 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. I have downloaded the .bug file from the text website and I will load it into R for viewing. First, lets take a look at the model described by BPM on p. 39. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) } # data statement list(J = 10, y = 7, alpha = 6, beta = 6) Next, we want to use the above model. Using OpensBUGS through R can be a little clunky as I had to create objects with the filepaths of the data and model code then get R to read those in through the function openbugs. Otherwise, the code is similar to style to the code used for calling Stan. # model code model.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Binomial/Binomial Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 3.6 1.1 2.6 2.8 3.2 4.0 6.7 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 4.0 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) ################################# # Prior distribution ################################# theta ~ dbeta(alpha, beta) } # data mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpyqRSeb/model31246f711351.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.589 0.101 0.390 0.519 0.591 0.663 0.781 1.001 2000 ## deviance 3.583 1.091 2.643 2.764 3.193 4.027 6.585 1.001 1900 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.6 and DIC = 4.2 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],["beta-bernoulli-example.html", "2.2 Beta-Bernoulli Example", " 2.2 Beta-Bernoulli Example For this next example, I use the same data as the previous model. But now, instead of treating the individual events as part of a whole and sum over the successes, I will treat the model in a more hierarchical manner. A hierarchical model here simply implies that Ill be using the same probability function for all individual observations. We express this by saying that the observations depend on the index (\\(j=1, 2, ..., J\\)) but that the parameter of interest does not vary across \\(j\\). Two DAG representations similar to the previous examples are shown below. The major difference in these representations from the previous example is the inclusion of a plate that represents the observations depend on the index \\(j\\). Figure 2.4: DAG for the beta-bernoulli model Figure 2.5: DAG with explicit representation for all beta-bernoulli model components In my favor representation, this model can be expressed as Figure 2.6: Model specification diagram for beta-bernoulli model We will use the same \\(\\mathrm{Beta}(\\alpha, \\beta)\\) prior for \\(\\theta\\) as in the previous example. The model code changes to the following, 2.2.1 Computation using Stan model_beta_bernoulli &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y[J]; //declare observations as an integer vector of length J real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { for(j in 1:J){ y[j] ~ bernoulli(theta); } theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,1,1,1), alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_bernoulli, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: 7a99cbb09826cf6efe5d323426433fa9. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.38 0.52 0.59 0.66 0.78 1434 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:17:02 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. First, lets take a look at the model described by BPM on p. 41. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } } # data statement list(J=10, y=c(1,0,1,0,1,1,1,1,0,1), alpha=6, beta=6) The code is similar to style to the code used for calling Stan. However youll notice a difference in how a probability distribution is referenced. # model code model.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Bernoulli/Bernoulli Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 13.2 1.1 12.2 12.3 12.7 13.5 16.3 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 13.6 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) } # data mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,NA,1,1), alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 9 ## Unobserved stochastic nodes: 2 ## Total graph size: 14 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpyqRSeb/model312454083d2a.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.569 0.104 0.359 0.499 0.570 0.640 0.765 1.004 700 ## deviance 12.218 0.970 11.458 11.550 11.852 12.507 14.973 1.003 790 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.5 and DIC = 12.7 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],["conceptual-issues-in-bayesian-inference.html", "Chapter 3 Conceptual Issues in Bayesian Inference", " Chapter 3 Conceptual Issues in Bayesian Inference This chapter was conceptual so there was no code. "],["normal-distribution-models.html", "Chapter 4 Normal Distribution Models", " Chapter 4 Normal Distribution Models This chapter was mainly analytic derivations, but there was one section that did code so I show that in JAGS and Stan. "],["stan-model-for-mean-and-variance-unknown.html", "4.1 Stan Model for mean and variance unknown", " 4.1 Stan Model for mean and variance unknown The model for mean and variance unknown for normal sampling. Figure 4.1: DAG with for mean and variance unknown: Variance parameterization Or, alternatively, Figure 4.2: Model specification diagram for normal model model_normal &lt;- &#39; data { int N; real x[N]; real mu0; real sigma0; real alpha0; real beta0; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { x ~ normal(mu, sigma); mu ~ normal(mu0, sigma0); sigma ~ inv_gamma(alpha0, beta0); } &#39; # data must be in a list mydata &lt;- list( N = 10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92), mu0 = 75, sigma0 = 50, alpha0 = 5, beta0 = 150 ) # start values start_values &lt;- function(){ list(mu=50, sigma=5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_normal, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 682d92f82066fa7e19436da3c3fccc69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu 84.06 0.05 4.81 74.39 81.05 84.09 87.08 93.55 7913 1 ## sigma 14.82 0.04 3.87 9.13 12.08 14.20 16.87 24.03 7794 1 ## lp__ -52.88 0.01 1.07 -55.73 -53.30 -52.57 -52.12 -51.83 5949 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:39:35 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;mu&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;mu&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot posterior &lt;- as.data.frame(posterior) p &lt;- ggplot(posterior, aes(x=mu, y=sigma))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), sd(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(60.01, 120, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sig &lt;- function(x){extraDistr::dinvgamma(x, 5, 150)} x.sig &lt;- seq(0.01, 60, 0.01) prior.sig &lt;- data.frame(sigma=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=posterior, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=posterior, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sigma, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],["jags-model-for-mean-and-variance-unknown-precision-parameterization.html", "4.2 JAGS Model for mean and variance unknown (precision parameterization)", " 4.2 JAGS Model for mean and variance unknown (precision parameterization) The model for mean and variance unknown for normal sampling. Figure 4.3: DAG with for mean and variance unknown: Precision parameterization Or, alternatively, Figure 4.4: Model specification diagram for normal model with precision parameterization Now for the computation using JAGS # model code jags.model &lt;- function(){ ############################################# # Conditional distribution for the data ############################################# for(i in 1:n){ x[i] ~ dnorm(mu, tau) # conditional distribution of the data } # closes loop over subjects ############################################# # Define the prior distributions for the unknown parameters # The mean of the data (mu) # The variance (sigma.squared) and precision (tau) of the data ############################################# mu ~ dnorm(mu.mu, tau.mu) # prior distribution for mu mu.mu &lt;- 75 # mean of the prior for mu sigma.squared.mu &lt;- 50 # variance of the prior for mu tau.mu &lt;- 1/sigma.squared.mu # precision of the prior for mu tau ~ dgamma(alpha, beta) # precision of the data sigma.squared &lt;- 1/tau # variance of the data sigma &lt;- pow(sigma.squared, 0.5) # taking square root nu.0 &lt;- 10 # hyperparameter for prior for tau sigma.squared.0 &lt;- 30 # hyperparameter for prior for tau alpha &lt;- nu.0/2 # hyperparameter for prior for tau beta &lt;- nu.0*sigma.squared.0/2 # hyperparameter for prior for tau } # data mydata &lt;- list( n=10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92)) # starting values start_values &lt;- function(){ list(&quot;mu&quot;=75, &quot;tau&quot;=0.1) } # vector of all parameters to save param_save &lt;- c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 10 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpc1TPTO/model171464e22ff9.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## mu 83.247 2.173 78.841 81.847 83.258 84.724 87.398 1.001 12000 ## sigma 7.171 1.223 5.264 6.312 7.000 7.854 10.023 1.002 3600 ## tau 0.021 0.007 0.010 0.016 0.020 0.025 0.036 1.002 3600 ## deviance 71.189 1.818 69.392 69.893 70.641 71.915 76.075 1.001 12000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.7 and DIC = 72.8 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;mu&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;tau&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot p &lt;- ggplot(plot.data, aes(x=mu, y=tau))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), 1/var(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(70.01, 100, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_tau &lt;- function(x){dgamma(x, 5, 150)} x.tau &lt;- seq(0.0001, 0.06, 0.0001) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],["markov-chain-monte-carlo-estimation.html", "Chapter 5 Markov Chain Monte Carlo Estimation", " Chapter 5 Markov Chain Monte Carlo Estimation This chapter on MCMC methods gives an introduction to some of the common and basic sampling approaches for Bayesian methods. These methods in Gibbs Sampling Metropolis Sampling Metropolis-Hastings and some notes on how these approaches are related. The most important take away for me was their section on practical issues in MCMC methods. These practical aspects of estimation that should be noted are: Assessing convergence - making sure enough iterations have been used including the potential scale reduction factor (\\(\\hat{R}\\)), Serial dependence - where the samples drawn from the posterior are autocorrelated. This means that within a chain the draws are dependent but with enough draws and thinning all samples are sufficiently independent, Mixing - that different chains search/sample from the same parameter space but different chains can sometimes get stuck sampling one part of the parameter space that is not the same as the other chains. Lastly, a major take away from this chapter is that MCMC methods help to approximate the posterior distribution. The distribution is the solution of a full Bayesian analysis and not a point estimate. "],["regression.html", "Chapter 6 Regression", " Chapter 6 Regression For the regression models, we have built up what the DAG could look like. These representations are shown below. Figure 6.1: DAG a simple regression model with 1 predictor Figure 6.2: DAG for a regression with \\(J\\) predictors Figure 6.3: Expanded DAG representation for regression with hyperparameters included Next, we gave a general representation of how the model specification diagram could be constructed. Figure 6.4: Model specification diagram for a linear regression model "],["stan-model-for-regression-model.html", "6.1 Stan Model for Regression Model", " 6.1 Stan Model for Regression Model model_reg &lt;- &#39; data { int N; real x1[N]; real x2[N]; real y[N]; } parameters { real beta[3]; real&lt;lower=0&gt; tau; } transformed parameters { real&lt;lower=0&gt; sigma; sigma = 1/sqrt(tau); } model { for(i in 1:N){ y[i] ~ normal(beta[1] + beta[2]*x1[i] + beta[3]*x2[i], sigma); } beta ~ normal(0, 100); tau ~ gamma(1, 1); } generated quantities { real varerror; real vary; real Rsquared; real error[N]; for(i in 1:N){ error[i] = y[i] - (beta[1] + beta[2]*x1[i] + beta[3]*x2[i]); } varerror = variance(error); vary = variance(y); Rsquared = 1 - (varerror/vary); } &#39; # data must be in a list dat &lt;- read.table(&quot;data/Chp4_Reg_Chapter_Tests.dat&quot;, header=T) mydata &lt;- list( N=nrow(dat), x1=dat$Ch1Test, x2=dat$Ch2Test, y =dat$Ch3Test ) # start values start_values &lt;- function(){ list(sigma=1, beta=c(0,0,0)) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_reg, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 66db47b16bda4d720cafb6a9769da243. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## beta[1] -2.50 0.02 1.94 -6.24 -3.81 -2.51 -1.18 1.31 7938 ## beta[2] 0.65 0.00 0.17 0.33 0.54 0.66 0.76 0.97 7389 ## beta[3] 0.38 0.00 0.10 0.19 0.32 0.38 0.45 0.59 8465 ## tau 0.28 0.00 0.06 0.18 0.24 0.28 0.32 0.41 9068 ## sigma 1.91 0.00 0.20 1.57 1.77 1.89 2.03 2.34 9035 ## varerror 3.65 0.00 0.15 3.51 3.55 3.61 3.71 4.07 5481 ## vary 8.79 0.00 0.00 8.79 8.79 8.79 8.79 8.79 2 ## Rsquared 0.58 0.00 0.02 0.54 0.58 0.59 0.60 0.60 5481 ## error[1] -0.26 0.02 1.41 -3.04 -1.22 -0.25 0.70 2.46 8023 ## error[2] 1.71 0.01 0.51 0.72 1.37 1.71 2.05 2.70 8555 ## error[3] -1.06 0.01 0.57 -2.16 -1.44 -1.06 -0.68 0.05 8104 ## error[4] -3.21 0.01 0.76 -4.69 -3.71 -3.22 -2.71 -1.73 7925 ## error[5] -2.79 0.01 0.52 -3.82 -3.14 -2.80 -2.44 -1.77 9333 ## error[6] 0.44 0.00 0.42 -0.39 0.16 0.44 0.72 1.25 9732 ## error[7] -1.95 0.00 0.40 -2.72 -2.21 -1.95 -1.68 -1.17 9812 ## error[8] -6.22 0.00 0.39 -6.98 -6.47 -6.22 -5.96 -5.46 10494 ## error[9] 3.40 0.00 0.34 2.74 3.18 3.40 3.62 4.06 11553 ## error[10] 4.02 0.00 0.31 3.40 3.81 4.02 4.22 4.63 12213 ## error[11] -0.75 0.00 0.35 -1.44 -0.99 -0.75 -0.51 -0.06 10525 ## error[12] 0.48 0.01 0.49 -0.47 0.16 0.48 0.81 1.43 9240 ## error[13] 2.48 0.01 0.49 1.53 2.16 2.48 2.81 3.43 9240 ## error[14] -0.72 0.01 0.69 -2.05 -1.18 -0.72 -0.26 0.63 9071 ## error[15] 0.36 0.00 0.30 -0.23 0.17 0.36 0.56 0.95 14290 ## error[16] 0.36 0.00 0.30 -0.23 0.17 0.36 0.56 0.95 14290 ## error[17] 0.98 0.00 0.27 0.44 0.80 0.98 1.16 1.52 16668 ## error[18] 1.98 0.00 0.27 1.44 1.80 1.98 2.16 2.52 16668 ## error[19] -0.40 0.00 0.28 -0.95 -0.59 -0.40 -0.22 0.16 15786 ## error[20] 1.60 0.00 0.28 1.05 1.41 1.60 1.78 2.16 15786 ## error[21] 1.21 0.00 0.32 0.58 1.00 1.21 1.43 1.86 13045 ## error[22] 2.21 0.00 0.32 1.58 2.00 2.21 2.43 2.86 13045 ## error[23] 0.83 0.00 0.39 0.07 0.57 0.83 1.09 1.59 11242 ## error[24] 0.83 0.00 0.39 0.07 0.57 0.83 1.09 1.59 11242 ## error[25] 3.01 0.01 0.88 1.31 2.42 3.01 3.60 4.75 8649 ## error[26] -1.37 0.01 0.78 -2.89 -1.90 -1.37 -0.84 0.18 8741 ## error[27] -0.91 0.00 0.44 -1.75 -1.20 -0.91 -0.61 -0.05 10068 ## error[28] 0.09 0.00 0.44 -0.75 -0.20 0.09 0.39 0.95 10068 ## error[29] 2.09 0.00 0.44 1.25 1.80 2.09 2.39 2.95 10068 ## error[30] -1.67 0.00 0.32 -2.29 -1.88 -1.67 -1.46 -1.05 13262 ## error[31] -0.67 0.00 0.32 -1.29 -0.88 -0.67 -0.46 -0.05 13262 ## error[32] -0.06 0.00 0.29 -0.63 -0.25 -0.06 0.14 0.53 15416 ## error[33] 0.94 0.00 0.29 0.37 0.75 0.94 1.14 1.53 15416 ## error[34] 1.94 0.00 0.29 1.37 1.75 1.94 2.14 2.53 15416 ## error[35] -1.44 0.00 0.30 -2.04 -1.64 -1.44 -1.24 -0.83 15538 ## error[36] -0.44 0.00 0.30 -1.04 -0.64 -0.44 -0.24 0.17 15538 ## error[37] 1.56 0.00 0.30 0.96 1.36 1.56 1.76 2.17 15538 ## error[38] 0.17 0.00 0.35 -0.50 -0.05 0.17 0.40 0.86 13787 ## error[39] 1.17 0.00 0.35 0.50 0.95 1.17 1.40 1.86 13787 ## error[40] -0.21 0.00 0.41 -1.01 -0.48 -0.21 0.06 0.60 12081 ## error[41] -1.33 0.00 0.43 -2.15 -1.61 -1.33 -1.04 -0.49 10069 ## error[42] 0.91 0.00 0.37 0.19 0.66 0.90 1.15 1.63 11958 ## error[43] -3.48 0.00 0.38 -4.22 -3.74 -3.48 -3.23 -2.73 12463 ## error[44] -2.48 0.00 0.38 -3.22 -2.74 -2.48 -2.23 -1.73 12463 ## error[45] -1.86 0.00 0.41 -2.68 -2.14 -1.86 -1.59 -1.03 12255 ## error[46] -0.86 0.00 0.41 -1.68 -1.14 -0.86 -0.59 -0.03 12255 ## error[47] -0.86 0.00 0.41 -1.68 -1.14 -0.86 -0.59 -0.03 12255 ## error[48] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## error[49] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## error[50] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## lp__ -59.40 0.02 1.44 -63.00 -60.11 -59.07 -58.35 -57.61 5286 ## Rhat ## beta[1] 1 ## beta[2] 1 ## beta[3] 1 ## tau 1 ## sigma 1 ## varerror 1 ## vary 1 ## Rsquared 1 ## error[1] 1 ## error[2] 1 ## error[3] 1 ## error[4] 1 ## error[5] 1 ## error[6] 1 ## error[7] 1 ## error[8] 1 ## error[9] 1 ## error[10] 1 ## error[11] 1 ## error[12] 1 ## error[13] 1 ## error[14] 1 ## error[15] 1 ## error[16] 1 ## error[17] 1 ## error[18] 1 ## error[19] 1 ## error[20] 1 ## error[21] 1 ## error[22] 1 ## error[23] 1 ## error[24] 1 ## error[25] 1 ## error[26] 1 ## error[27] 1 ## error[28] 1 ## error[29] 1 ## error[30] 1 ## error[31] 1 ## error[32] 1 ## error[33] 1 ## error[34] 1 ## error[35] 1 ## error[36] 1 ## error[37] 1 ## error[38] 1 ## error[39] 1 ## error[40] 1 ## error[41] 1 ## error[42] 1 ## error[43] 1 ## error[44] 1 ## error[45] 1 ## error[46] 1 ## error[47] 1 ## error[48] 1 ## error[49] 1 ## error[50] 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:57:59 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## &#39;pars&#39; not specified. Showing first 10 parameters by default. ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;beta&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;beta&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p2 &lt;- ggs_grb(ggs(fit, family = &quot;sigma&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 + p2 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;beta&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p2 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;sigma&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 + p2 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;Rsquared&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot fit.lm &lt;- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat)) MLE &lt;- c(fit.lm$coefficients[,1], fit.lm$sigma**2, fit.lm$r.squared) prior_beta &lt;- function(x){dnorm(x, 0, 1000)} x.beta &lt;- seq(-10, 4.99, 0.01) prior.beta &lt;- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta)) prior_sig &lt;- function(x){dgamma(x, 1, 1)} x.sig &lt;- seq(0.01, 2.5, 0.01) prior.sig &lt;- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=Rsquared, color=&quot;Posterior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 268 rows containing non-finite values (stat_density). ## Warning: Removed 1399 row(s) containing missing values (geom_path). ## Warning: Removed 1399 row(s) containing missing values (geom_path). "],["jags-model-for-regression-model.html", "6.2 JAGS Model for Regression Model", " 6.2 JAGS Model for Regression Model # model code jags.model &lt;- function(){ ############################################ # Prior distributions ############################################ beta.0 ~ dnorm(0, .001) # prior for the intercept beta.1 ~ dnorm(0, .001) # prior for coefficient 1 beta.2 ~ dnorm(0, .001) # prior for coefficient 2 tau.e ~ dgamma(1, 1) # prior for the error precision sigma.e &lt;- 1/sqrt(tau.e) # standard deviation of the errors ############################################ # Conditional distribution of the data # Via a regression model ############################################ for(i in 1:n){ y.prime[i] &lt;- beta.0 + beta.1*x1[i] + beta.2*x2[i] y[i] ~ dnorm(y.prime[i], tau.e) } ############################################ # Calculate R-squared ############################################ for(i in 1:n){ error[i] &lt;- y[i] - y.prime[i] } var.error &lt;- sd(error[])*sd(error[]) var.y &lt;- sd(y[])*sd(y[]) R.squared &lt;- 1 - (var.error/var.y) } # data dat &lt;- read.table(&quot;data/Chp4_Reg_Chapter_Tests.dat&quot;, header=T) mydata &lt;- list( n=nrow(dat), x1=dat$Ch1Test, x2=dat$Ch2Test, y =dat$Ch3Test ) # starting values start_values &lt;- function(){ list(&quot;tau.e&quot;=0.01, &#39;beta.0&#39;=0, &quot;beta.1&quot;=0, &quot;beta.2&quot;=0) } # vector of all parameters to save param_save &lt;- c(&quot;tau.e&quot;, &quot;beta.0&quot;, &quot;beta.1&quot;, &quot;beta.2&quot;, &quot;R.squared&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 4 ## Total graph size: 262 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmp6LGnyB/model44a8e65278.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## R.squared 0.584 0.020 0.538 0.578 0.590 0.596 0.601 1.037 7100 ## beta.0 -2.513 1.908 -6.295 -3.771 -2.506 -1.259 1.249 1.001 12000 ## beta.1 0.656 0.164 0.337 0.543 0.659 0.767 0.983 1.001 12000 ## beta.2 0.382 0.103 0.181 0.313 0.382 0.451 0.583 1.001 7900 ## tau.e 0.282 0.057 0.182 0.243 0.278 0.318 0.404 1.001 9400 ## deviance 207.595 2.887 204.084 205.498 206.954 208.944 214.745 1.001 6200 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.2 and DIC = 211.8 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;beta.0&quot;, &quot;beta.1&quot;, &quot;beta.2&quot;, &quot;tau.e&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;R.squared&quot;), prob = 0.8) + plot_title # Expanded Posterior Plot fit.lm &lt;- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat)) MLE &lt;- c(fit.lm$coefficients[,1], 1/fit.lm$sigma**2, fit.lm$r.squared) prior_beta &lt;- function(x){dnorm(x, 0, 1000)} x.beta &lt;- seq(-5, 4.99, 0.01) prior.beta &lt;- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta)) prior_tau &lt;- function(x){dgamma(x, 1, 1)} x.tau &lt;- seq(0.01, 0.50, 0.01) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.0, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.1, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.2, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau.e, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=R.squared, color=&quot;Posterior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.5, 0.65))+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 235 rows containing non-finite values (stat_density). ## Warning: Removed 899 row(s) containing missing values (geom_path). ## Warning: Removed 4 rows containing non-finite values (stat_density). ## Warning: Removed 899 row(s) containing missing values (geom_path). ## Warning: Removed 39 rows containing non-finite values (stat_density). "],["canonical-bayesian-psychometric-modeling.html", "Chapter 7 Canonical Bayesian Psychometric Modeling", " Chapter 7 Canonical Bayesian Psychometric Modeling This chapter provides an overview of the purposes that psychometric modeling serve and how a Bayesian approach can fit into this purpose. Other information was introduced such as directed acyclic graph (DAG) representations of basic models and notation for future chapters. A canonical (conventional) psychometric analysis focuses on scoring and calibration. Scoring refers to arriving at a representation for an examinee based on their performance on an assessment, and Calibration refers to arriving at a representation for the measurement model parameters (and possibly hyper-parameters). In a research or operational setting, these two core components can be the focus separately or together. When the focus is on the simultaneous assessment of measurement model parameters and person parameters, there may need to be concession made with how to conduct both. In one sense, we would like to estimate all the parameters simultaneous so that uncertainty in measurement model parameters is reflected in person parameter (ability) estimates. However, traditional psychometric analysis tends to proceed by 1) estimating the measurement model parameters only by first integrating the person parameter distribution out of the likelihood function, then 2) using the now estimated measurement model parameters as fixed quantities, we estimate the person parameters (factor scores). This process results in the uncertainty in the person parameters being decreased. One potential benefit of a Bayesian approach is that both calibration and scoring can be done simultaneously. This may not always be of interest in the current application so the traditional approach may still be done. A canonical psychometric model can be expressed as a DAG using a model similar to the path models traditionally used in SEM. We expressed two forms of a model shown in the book below. Figure 7.1: DAG for canonical psychometric modeling Figure 7.2: Expanded DAG to include measurement model parameters "],["classical-test-theory.html", "Chapter 8 Classical Test Theory", " Chapter 8 Classical Test Theory The traditional model specification for CTT is \\[X = T + E,\\] where \\(X\\) is the observed test/measure score, \\(T\\) is the truce score we wish to make inferences about, and \\(E\\) is the error. The true scores have population mean \\(\\mu_T\\) and variance \\(\\sigma^2_T\\). The errors for any individual are expected to be 0 on average, \\(\\mathbb{E}(E_i)=0\\) with variance \\(\\sigma^2_E\\). The errors are uncorrelated with the true score in the population, that is \\[\\mathbb{COV}(T, E) = \\sigma_{TE} = \\rho_{TE}\\sigma_{T}\\sigma_E = 0.\\] Some implications associated with the CTT model are: The population mean of observed scores is the same as the true scores \\[\\mu_x = \\mu_T.\\] The observed score variance can be decomposed into \\[\\begin{align*} \\sigma^2_X &amp;= \\sigma^2_T + \\sigma^2_E + 2\\sigma_{TE}\\\\ &amp;= \\sigma^2_T + \\sigma^2_E. \\end{align*}\\] We can define the reliability in terms of the ratio of true score variance to observed score variance, that is \\[\\rho = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E}.\\] An interesting approach to deriving estimates of true scores is to flip the traditional CTT model around so that we define the true score as a function of the observed score. This uses Kelleys formula (Kelley 1923), \\[\\begin{align*} \\hat{T}_i &amp;= \\rho x_i + (1-\\rho)\\mu_x\\\\ &amp;= \\mu_x + \\rho (x_i - \\mu_x), \\end{align*}\\] where \\(\\mu_x\\) is the mean of the observed scores and \\(\\hat{T}_i\\) is the estimated true score of individual \\(i\\). This is an interesting formula since theres the notion about how to incorporate uncertainty into the estimation of the true score. The higher the uncertainty (lower the reliability) the less we weight the observed score and more we rely on the population mean as our estimate. This has a very Bayesian feel to is, because its nearly identical to how we derive the posterior mean in a conjugate normal model (see p.158). "],["example-1-known-measurement-model-parameters-with-1-measure.html", "8.1 Example 1 - Known measurement model parameters with 1 measure", " 8.1 Example 1 - Known measurement model parameters with 1 measure Here, we will discuss a simple CTT example where we assume that the measurement model parameters are known. This means we assume a value for \\(\\mu_t\\), \\(\\sigma^2_T\\), and \\(\\sigma^2_E\\). We would nearly always need to estimate these quantities to provide an informed decision as to what these parameters should be. This example using 3 observations (individuals) with 1 measure per individual. The DAG for this model is shown below. Figure 8.1: Simple CTT model with 1 measure and known measurement parameters With a simply model specification using normal distributions as the underly probability functions. Figure 8.2: Model specification diagram for the known parameters CTT model "],["example-1-stan.html", "8.2 Example 1 - Stan", " 8.2 Example 1 - Stan model_ctt1 &lt;- &#39; data { int N; real x[N]; real muT; real sigmaT; real sigmaE; } parameters { real T[N]; } model { for(i in 1:N){ x[i] ~ normal(T[i], sigmaE); T[i] ~ normal(muT, sigmaT); } } &#39; # data must be in a list mydata &lt;- list( N=3, x=c(70, 80, 96), muT = 80, sigmaT = 6, #sqrt(36) sigmaE = 4 # sqrt(16) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt1, # model code to be compiled data = mydata, # my data chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: f5d98d6088830a6665ad87ae5eaa93c8. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 73.06 0.03 3.33 66.58 70.84 73.03 75.31 79.59 15634 1 ## T[2] 80.00 0.03 3.31 73.61 77.75 80.00 82.23 86.45 15102 1 ## T[3] 91.09 0.03 3.37 84.55 88.80 91.08 93.35 97.73 16506 1 ## lp__ -4.93 0.01 1.24 -8.17 -5.48 -4.61 -4.03 -3.53 7836 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:26:32 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;,&quot;T[2]&quot;,&quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- mydata$x prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-1-jags.html", "8.3 Example 1 - JAGS", " 8.3 Example 1 - JAGS # model code jags.model.ctt1 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY MODEL # WITH KNOWN HYPERPARAMETERS # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # KNOWN HYPERPARAMETERS ############################################ mu.T &lt;- 80 # Mean of the true scores sigma.squared.T &lt;- 36 # Variance of the true scores sigma.squared.E &lt;- 16 # Variance of the errors tau.T &lt;- 1/sigma.squared.T # Precision of the true scores tau.E &lt;- 1/sigma.squared.E # Precision of the errors ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:n) { T[i] ~ dnorm(mu.T, tau.T) # Distribution of true scores x[i] ~ dnorm(T[i], tau.E) # Distribution of observables } } # data mydata &lt;- list( n=3, x=c(70, 80, 96) ) # starting values start_values &lt;- function(){ list(&quot;T&quot;=c(80,80,80)) } # vector of all parameters to save param_save &lt;- c(&quot;T&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt1, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 13 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpQ7YNVi/model49f46422dfa.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## T[1] 73.117 3.319 66.487 70.918 73.108 75.357 79.648 1.001 9200 ## T[2] 80.044 3.329 73.532 77.825 80.032 82.300 86.535 1.002 3500 ## T[3] 90.991 3.335 84.418 88.774 90.974 93.255 97.499 1.001 12000 ## deviance 18.083 2.990 14.240 15.841 17.479 19.645 25.392 1.001 12000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.5 and DIC = 22.6 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;, &quot;T[2]&quot;, &quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- mydata$x prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-2-known-measurement-model-with-multiple-measures.html", "8.4 Example 2 - Known Measurement Model with Multiple Measures", " 8.4 Example 2 - Known Measurement Model with Multiple Measures Here, the only thing that changes from the first example is that now we have multiple observations per individual. We can think of this as if we could administer a test (or parallel tests) repeatedly without learning occuring. The DAG and model-specification change to: Figure 8.3: Simple CTT model with known measurement parameters and multiple measures Figure 8.4: Model specification diagram for the known parameters CTT model and multiple measures "],["example-2-stan.html", "8.5 Example 2 - Stan", " 8.5 Example 2 - Stan model_ctt2 &lt;- &#39; data { int N; int J; matrix[N, J] X; real muT; real sigmaT; real sigmaE; } parameters { real T[N]; } model { for(i in 1:N){ T[i] ~ normal(muT, sigmaT); for(j in 1:J){ X[i, j] ~ normal(T[i], sigmaE); } } } &#39; # data must be in a list mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T), muT = 80, sigmaT = 6, #sqrt(36) sigmaE = 4 # sqrt(16) ) # initial values start_values &lt;- function(){ list(T=c(80,80,80,80,80,80,80,80,80,80)) } # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt2, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 5eefd59584a6c5be6da9aa620ec6167e. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 76.89 0.01 1.72 73.49 75.73 76.90 78.07 80.21 31183 1 ## T[2] 79.09 0.01 1.72 75.71 77.94 79.08 80.25 82.46 30038 1 ## T[3] 82.03 0.01 1.72 78.68 80.87 82.02 83.20 85.41 31248 1 ## T[4] 75.05 0.01 1.71 71.69 73.89 75.05 76.21 78.36 30749 1 ## T[5] 72.64 0.01 1.73 69.28 71.46 72.64 73.83 76.01 31455 1 ## T[6] 88.47 0.01 1.71 85.11 87.31 88.47 89.63 91.84 30882 1 ## T[7] 77.24 0.01 1.70 73.90 76.09 77.25 78.38 80.61 32344 1 ## T[8] 80.55 0.01 1.69 77.27 79.43 80.54 81.67 83.84 29137 1 ## T[9] 80.18 0.01 1.71 76.88 79.02 80.18 81.34 83.60 32971 1 ## T[10] 89.01 0.01 1.71 85.64 87.84 89.00 90.17 92.37 29607 1 ## lp__ -23.47 0.03 2.24 -28.69 -24.74 -23.14 -21.83 -20.11 6988 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:34:50 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = paste0(&quot;T[&quot;,1:10,&quot;]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- rowMeans(mydata$X) prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + plot_layout(guides=&quot;collect&quot;) "],["example-2-jags.html", "8.6 Example 2 - JAGS", " 8.6 Example 2 - JAGS # model code jags.model.ctt2 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY # WITH KNOWN # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # KNOWN HYPERPARAMETERS ############################################ mu.T &lt;- 80 # Mean of the true scores sigma.squared.T &lt;- 36 # Variance of the true scores sigma.squared.E &lt;- 16 # Variance of the errors tau.T &lt;- 1/sigma.squared.T # Precision of the true scores tau.E &lt;- 1/sigma.squared.E # Precision of the errors ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:N) { T[i] ~ dnorm(mu.T, tau.T) # Distribution of true scores for(j in 1:J){ x[i, j] ~ dnorm(T[i], tau.E) # Distribution of observables } } } # data mydata &lt;- list( N = 10, J = 5, x = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # starting values start_values &lt;- function(){ list(&quot;T&quot;=rep(80,10)) } # vector of all parameters to save param_save &lt;- c(&quot;T&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 10 ## Total graph size: 68 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpq8pU6a/model4d046a677c1.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## T[1] 76.888 1.720 73.515 75.728 76.892 78.032 80.294 1.001 12000 ## T[2] 79.064 1.709 75.655 77.935 79.083 80.206 82.408 1.001 11000 ## T[3] 82.045 1.725 78.681 80.880 82.046 83.204 85.450 1.001 12000 ## T[4] 75.053 1.729 71.659 73.884 75.063 76.228 78.436 1.001 12000 ## T[5] 72.673 1.689 69.365 71.570 72.651 73.805 75.988 1.001 7300 ## T[6] 88.407 1.714 85.006 87.274 88.393 89.578 91.730 1.001 12000 ## T[7] 77.246 1.710 73.864 76.083 77.266 78.417 80.576 1.001 12000 ## T[8] 80.535 1.712 77.170 79.390 80.521 81.703 83.841 1.001 12000 ## T[9] 80.178 1.722 76.833 78.988 80.190 81.330 83.543 1.001 12000 ## T[10] 88.952 1.725 85.559 87.763 88.955 90.114 92.331 1.001 12000 ## deviance 269.701 4.423 262.951 266.485 269.038 272.243 280.007 1.001 11000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 9.8 and DIC = 279.5 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;, &quot;T[2]&quot;, &quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- rowMeans(mydata$X) ## Error in rowMeans(mydata$X): &#39;x&#39; must be an array of at least two dimensions prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-3-unknown-measurement-model-with-multiple-measures.html", "8.7 Example 3 - Unknown Measurement Model with Multiple Measures", " 8.7 Example 3 - Unknown Measurement Model with Multiple Measures Here, we finally get to the (more) realistic case when we dont have as much prior knowledge about the measurement model parameters (namely, variances). The structure relies on hierarchically specifying priors to induce conditional independence. The DAG and model-specification change to: Figure 8.5: Simple CTT model with unknown measurement parameters Figure 8.6: Model specification diagram for the unknown measurement model parameters "],["example-3-stan.html", "8.8 Example 3 - Stan", " 8.8 Example 3 - Stan model_ctt3 &lt;- &#39; data { int N; int J; matrix[N, J] X; } parameters { real T[N]; real muT; real&lt;lower=0&gt; sigmaT; real&lt;lower=0&gt; sigmaE; } model { for(i in 1:N){ T[i] ~ normal(muT, sigmaT); for(j in 1:J){ X[i, j] ~ normal(T[i], sigmaE); } } muT ~ normal(80, 10); sigmaT ~ inv_gamma(1, 6); sigmaE ~ inv_gamma(1, 4); } generated quantities { real rho; real rhocomp; rho = square(sigmaT)/(square(sigmaT) + square(sigmaE)); rhocomp = J*rho/((J-1)*rho + 1); } &#39; # data must be in a list mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # initial values start_values &lt;- function(){ list(T=c(80,80,80,80,80,80,80,80,80,80), muT=80, sigmaT=10, sigmaE=5) } # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt3, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 8af61d5f99903650a9f4417c76fd9a69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## T[1] 76.87 0.01 1.53 73.87 75.86 76.86 77.90 79.90 ## T[2] 79.08 0.01 1.54 76.09 78.06 79.08 80.11 82.08 ## T[3] 82.04 0.01 1.51 79.09 81.03 82.04 83.04 85.03 ## T[4] 75.02 0.01 1.52 72.05 74.01 75.02 76.02 78.06 ## T[5] 72.60 0.01 1.52 69.64 71.58 72.60 73.61 75.60 ## T[6] 88.52 0.01 1.54 85.49 87.49 88.54 89.56 91.53 ## T[7] 77.23 0.01 1.54 74.25 76.21 77.21 78.26 80.28 ## T[8] 80.55 0.01 1.51 77.56 79.55 80.54 81.53 83.53 ## T[9] 80.19 0.01 1.51 77.26 79.18 80.18 81.19 83.14 ## T[10] 89.07 0.01 1.55 85.97 88.06 89.09 90.11 92.06 ## muT 80.10 0.02 1.99 76.17 78.84 80.10 81.34 84.04 ## sigmaT 6.03 0.01 1.68 3.67 4.87 5.73 6.86 10.04 ## sigmaE 3.50 0.00 0.40 2.82 3.21 3.46 3.74 4.39 ## rho 0.72 0.00 0.11 0.49 0.66 0.73 0.80 0.90 ## rhocomp 0.92 0.00 0.04 0.83 0.90 0.93 0.95 0.98 ## lp__ -115.06 0.04 2.86 -121.63 -116.72 -114.66 -112.98 -110.62 ## n_eff Rhat ## T[1] 25673 1 ## T[2] 29294 1 ## T[3] 27260 1 ## T[4] 27424 1 ## T[5] 23635 1 ## T[6] 22939 1 ## T[7] 30949 1 ## T[8] 25073 1 ## T[9] 28241 1 ## T[10] 25912 1 ## muT 17119 1 ## sigmaT 16441 1 ## sigmaE 16557 1 ## rho 19364 1 ## rhocomp 18382 1 ## lp__ 5897 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:41:05 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## &#39;pars&#39; not specified. Showing first 10 parameters by default. ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;, &quot;muT&quot;, &quot;sigmaT&quot;, &quot;sigmaE&quot;, &quot;rho&quot;, &quot;rhocomp&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;T[&quot;,1:10,&quot;]&quot;), &quot;muT&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigmaT&quot;, &quot;sigmaE&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;rho&quot;, &quot;rhocomp&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- c(rowMeans(mydata$X), mean(mydata$X)) prior_mu &lt;- function(x){dnorm(x, 80, 10)} x.mu&lt;- seq(50.1, 100, 0.1) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sigt &lt;- function(x){dinvgamma(x, 1, 6)} x.sigt&lt;- seq(.1, 15, 0.1) prior.sigt &lt;- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_t &lt;- function(x){ mu &lt;- rnorm(1, 80, 10) sig &lt;- rinvgamma(1, 1, 4) rnorm(x, mu, sig) } x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=prior_t(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`muT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[11], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaE`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=&quot;collect&quot;) "],["example-3-jags.html", "8.9 Example 3 - JAGS", " 8.9 Example 3 - JAGS # model code jags.model.ctt3 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY MODEL # WITH UnkNOWN HYPERPARAMETERS # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # PRIOR DISTRIBUTIONS FOR HYPERPARAMETERS ############################################ muT ~ dnorm(80,.01) # Mean of the true scores tau.T ~ dgamma(1, 36) # Precision of the true scores tau.E ~ dgamma(1, 16) # Precision of the errors sigma.squared.T &lt;- 1/tau.T # Variance of the true scores sigma.squared.E &lt;- 1/tau.E # Variance of the errors # get SD for summarizing sigmaT &lt;- pow(sigma.squared.T, 0.5) sigmaE &lt;- pow(sigma.squared.E, 0.5) ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:N) { T[i] ~ dnorm(muT, tau.T) # Distribution of true scores for(j in 1:J){ X[i,j] ~ dnorm(T[i], tau.E) # Distribution of observables } } ############################################ # RELIABILITY ############################################ rho &lt;- sigma.squared.T/(sigma.squared.T+sigma.squared.E) rhocomp &lt;- J*rho/((J-1)*rho+1) } # data mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # initial values start_values &lt;- list( list(&quot;T&quot;=c(60,85,80,95,74,69,91,82,87,78), &quot;muT&quot;=80, &quot;tau.E&quot;=0.06, &quot;tau.T&quot;=0.023), list(&quot;T&quot;=c(63, 79, 74, 104, 80, 71, 95, 72, 80, 82), &quot;muT&quot;=100, &quot;tau.E&quot;=0.09, &quot;tau.T&quot;=0.05), list(&quot;T&quot;=c(59, 86, 88, 89, 76, 65, 94, 72, 95, 84), &quot;muT&quot;=70, &quot;tau.E&quot;=0.03, &quot;tau.T&quot;=0.001), list(&quot;T&quot;=c(60, 87, 90, 91, 77, 74, 95, 76, 83, 87), &quot;muT&quot;=90, &quot;tau.E&quot;=0.01, &quot;tau.T&quot;=0.1) ) # vector of all parameters to save param_save &lt;- c(&quot;T&quot;,&quot;muT&quot;,&quot;sigmaT&quot;,&quot;sigmaE&quot;, &quot;rho&quot;, &quot;rhocomp&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Warning in jags.model(model.file, data = data, inits = init.values, n.chains = n.chains, : Unused ## variable &quot;X&quot; in data ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 60 ## Total graph size: 68 ## ## Deleting model ## Error in setParameters(init.values[[i]], i): Error in node tau.E ## Cannot set value of non-variable node print(fit) ## Inference for Stan model: 8af61d5f99903650a9f4417c76fd9a69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 76.87 0.01 1.53 73.87 75.86 76.86 77.90 79.90 25673 1 ## T[2] 79.08 0.01 1.54 76.09 78.06 79.08 80.11 82.08 29294 1 ## T[3] 82.04 0.01 1.51 79.09 81.03 82.04 83.04 85.03 27260 1 ## T[4] 75.02 0.01 1.52 72.05 74.01 75.02 76.02 78.06 27424 1 ## T[5] 72.60 0.01 1.52 69.64 71.58 72.60 73.61 75.60 23635 1 ## T[6] 88.52 0.01 1.54 85.49 87.49 88.54 89.56 91.53 22939 1 ## T[7] 77.23 0.01 1.54 74.25 76.21 77.21 78.26 80.28 30949 1 ## T[8] 80.55 0.01 1.51 77.56 79.55 80.54 81.53 83.53 25073 1 ## T[9] 80.19 0.01 1.51 77.26 79.18 80.18 81.19 83.14 28241 1 ## T[10] 89.07 0.01 1.55 85.97 88.06 89.09 90.11 92.06 25912 1 ## muT 80.10 0.02 1.99 76.17 78.84 80.10 81.34 84.04 17119 1 ## sigmaT 6.03 0.01 1.68 3.67 4.87 5.73 6.86 10.04 16441 1 ## sigmaE 3.50 0.00 0.40 2.82 3.21 3.46 3.74 4.39 16557 1 ## rho 0.72 0.00 0.11 0.49 0.66 0.73 0.80 0.90 19364 1 ## rhocomp 0.92 0.00 0.04 0.83 0.90 0.93 0.95 0.98 18382 1 ## lp__ -115.06 0.04 2.86 -121.63 -116.72 -114.66 -112.98 -110.62 5897 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:41:05 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) ## Error in xy.coords(x, y, xlabel, ylabel, log = log, recycle = TRUE): &#39;list&#39; object cannot be coerced to type &#39;double&#39; # gelman-rubin-brook gelman.plot(jags.mcmc) ## Error in gelman.preplot(x, bin.width = bin.width, max.bins = max.bins, : Insufficient iterations to produce Gelman-Rubin plot # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) ## Error in jags.mcmc[[1]]: this S4 class is not subsettable plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) ## Error in y[, var.cols] &lt;- x: number of items to replace is not a multiple of replacement length colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;T[&quot;,1:10,&quot;]&quot;), &quot;muT&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero mcmc_areas( plot.data, pars = c(&quot;sigmaT&quot;, &quot;sigmaE&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero mcmc_areas( plot.data, pars = c(&quot;rho&quot;, &quot;rhocomp&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(rowMeans(mydata$X), mean(mydata$X)) prior_mu &lt;- function(x){dnorm(x, 80, 10)} x.mu&lt;- seq(50.1, 100, 0.1) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sigt &lt;- function(x){dinvgamma(x, 1, 6)} x.sigt&lt;- seq(.1, 15, 0.1) prior.sigt &lt;- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_t &lt;- function(x){ mu &lt;- rnorm(1, 80, 10) sig &lt;- rinvgamma(1, 1, 4) rnorm(x, mu, sig) } x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=prior_t(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`muT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[11], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaE`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=&quot;collect&quot;) ## Error in FUN(X[[i]], ...): object &#39;muT&#39; not found "],["confirmatory-factor-analysis.html", "Chapter 9 Confirmatory Factor Analysis", " Chapter 9 Confirmatory Factor Analysis The full Bayesian specification of a general CFA model for all associated unknowns is as follows. This includes probability statements, notation, parameters, likelihood, priors, and hyperparameters. The observed data is defined as the \\(n\\times J\\) matrix \\(\\mathbf{X}\\) for the \\(J\\) observed measures. The CFA model parameters are defined as \\[\\begin{align*} \\mathbf{x}_i &amp;= \\tau + \\Lambda\\xi_i + \\varepsilon_i\\\\ \\Sigma (\\mathbf{x}) &amp;= \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi \\end{align*}\\] \\(\\Xi\\) is the \\(n\\times M\\) matrix of latent variable scores on the \\(M\\) latent variables for the \\(n\\) respondents/subjects. For an single subject, \\(\\xi_i\\) represents the vector of scores on the latent variable(s). Values (location, scale, orientation, etc.) or \\(\\xi_i\\) are conditional on (1) \\(\\kappa\\), the \\(M\\times 1\\) vector of latent variable means, and (2) \\(\\Phi\\), the \\(M\\times M\\) covariance matrix of variable variables; \\(\\tau\\) is the \\(J\\times 1\\) vector of observed variable intercepts which is the expected value for the observed measures when the latent variable(s) are all \\(0\\); \\(\\Lambda\\) is the \\(J\\times M\\) matrix of factor loadings where the \\(j\\)th row and \\(m\\)th column represents the factor loading of the \\(j\\)th observed variable on the \\(m\\)th latent variable; \\(\\delta_i\\) is the \\(J\\times 1\\) vector of errors, where \\(E(\\delta_i)=\\mathbf{0}\\) with \\(\\mathrm{var}(\\delta_i)=\\Psi\\) which is the \\(J\\times J\\) error covariance matrix. \\[\\begin{align*} p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi\\mid \\mathbf{X}) &amp;\\propto p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)\\\\ &amp;= p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi) p(\\Xi\\mid\\kappa, \\Phi) p(\\kappa) p(\\Phi) p(\\tau) p(\\Lambda) p(\\Psi)\\\\ &amp;= \\prod_{i=1}^{n}\\prod_{j=1}^J\\prod_{m=1}^M p(x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj}) p(\\xi_i\\mid\\kappa, \\Phi) p(\\kappa_m) p(\\Phi) p(\\tau_j) p(\\lambda_j) p(\\psi_{jj}) \\end{align*}\\] where \\[\\begin{align*} x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj} &amp;\\sim \\mathrm{Normal}(\\tau_j+\\xi_i\\lambda^{\\prime}_j, \\psi_{jj}),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\xi_i\\mid\\kappa, \\Phi &amp;\\sim \\mathrm{Normal}(\\kappa, \\Phi),\\ \\mathrm{for}\\ i=1, \\cdots, n;\\\\ \\kappa_m &amp;\\sim \\mathrm{Normal}(\\mu_{\\kappa},\\sigma^2_{\\kappa}),\\ \\mathrm{for}\\ m = 1, \\cdots, M;\\\\ \\Phi &amp;\\sim \\mathrm{Inverse-Wishart}(\\Phi_0, d);\\\\ \\tau_j &amp;\\sim \\mathrm{Normal}(\\mu_{\\tau},\\sigma^2_{\\tau}),\\ \\mathrm{for}\\ j = 1, \\cdots, J;\\\\ \\lambda_{j,m} &amp;\\sim \\mathrm{Normal}(\\mu_{\\lambda}, \\sigma^2_{\\lambda}),\\ \\mathrm{for}\\ j = 1, \\cdots, J,\\ m = 1, \\cdots, M;\\\\ \\psi_{jj} &amp;\\sim \\mathrm{Inverse-Gamma}(\\nu_{\\psi}/2, \\nu_{\\psi}\\psi_0/2),\\ \\mathrm{for}\\ j=1, \\cdots, J. \\end{align*}\\] With the hyperparameters that are supplied by the analyst being defined as \\(\\mu_{\\kappa}\\) is the prior mean for the latent variable, \\(\\sigma^2_{\\kappa}\\) is the prior variance for the latent variable, \\(\\Phi_0\\) is the prior expectation for the covariance matrix among latent variables, \\(d\\) represents a dispersion parameter reflecting the magnitude of our beliefs about \\(\\Phi_0\\), \\(\\mu_{\\tau}\\) is the prior mean for the intercepts which reflects our knowledge about the location of the observed variables, \\(\\sigma^2_{\\tau}\\) is a measure of how much weight we want to give to the prior mean, \\(\\mu_{\\lambda}\\) is the prior mean for the factor loadings which can vary over items and latent variables, \\(\\sigma^2_{\\lambda}\\) is the measure of dispersion for the the factor loadings, where lower variances indicate a stronger belief about the values for the loadings, \\(\\nu_{\\psi}\\) is the measure of location for the gamma prior indicating our expectation for the magnitude of the error variance, \\(\\psi_0\\) is our uncertainty with respect to the location we selected for the variance, and Alternatively, we could place a prior on \\(\\Psi\\) instead of the individual residual variances. This would mean we would be placing a prior on the error-covariance matrix similar to how we specified a prior for latent variance covariance matrix. "],["single-latent-variable-model.html", "9.1 Single Latent Variable Model", " 9.1 Single Latent Variable Model Here we consider the model in section 9.3 which is a CFA model with 1 latent variable and 5 observed indicators. The graphical representation of these factor models get pretty complex pretty quickly, but for this example I have reproduced a version of Figure 9.3b, shown below. Figure 9.1: DAG for CFA model with 1 latent variable However, as the authors noted, the path diagram tradition of conveying models is also very useful in discussing and describing the model, which I give next. Figure 9.2: Path diagram for CFA model with 1 latent variable For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 9.3: Model specification diagram for the CFA model with 1 latent factor "],["jags-single-latent-variable.html", "9.2 JAGS - Single Latent Variable", " 9.2 JAGS - Single Latent Variable # model code jags.model.cfa1 &lt;- function(){ ######################################## # Specify the factor analysis measurement model for the observables ############################################## for (i in 1:n){ for(j in 1:J){ mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # model implied expectation for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ################################## # Specify the (prior) distribution for the latent variables #################################### for (i in 1:n){ ksi[i] ~ dnorm(kappa, inv.phi) # distribution for the latent variables } ###################################### # Specify the prior distribution for the parameters that govern the latent variables ################################### kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(.1, .1, .1, .1, .1), &quot;lambda&quot;=c(NA, 0, 0, 0, 0), &quot;inv.phi&quot;=1, &quot;inv.psi&quot;=c(1, 1, 1, 1, 1)), list(&quot;tau&quot;=c(3, 3, 3, 3, 3), &quot;lambda&quot;=c(NA, 3, 3, 3, 3), &quot;inv.phi&quot;=2, &quot;inv.psi&quot;=c(2, 2, 2, 2, 2)), list(&quot;tau&quot;=c(5, 5, 5, 5, 5), &quot;lambda&quot;=c(NA, 6, 6, 6, 6), &quot;inv.phi&quot;=.5, &quot;inv.psi&quot;=c(.5, .5, .5, .5, .5)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,2:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa1, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 3, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpu2Wtns/model27604c204620.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 7500 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda[2] 0.732 0.046 0.645 0.700 0.730 0.762 0.826 1.003 1000 ## lambda[3] 0.421 0.038 0.349 0.395 0.420 0.447 0.499 1.004 560 ## lambda[4] 1.056 0.067 0.931 1.010 1.055 1.100 1.195 1.004 600 ## lambda[5] 0.989 0.061 0.878 0.947 0.986 1.028 1.114 1.004 690 ## phi 0.433 0.043 0.353 0.403 0.430 0.460 0.523 1.005 530 ## psi[1] 0.373 0.028 0.322 0.354 0.372 0.392 0.429 1.001 7500 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.191 0.212 1.001 7500 ## psi[3] 0.180 0.012 0.158 0.171 0.179 0.188 0.205 1.001 7500 ## psi[4] 0.378 0.030 0.323 0.357 0.376 0.397 0.440 1.001 7500 ## psi[5] 0.266 0.022 0.225 0.251 0.265 0.281 0.311 1.001 3600 ## tau[1] 3.334 0.040 3.256 3.306 3.333 3.361 3.411 1.001 7500 ## tau[2] 3.898 0.029 3.842 3.879 3.898 3.917 3.956 1.001 7500 ## tau[3] 4.596 0.023 4.551 4.581 4.597 4.612 4.640 1.001 7500 ## tau[4] 3.034 0.041 2.952 3.006 3.034 3.061 3.115 1.001 7500 ## tau[5] 3.713 0.037 3.642 3.689 3.713 3.738 3.787 1.001 7500 ## deviance 3380.640 42.695 3301.046 3350.905 3379.767 3409.341 3467.116 1.001 7500 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 911.6 and DIC = 4292.2 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;lambda[&quot;, 2:5, &quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title "],["stan-single-latent-variable.html", "9.3 Stan - Single Latent Variable", " 9.3 Stan - Single Latent Variable model_cfa1 &lt;- &#39; data { int N; int J; matrix[N, J] X; } parameters { real ksi[N]; //latent variable values real tau[J]; //intercepts real load[J-1]; //factor loadings real&lt;lower=0&gt; psi[J]; //residual variance //real kappa; // factor means real&lt;lower=0&gt; phi; // factor variances } transformed parameters { real lambda[J]; lambda[1] = 1; lambda[2:J] = load; } model { real kappa; kappa = 0; // likelihood for data for(i in 1:N){ for(j in 1:J){ X[i, j] ~ normal(tau[j] + ksi[i]*lambda[j], psi[j]); } } // prior for latent variable parameters ksi ~ normal(kappa, phi); phi ~ inv_gamma(5, 10); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); for(j in 1:(J-1)){ load[j] ~ normal(1, 10); } } &#39; # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( N = 500, J = 5, X = as.matrix(dat) ) # initial values start_values &lt;- list( list(tau = c(.1,.1,.1,.1,.1), lambda=c(0, 0, 0, 0, 0), phi = 1, psi=c(1, 1, 1, 1, 1)), list(tau = c(3,3,3,3,3), lambda=c(3, 3, 3, 3, 3), phi = 2, psi=c(.5, .5, .5, .5, .5)), list(tau = c(5, 5, 5, 5, 5), lambda=c(6, 6, 6, 6, 6), phi = 2, psi=c(2, 2, 2, 2, 2)) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa1, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 1, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## Inference for Stan model: 2d84cef0cbb0e7ac36bc8c7a29bca5b9. ## 3 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## lambda[2] 0.81 0 0.05 0.72 0.78 0.81 0.85 0.92 1355 1 ## lambda[3] 0.47 0 0.04 0.40 0.45 0.47 0.50 0.56 2095 1 ## lambda[4] 1.10 0 0.07 0.97 1.05 1.10 1.15 1.26 1625 1 ## lambda[5] 1.06 0 0.07 0.93 1.01 1.06 1.10 1.20 1355 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 2162 1 ## tau[2] 3.90 0 0.03 3.84 3.88 3.90 3.92 3.95 1730 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 2974 1 ## tau[4] 3.03 0 0.04 2.95 3.01 3.03 3.06 3.11 2063 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 1678 1 ## psi[1] 0.60 0 0.02 0.55 0.58 0.60 0.61 0.64 6962 1 ## psi[2] 0.36 0 0.02 0.33 0.35 0.36 0.37 0.40 4102 1 ## psi[3] 0.37 0 0.01 0.35 0.37 0.37 0.38 0.40 8740 1 ## psi[4] 0.60 0 0.02 0.56 0.59 0.60 0.62 0.65 7053 1 ## psi[5] 0.48 0 0.02 0.44 0.47 0.48 0.49 0.52 5340 1 ## phi 0.60 0 0.03 0.54 0.58 0.60 0.63 0.67 1326 1 ## ksi[1] -0.23 0 0.22 -0.68 -0.38 -0.23 -0.08 0.21 18662 1 ## ksi[8] 0.85 0 0.23 0.41 0.70 0.85 1.01 1.30 12002 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:49:16 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ## Warning: Removed 50 row(s) containing missing values (geom_path). ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning: Removed 150 rows containing missing values (geom_bar). ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot colnames(dat) &lt;- paste0(&quot;x&quot;,1:5) lav.mod &lt;- &#39; xi =~ 1*x1 + x2 + x3 + x4 + x5 xi ~~ xi x1 ~ 1 x2 ~ 1 x3 ~ 1 x4 ~ 1 x5 ~ 1 &#39; lav.fit &lt;- lavaan::cfa(lav.mod, data=dat) MLE &lt;- lavaan::parameterEstimates(lav.fit) prior_tau &lt;- function(x){dnorm(x, 3, 10)} x.tau&lt;- seq(1, 5, 0.01) prior.tau &lt;- data.frame(tau=x.tau, dens.mtau = prior_tau(x.tau)) prior_lambda &lt;- function(x){dnorm(x, 1, 10)} x.lambda&lt;- seq(0, 2, 0.01) prior.lambda &lt;- data.frame(lambda=x.lambda, dens.lambda = prior_lambda(x.lambda)) prior_sig &lt;- function(x){dinvgamma(x, 5, 10)} x.sig&lt;- seq(.01, 1, 0.01) prior.sig &lt;- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_ksi &lt;- function(x){ mu &lt;- 0 sig &lt;- rinvgamma(1, 5, 10) rnorm(x, mu, sig) } x.ksi&lt;- seq(-5, 5, 0.01) prior.ksi &lt;- data.frame(ksi=prior_ksi(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; # get stan samples plot.data &lt;- as.data.frame(plot.data) # make plotting pieces p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). # phi p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`phi`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[6,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) # psi p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[12,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[13,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[14,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[15,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[16,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(guides = &quot;collect&quot;) "],["blavaan-single-latent-variable.html", "9.4 Blavaan - Single Latent Variable", " 9.4 Blavaan - Single Latent Variable # model model_cfa_blavaan &lt;- &quot; f1 =~ 1*PI + AD + IGC + FI + FC &quot; dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) fit &lt;- blavaan::bcfa(model_cfa_blavaan, data=dat) ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.001 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 1: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 1: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 1: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 1: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 1: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 1: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 1: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 1: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 1: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 1: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 1: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 5.473 seconds (Warm-up) ## Chain 1: 8.591 seconds (Sampling) ## Chain 1: 14.064 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.001 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 2: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 2: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 2: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 2: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 2: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 2: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 2: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 2: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 2: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 2: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 2: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 5.892 seconds (Warm-up) ## Chain 2: 9.185 seconds (Sampling) ## Chain 2: 15.077 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 3: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 3: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 3: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 3: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 3: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 3: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 3: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 3: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 3: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 3: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 3: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 5.008 seconds (Warm-up) ## Chain 3: 8.132 seconds (Sampling) ## Chain 3: 13.14 seconds (Total) ## Chain 3: ## Computing posterior predictives... summary(fit) ## blavaan (0.3-15) results of 1000 samples after 500 adapt/burnin iterations ## ## Number of observations 500 ## ## Number of missing patterns 1 ## ## Statistic MargLogLik PPP ## Value -2196.709 0.000 ## ## Latent Variables: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 =~ ## PI 1.000 NA ## AD 0.847 0.054 0.748 0.96 1.000 normal(0,10) ## IGC 0.494 0.039 0.424 0.575 1.000 normal(0,10) ## FI 1.130 0.076 0.984 1.289 1.000 normal(0,10) ## FC 1.090 0.070 0.958 1.237 1.000 normal(0,10) ## ## Intercepts: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 3.333 0.038 3.258 3.408 1.001 normal(0,32) ## .AD 3.898 0.027 3.846 3.95 1.002 normal(0,32) ## .IGC 4.596 0.021 4.555 4.638 1.002 normal(0,32) ## .FI 3.034 0.041 2.951 3.114 1.002 normal(0,32) ## .FC 3.712 0.037 3.64 3.784 1.001 normal(0,32) ## f1 0.000 NA ## ## Variances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 0.353 0.026 0.304 0.407 1.002 gamma(1,.5)[sd] ## .AD 0.120 0.012 0.097 0.146 0.999 gamma(1,.5)[sd] ## .IGC 0.133 0.009 0.116 0.152 1.000 gamma(1,.5)[sd] ## .FI 0.362 0.030 0.306 0.423 0.999 gamma(1,.5)[sd] ## .FC 0.223 0.021 0.185 0.267 1.000 gamma(1,.5)[sd] ## f1 0.337 0.040 0.263 0.419 1.000 gamma(1,.5)[sd] plot(fit) ## Two Latent Variable Model Here we consider the model in section 9.4 which is a CFA model with 2 latent variables and 5 observed indicators. The graphical representation of these factor models get pretty complex pretty quickly, but for this example I have reproduced a version of Figure 9.4, shown below. Figure 9.4: DAG for CFA model with 2 latent variables However, as the authors noted, the path diagram tradition of conveying models is also very useful in discussing and describing the model, which I give next. Figure 9.5: Path Diagram for CFA model with 2 latent variables For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 9.6: Model specification diagram for the CFA model with 2 latent factors "],["jags-two-latent-variable.html", "9.5 JAGS - Two Latent Variable", " 9.5 JAGS - Two Latent Variable # model code jags.model.cfa2 &lt;- function(){ ################### # Specify the factor analysis measurement model for the observables #################### for (i in 1:n){ # expected value for each examinee for each observable mu[i,1] &lt;- tau[1] + lambda[1,1]*ksi[i,1] mu[i,2] &lt;- tau[2] + lambda[2,1]*ksi[i,1] mu[i,3] &lt;- tau[3] + lambda[3,1]*ksi[i,1] mu[i,4] &lt;- tau[4] + lambda[4,2]*ksi[i,2] mu[i,5] &lt;- tau[5] + lambda[5,2]*ksi[i,2] for(j in 1:J){ x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ###################################################################### # Specify the (prior) distribution for the latent variables ###################################################################### for (i in 1:n){ ksi[i, 1:M] ~ dmnorm(kappa[], inv.phi[,]) # distribution for the latent variables } ###################################################################### # Specify the prior distribution for the parameters that govern the latent variables ###################################################################### for(m in 1:M){ kappa[m] &lt;- 0 # Means of latent variables } inv.phi[1:M,1:M] ~ dwish(dxphi.0[ , ], d); # prior for precision matrix for the latent variables phi[1:M,1:M] &lt;- inverse(inv.phi[ , ]); # the covariance matrix for the latent vars phi.0[1,1] &lt;- 1; phi.0[1,2] &lt;- .3; phi.0[2,1] &lt;- .3; phi.0[2,2] &lt;- 1; d &lt;- 2; for (m in 1:M){ for (mm in 1:M){ dxphi.0[m,mm] &lt;- d*phi.0[m,mm]; } } ###################################################################### # Specify the prior distribution for the measurement model parameters ###################################################################### for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1,1] &lt;- 1.0 # loading fixed to 1.0 lambda[4,2] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:3){ lambda[j,1] ~ dnorm(1, .1) # prior distribution for the remaining loadings } lambda[5,2] ~ dnorm(1, .1) # prior distribution for the remaining loadings } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, M =2, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01), lambda= structure( .Data= c( NA, 2.00E+00, 2.00E+00, NA, NA, NA, NA, NA, NA, 2.00E+00), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.00E+00, 0.00E+00, 0.00E+00, 1.00E+00), .Dim=c(2, 2)), inv.psi=c(1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00)), list(tau=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), lambda= structure( .Data= c( NA, 5.00E-01, 5.00E-01, NA, NA, NA, NA, NA, NA, 5.00E-01), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.33E+00, -6.67E-01, -6.67E-01, 1.33E+00), .Dim=c(2, 2)), inv.psi=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)) , list(tau=c(5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00), lambda= structure( .Data= c( NA, 1.00E+00, 1.00E+00, NA, NA, NA, NA, NA, NA, 1.00E+00), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.96E+00, -1.37E+00, -1.37E+00, 1.96E+00), .Dim=c(2, 2)), inv.psi=c(5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau&quot;, &quot;lambda[2,1]&quot;,&quot;lambda[3,1]&quot;,&quot;lambda[5,2]&quot;, &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 3, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 514 ## Total graph size: 9035 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpu2Wtns/model27604325135c.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 7500 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda[2,1] -0.192 1.385 -2.439 -2.002 0.735 0.786 0.873 15.478 3 ## lambda[3,1] -0.153 0.883 -1.599 -1.293 0.429 0.471 0.542 13.567 3 ## lambda[5,2] 0.928 0.145 0.815 0.880 0.915 0.955 1.043 1.008 590 ## phi[1,1] 0.273 0.161 0.042 0.059 0.353 0.397 0.467 7.702 3 ## phi[2,1] 0.215 0.231 -0.129 -0.097 0.353 0.389 0.448 10.412 3 ## phi[1,2] 0.215 0.231 -0.129 -0.097 0.353 0.389 0.448 10.412 3 ## phi[2,2] 0.492 0.065 0.391 0.459 0.494 0.529 0.601 1.002 1900 ## psi[1] 0.558 0.277 0.315 0.355 0.383 0.898 1.039 9.987 3 ## psi[2] 0.173 0.159 0.144 0.160 0.170 0.180 0.202 1.034 1600 ## psi[3] 0.169 0.221 0.141 0.157 0.166 0.175 0.194 1.225 16 ## psi[4] 0.344 0.130 0.283 0.319 0.339 0.360 0.411 1.012 740 ## psi[5] 0.246 0.158 0.203 0.228 0.242 0.257 0.290 1.018 1600 ## tau[1] 3.333 0.041 3.250 3.306 3.332 3.360 3.413 1.007 7500 ## tau[2] 3.897 0.028 3.841 3.878 3.897 3.916 3.951 1.001 4700 ## tau[3] 4.596 0.022 4.551 4.581 4.596 4.611 4.639 1.001 7500 ## tau[4] 3.034 0.041 2.953 3.006 3.034 3.061 3.116 1.002 1800 ## tau[5] 3.713 0.036 3.643 3.689 3.713 3.738 3.783 1.002 2700 ## deviance 3340.837 253.362 3081.261 3163.988 3220.955 3591.817 3741.760 3.914 3 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 8675.6 and DIC = 12016.4 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;lambda[2,1]&quot;,&quot;lambda[3,1]&quot;,&quot;lambda[5,2]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title ## Error: Some &#39;pars&#39; don&#39;t match parameter names: phi FALSE "],["stan-two-latent-variable.html", "9.6 Stan - Two Latent Variable", " 9.6 Stan - Two Latent Variable 9.6.1 Inverse-Wishart Prior Using Stan based on a nearly identical model structure presented in the text. model_cfa_2factor &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; matrix[M, M] phi0; } parameters { matrix[M, M] phi; // latent variable covaraince matrix matrix[N, M] ksi; //latent variable values real lambda[J]; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); // prior for ksi ksi[i] ~ multi_normal(rep_vector(0, M), phi); } // latent variable variance matrix phi ~ inv_wishart(2, phi0); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat, phi0 = matrix(c(1, .3, .3, 1), ncol=2) ) # # initial values start_values &lt;- list( list( phi= structure( .Data= c(1, 0.30, 0.30, 1), .Dim=c(2, 2)), tau = c(3, 3, 3, 3, 3), lambda= c(1, 1, 1, 1, 1), psi=c(.5, .5, .5, .5, .5) ), list( phi= structure( .Data= c(1, 0, 0, 1), .Dim=c(2, 2)), tau = c(5, 5, 5, 5, 5), lambda= c(1, .7, .7, 1, .7), psi=c(2, 2, 2, 2, 2) ), list( phi= structure( .Data= c(1, 0.10, 0.10, 1), .Dim=c(2, 2)), tau = c(1, 1, 1, 1, 1), lambda= c(1, 1.3, 1.3, 1, 1.3), psi=c(1, 1, 1, 1, 1) ) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa_2factor, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) control = list(adapt_delta = 0.9, max_treedepth = 12), refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found ## Warning: There were 14910 divergent transitions after warmup. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## to find out why this is a problem and how to eliminate them. ## Warning: There were 90 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 12. See ## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded ## Warning: Examine the pairs() plot to diagnose sampling problems ## Warning: The largest R-hat is 3.8, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 9cba7ccbea0f9cb6bfe744cf6be59b64. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## lambda[1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## lambda[2] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## lambda[3] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## lambda[4] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## lambda[5] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## tau[1] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[2] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[3] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[4] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[5] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## psi[1] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[2] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[3] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[4] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[5] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## phi[1,1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 5 ## phi[1,2] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 ## phi[2,1] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 ## phi[2,2] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## ksi[1,1] 0.81 1.01 1.24 -0.91 -0.91 1.43 1.93 1.93 2 ## ksi[1,2] 0.62 0.72 0.89 -0.57 -0.57 0.85 1.56 1.56 2 ## ksi[8,1] 0.14 0.62 0.76 -0.84 -0.84 0.27 1.00 1.00 2 ## ksi[8,2] -0.12 0.17 0.20 -0.34 -0.34 -0.17 0.15 0.15 2 ## Rhat ## lambda[1] 3.76 ## lambda[2] 200082.02 ## lambda[3] 110868.47 ## lambda[4] 3.24 ## lambda[5] 94909.59 ## tau[1] 971982.86 ## tau[2] 1168347.77 ## tau[3] 1446723.31 ## tau[4] 842172.13 ## tau[5] 1190014.95 ## psi[1] 277869.53 ## psi[2] 256094.75 ## psi[3] 333243.02 ## psi[4] 272051.82 ## psi[5] 201375.90 ## phi[1,1] 1.62 ## phi[1,2] 103038.69 ## phi[2,1] 103035.55 ## phi[2,2] 2.68 ## ksi[1,1] 536377.12 ## ksi[1,2] 706233.82 ## ksi[8,1] 519297.43 ## ksi[8,2] 159190.73 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 04:23:50 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot( fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) 9.6.2 LKJ Cholesky Parameterization Because I had such massive problems with the above, I search for how people estimate CFA models in Stan. I found that most people use the LKJ Cholesky parameterization. Some helpful pages that I used to help get this to work. Stan Users Guide on Factor Covaraince Parameterization Michael DeWitt - Confirmatory Factor Analysis in Stan Rick Farouni - Fitting a Bayesian Factor Analysis Model in Stan model_cfa2 &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; } parameters { cholesky_factor_corr[M] L; // Cholesky decomp of // corr mat of random slopes vector[M] A; // Vector of factor variances matrix[N, M] ksi; //latent variable values vector[J] lambda; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } transformed parameters { matrix[M, M] A0; vector[M] S; A0 = diag_pre_multiply(A, L); S = sqrt(A); } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); } // latent variable parameters A ~ inv_gamma(5, 10); L ~ lkj_corr_cholesky(M); for(i in 1:N){ ksi[i] ~ multi_normal_cholesky(rep_vector(0, M), A0); } // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); // factor loading patterns lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } generated quantities { matrix[M, M] R; matrix[M, M] phi; R = tcrossprod(L); phi = quad_form_diag(R, S); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa2, # model code to be compiled data = mydata, # my data #init = init_fun, #start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;R&quot;, &quot;A&quot;, &quot;A0&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 09d22e8b8116ca5395ea85ab1df26f50. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 16739 1 ## lambda[2] 0.87 0 0.06 0.76 0.83 0.87 0.91 0.99 2092 1 ## lambda[3] 0.52 0 0.04 0.44 0.49 0.52 0.54 0.60 3029 1 ## lambda[4] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 16022 1 ## lambda[5] 0.96 0 0.05 0.86 0.92 0.96 1.00 1.07 2956 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 4249 1 ## tau[2] 3.90 0 0.03 3.84 3.88 3.90 3.92 3.95 3252 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 4964 1 ## tau[4] 3.03 0 0.04 2.95 3.01 3.03 3.06 3.11 3855 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 3324 1 ## psi[1] 0.61 0 0.02 0.56 0.59 0.61 0.62 0.65 10217 1 ## psi[2] 0.32 0 0.02 0.28 0.31 0.32 0.33 0.36 3488 1 ## psi[3] 0.36 0 0.01 0.33 0.35 0.36 0.36 0.38 11296 1 ## psi[4] 0.57 0 0.03 0.52 0.55 0.57 0.58 0.62 7958 1 ## psi[5] 0.42 0 0.03 0.37 0.41 0.42 0.44 0.47 4437 1 ## R[1,1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## R[1,2] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2366 1 ## R[2,1] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2366 1 ## R[2,2] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## A[1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## A[2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 3536 1 ## A0[1,1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## A0[1,2] 0.00 NaN 0.00 0.00 0.00 0.00 0.00 0.00 NaN NaN ## A0[2,1] 0.61 0 0.04 0.53 0.58 0.60 0.63 0.68 4887 1 ## A0[2,2] 0.36 0 0.04 0.29 0.34 0.36 0.39 0.43 1869 1 ## phi[1,1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## phi[1,2] 0.56 0 0.03 0.49 0.54 0.56 0.58 0.62 3882 1 ## phi[2,1] 0.56 0 0.03 0.49 0.54 0.56 0.58 0.62 3882 1 ## phi[2,2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 3536 1 ## ksi[1,1] -0.22 0 0.23 -0.67 -0.38 -0.22 -0.07 0.24 24053 1 ## ksi[1,2] -0.36 0 0.28 -0.91 -0.55 -0.37 -0.18 0.18 22191 1 ## ksi[8,1] 0.91 0 0.23 0.46 0.75 0.91 1.06 1.38 19097 1 ## ksi[8,2] 0.88 0 0.27 0.34 0.70 0.88 1.07 1.42 23361 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 04:46:37 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi[1,1]&quot;, &quot;phi[1,2]&quot;, &quot;phi[2,2]&quot;), prob = 0.8) + plot_title "],["blavaan-two-latent-variables.html", "9.7 Blavaan - Two Latent Variables", " 9.7 Blavaan - Two Latent Variables # model model_cfa2_blavaan &lt;- &quot; f1 =~ 1*PI + AD + IGC f2 =~ 1*FI + FC f1 ~~ f2 &quot; dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) fit &lt;- blavaan::bcfa(model_cfa2_blavaan, data=dat) ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.001 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 1: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 1: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 1: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 1: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 1: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 1: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 1: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 1: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 1: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 1: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 1: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 6.738 seconds (Warm-up) ## Chain 1: 13.97 seconds (Sampling) ## Chain 1: 20.708 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 2: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 2: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 2: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 2: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 2: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 2: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 2: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 2: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 2: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 2: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 2: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 7.142 seconds (Warm-up) ## Chain 2: 13.648 seconds (Sampling) ## Chain 2: 20.79 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 3: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 3: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 3: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 3: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 3: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 3: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 3: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 3: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 3: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 3: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 3: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 6.865 seconds (Warm-up) ## Chain 3: 12.483 seconds (Sampling) ## Chain 3: 19.348 seconds (Total) ## Chain 3: ## Computing posterior predictives... summary(fit) ## blavaan (0.3-15) results of 1000 samples after 500 adapt/burnin iterations ## ## Number of observations 500 ## ## Number of missing patterns 1 ## ## Statistic MargLogLik PPP ## Value -2174.086 0.000 ## ## Latent Variables: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 =~ ## PI 1.000 NA ## AD 0.959 0.069 0.833 1.101 1.000 normal(0,10) ## IGC 0.563 0.046 0.478 0.656 1.000 normal(0,10) ## f2 =~ ## FI 1.000 NA ## FC 1.007 0.062 0.888 1.137 1.000 normal(0,10) ## ## Covariances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 ~~ ## f2 0.312 0.035 0.249 0.384 1.000 lkj_corr(1) ## ## Intercepts: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 3.334 0.038 3.258 3.41 1.000 normal(0,32) ## .AD 3.898 0.027 3.846 3.95 1.001 normal(0,32) ## .IGC 4.597 0.021 4.556 4.636 1.000 normal(0,32) ## .FI 3.035 0.039 2.959 3.111 1.001 normal(0,32) ## .FC 3.714 0.035 3.646 3.782 1.001 normal(0,32) ## f1 0.000 NA ## f2 0.000 NA ## ## Variances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 0.382 0.030 0.327 0.446 0.999 gamma(1,.5)[sd] ## .AD 0.077 0.013 0.052 0.103 1.000 gamma(1,.5)[sd] ## .IGC 0.116 0.009 0.1 0.135 0.999 gamma(1,.5)[sd] ## .FI 0.322 0.029 0.268 0.38 0.999 gamma(1,.5)[sd] ## .FC 0.151 0.024 0.104 0.199 1.000 gamma(1,.5)[sd] ## f1 0.311 0.041 0.234 0.394 1.000 gamma(1,.5)[sd] ## f2 0.465 0.051 0.372 0.569 1.000 gamma(1,.5)[sd] plot(fit) "],["indeterminacy-in-one-factor-cfa.html", "9.8 Indeterminacy in One Factor CFA", " 9.8 Indeterminacy in One Factor CFA # model code jags.model.cfa.ind &lt;- function(){ ###################################################################### # Specify the factor analysis measurement model for the observables ###################################################################### for (i in 1:n){ for(j in 1:J){ mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # model implied expectation for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ###################################################################### # Specify the (prior) distribution for the latent variables ###################################################################### for (i in 1:n){ ksi[i] ~ dnorm(kappa, inv.phi) # distribution for the latent variables } ###################################################################### # Specify the prior distribution for the parameters that govern the latent variables ###################################################################### kappa &lt;- 0 # Mean of factor 1 inv.phi &lt;-1 # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ###################################################################### # Specify the prior distribution for the measurement model parameters ###################################################################### for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } for (j in 1:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;lambda&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;inv.psi&quot;=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)), list(&quot;tau&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;lambda&quot;=c(-3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00), &quot;inv.psi&quot;=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau[1]&quot;, &quot;lambda[1]&quot;, &quot;phi&quot;, &quot;psi[1]&quot;, &quot;ksi[8]&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa.ind, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 2, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpAH8d3B/model2a0046052851.txt&quot;, fit using jags, ## 2 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 5000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## ksi[8] -0.009 1.414 -2.045 -1.363 0.054 1.322 2.055 8.057 2 ## lambda[1] -0.002 0.597 -0.652 -0.591 -0.017 0.588 0.649 21.322 2 ## phi 1.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000 1 ## psi[1] 0.382 0.029 0.329 0.362 0.381 0.401 0.440 1.005 400 ## tau[1] 3.333 0.039 3.257 3.308 3.333 3.359 3.408 1.001 5000 ## deviance 3384.995 63.370 3300.938 3353.736 3383.034 3413.631 3473.403 1.001 5000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 2008.3 and DIC = 5393.3 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) "],["model-evaluation.html", "Chapter 10 Model Evaluation", " Chapter 10 Model Evaluation The goal of evaluating the model is to determine if the inferences suggested by the model are reasonable based ones content area knowledge. The text and (BDA3) say that this evaluation of reasonableness is really a form of prior, that is Gelman et al. (2013) argued tha when analysts deem that the posterior distribution and inferences are unreasonable, what they are really expressing is that there is additional information available that was not included in the analysis. Having some knowledge about what the posterior distribution should look like can be extremely helpful in developing how the model takes shape (e.g., setting reasonable boundaries on parameters). Three aspects/questions we aim to address as part of the logic of model checking: What is going on in our data, possibly in relation to the model? What the model has to say about what should be going on? How what is going on in the data compared to what the model says should be going on? Model evaluation is accomplished through Residual analysis, Posterior predictive distributions, and Model comparisons. "],["residual-analysis.html", "10.1 Residual Analysis", " 10.1 Residual Analysis A residual is a key component of analysis in many statistical procedures. Here, we describe how this important feature can be used in Bayesian psychometric analysis. At a fundamental level, a residual is one component in the data-model relationship, that is \\[DATA = MODEL + RESIDUALS.\\] This view highlights the three major pieces we are going to use in the model evaluation process. We can rewrite the above to focus on the residuals as well. In factor analysis (CFA in particular), we tend to be interested in the residuals of the covariances among variables because our factor model is a hypothesis for how the observed variables are interrelated. In the observed data, we capture this relationship with the observed sample covariance matrix \\(\\mathbf{S}\\), which for variable \\(j\\) and \\(k\\) is usually computed as \\[s_{jk} = \\frac{\\sum_{\\forall i (x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)}}{n-1}.\\] And, the whole covariance matrix is \\(\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\prime}\\mathbf{X}\\) where \\(\\mathbf{X}\\) is in centered form. In CFA, we can get the model implied covariance matrix by using the estimated parameters to get \\(\\Sigma(\\theta)\\), that is \\[\\Sigma(\\theta) = \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi.\\] Then, to get the residual matrix we simple find the difference scores (\\(\\mathbf{E}\\)) between these matrices \\[\\mathbf{E} = \\mathbf{S} - \\Sigma(\\theta).\\] Although, we would probably want to rescale \\(\\mathbf{S}\\) and \\(\\Sigma(\\theta)\\) to be correlation matrices because the scale of the covariances and variances are likely not of primary interest. This means we can use the residuals correlations instead. Generating the residual correlations is accomplished using the posterior predictive distribution. "],["posterior-predictive-distributions.html", "10.2 Posterior Predictive Distributions", " 10.2 Posterior Predictive Distributions The posterior predictive distribution is used heavily in model evaluation. (look at Bayes notes) Basically, the posterior predictive distribution is the what values of the observed data (\\(Y\\)) are mostly likely given the posterior distribution. 10.2.1 Example of posterior predictive distribution of correlations In this example, we use the correlations of the observed variable as the function of interest. # model code jags.model.cfa &lt;- function(){ # # Specify the factor analysis measurement # model for the observables # for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, parameters.to.save = param_save, n.iter=15000, n.burnin = 5000, n.chains = 1, # for simplicity n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpUNZc83/model3bfc8bd33b6.txt&quot;, fit using jags, ## 1 chains, each with 15000 iterations (first 5000 discarded) ## n.sims = 10000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.729 0.045 0.645 0.698 0.727 0.759 0.822 ## lambda[3] 0.420 0.038 0.349 0.395 0.420 0.446 0.495 ## lambda[4] 1.052 0.065 0.929 1.007 1.051 1.096 1.182 ## lambda[5] 0.985 0.059 0.874 0.944 0.983 1.024 1.104 ## phi 0.435 0.042 0.357 0.405 0.433 0.462 0.523 ## psi[1] 0.373 0.028 0.321 0.354 0.372 0.391 0.431 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.191 0.211 ## psi[3] 0.180 0.012 0.157 0.171 0.179 0.187 0.204 ## psi[4] 0.377 0.030 0.323 0.356 0.376 0.396 0.440 ## psi[5] 0.266 0.022 0.225 0.251 0.265 0.280 0.310 ## tau[1] 3.334 0.040 3.254 3.306 3.334 3.360 3.413 ## tau[2] 3.897 0.029 3.841 3.878 3.897 3.917 3.952 ## tau[3] 4.596 0.023 4.552 4.581 4.596 4.611 4.640 ## tau[4] 3.034 0.041 2.955 3.006 3.034 3.061 3.114 ## tau[5] 3.713 0.037 3.641 3.688 3.713 3.738 3.786 ## deviance 3379.345 42.258 3298.273 3350.553 3379.160 3407.319 3463.922 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 892.9 and DIC = 4272.2 ## DIC is an estimate of expected predictive error (lower deviance is better). plot(fit) # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # compute model implied covariance/correlations # for each iterations out.mat &lt;- matrix(ncol=10,nrow=nrow(plot.data)) colnames(out.mat) &lt;- c(&quot;r12&quot;, &quot;r13&quot;, &quot;r14&quot;, &quot;r15&quot;, &quot;r23&quot;, &quot;r24&quot;, &quot;r25&quot;, &quot;r34&quot;,&quot;r35&quot;, &quot;r45&quot;) plot.data1 &lt;- cbind(plot.data,out.mat) # compute the model implied correlations for each iterations i &lt;- 1 for(i in 1:nrow(plot.data1)){ x &lt;- plot.data1[i,] x &lt;- unlist(x) lambda &lt;- matrix(x[4:8], ncol=1) phi &lt;- matrix(x[9], ncol=1) psi &lt;- diag(x[10:14], ncol=5, nrow=5) micov &lt;- lambda%*%phi%*%t(lambda)+psi D &lt;- diag(sqrt(diag(micov)), ncol=5, nrow=5) Dinv &lt;- solve(D) micor &lt;- Dinv%*%micov%*%Dinv outr &lt;- micor[lower.tri(micor)] # combine plot.data1[i,20:29] &lt;- outr } obs.mat &lt;- matrix(cor(dat)[lower.tri(cor(dat))],byrow=T, ncol=10,nrow=nrow(plot.data)) colnames(obs.mat) &lt;- c(&quot;ObsR12&quot;, &quot;ObsR13&quot;, &quot;ObsR14&quot;, &quot;ObsR15&quot;, &quot;ObsR23&quot;, &quot;ObsR24&quot;, &quot;ObsR25&quot;, &quot;ObsR34&quot;,&quot;ObsR35&quot;, &quot;ObsR45&quot;) plot.data1 &lt;- cbind(plot.data1,obs.mat) theme_set(theme_classic()) t1 &lt;- grid::textGrob(&#39;PI&#39;) t2 &lt;- grid::textGrob(&#39;AD&#39;) t3 &lt;- grid::textGrob(&#39;IGC&#39;) t4 &lt;- grid::textGrob(&#39;FI&#39;) t5 &lt;- grid::textGrob(&#39;FC&#39;) p12 &lt;- ggplot(plot.data1) + geom_density(aes(x=r12))+ geom_vline(aes(xintercept = ObsR12), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p13 &lt;- ggplot(plot.data1) + geom_density(aes(x=r13))+ geom_vline(aes(xintercept = ObsR13), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p14 &lt;- ggplot(plot.data1) + geom_density(aes(x=r14))+ geom_vline(aes(xintercept = ObsR14), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p15 &lt;- ggplot(plot.data1) + geom_density(aes(x=r15))+ geom_vline(aes(xintercept = ObsR15), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p23 &lt;- ggplot(plot.data1) + geom_density(aes(x=r23))+ geom_vline(aes(xintercept = ObsR23), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p24 &lt;- ggplot(plot.data1) + geom_density(aes(x=r24))+ geom_vline(aes(xintercept = ObsR24), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p25 &lt;- ggplot(plot.data1) + geom_density(aes(x=r25))+ geom_vline(aes(xintercept = ObsR25), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p34 &lt;- ggplot(plot.data1) + geom_density(aes(x=r34))+ geom_vline(aes(xintercept = ObsR34), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p35 &lt;- ggplot(plot.data1) + geom_density(aes(x=r35))+ geom_vline(aes(xintercept = ObsR35), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p45 &lt;- ggplot(plot.data1) + geom_density(aes(x=r45))+ geom_vline(aes(xintercept = ObsR45), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) layout &lt;- &#39; A#### BC### DEF## GHIJ# KLMNO &#39; wrap_plots(A=t1,C=t2,F=t3,J=t4,O=t5, B=p12,D=p13,G=p14,K=p15, E=p23,H=p24,L=p25, I=p34,M=p35, N=p45,design = layout) 10.2.2 PPD SRMR # model code jags.model.cfa &lt;- function(){ ######################################## # Specify the factor analysis measurement # model for the observables ######################################## for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # Posterior Predictive Distribution of x # needed for SRMR # set mean to 0 # x.ppd[i,j] ~ dnorm(0, inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;)# &quot;Sigma&quot;, # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, #inits=start_values, parameters.to.save = param_save, n.iter=10000, n.burnin = 5000, n.chains = 1, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpUNZc83/model3bfc6075631.txt&quot;, fit using jags, ## 1 chains, each with 10000 iterations (first 5000 discarded) ## n.sims = 5000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.728 0.045 0.643 0.696 0.727 0.757 0.819 ## lambda[3] 0.419 0.037 0.348 0.395 0.419 0.444 0.492 ## lambda[4] 1.051 0.066 0.927 1.006 1.049 1.096 1.183 ## lambda[5] 0.986 0.059 0.873 0.945 0.985 1.025 1.107 ## phi 0.436 0.043 0.359 0.406 0.433 0.462 0.527 ## psi[1] 0.373 0.028 0.320 0.354 0.372 0.391 0.433 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.192 0.211 ## psi[3] 0.180 0.012 0.158 0.171 0.179 0.188 0.205 ## psi[4] 0.377 0.030 0.321 0.356 0.375 0.396 0.437 ## psi[5] 0.265 0.022 0.225 0.250 0.264 0.279 0.311 ## tau[1] 3.332 0.040 3.254 3.305 3.332 3.360 3.411 ## tau[2] 3.898 0.029 3.843 3.878 3.898 3.917 3.953 ## tau[3] 4.596 0.023 4.551 4.581 4.596 4.611 4.640 ## tau[4] 3.033 0.042 2.951 3.005 3.033 3.062 3.113 ## tau[5] 3.712 0.037 3.640 3.688 3.712 3.737 3.786 ## deviance 3379.601 42.607 3298.725 3350.919 3379.051 3407.511 3466.042 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 907.7 and DIC = 4287.3 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit)[[1]] # function for estimating SRMR SRMR.function &lt;- function(data.cov.matrix, mod.imp.cov.matrix){ J=nrow(data.cov.matrix) temp &lt;- matrix(NA, nrow=J, ncol=J) for(j in 1:J){ for(jprime in 1:j){ temp[j, jprime] &lt;- ((data.cov.matrix[j, jprime] - mod.imp.cov.matrix[j, jprime])/(data.cov.matrix[j, j] * data.cov.matrix[jprime, jprime]))^2 } } SRMR &lt;- sqrt((2*sum(temp,na.rm=TRUE))/(J*(J+1))) SRMR } # set up the parameters for # (1) model implied covariance # (2) PPD of x/covariance iter &lt;- nrow(jags.mcmc) srmr.realized &lt;- rep(NA, iter) srmr.ppd &lt;- rep(NA, iter) srmr.rpv &lt;- rep(NA, iter) jags.mcmc &lt;- cbind(jags.mcmc, srmr.realized, srmr.ppd, srmr.rpv) N &lt;- 500; J &lt;- 5; M &lt;- 1 cov.x &lt;- cov(mydata$x) i &lt;- 1 for(i in 1:iter){ # set up parameters x &lt;- jags.mcmc[i,] lambda &lt;- matrix(x[2:6], ncol=M, nrow=J) phi &lt;- matrix(x[7], ncol=M, nrow=M) psi &lt;- diag(x[8:12], ncol=J, nrow=J) # estimate model implied covariance matrix cov.imp &lt;- lambda%*%phi%*%t(lambda) + psi # get posterior predicted observed x x.ppd &lt;- mvtnorm::rmvnorm(N, mean=rep(0, J), sigma=cov.imp) # compute posterior predictied covariance matrix cov.ppd &lt;- cov(x.ppd) # estimate SRMR values jags.mcmc[i,18] &lt;- SRMR.function(cov.x, cov.imp)# srmr realized jags.mcmc[i,19] &lt;- SRMR.function(cov.ppd, cov.imp)# srmr ppd # posterior predicted p-value of realized SRMR being &lt;= 0.08. jags.mcmc[i,20] &lt;- ifelse(jags.mcmc[i,18]&lt;=0.08, 1, 0) } plot.dat &lt;- as.data.frame(jags.mcmc) p1 &lt;- ggplot(plot.dat, aes(x=srmr.realized, y=srmr.ppd))+ geom_point()+ geom_abline(slope=1, intercept = 0)+ lims(x=c(0,1),y=c(0,1))+ labs(x=&quot;Realized SRMR&quot;,y=&quot;Posterior Predicted SRMR&quot;) + theme_bw()+theme(panel.grid = element_blank()) p2 &lt;- ggplot(plot.dat, aes(x=srmr.realized))+ geom_density()+ lims(x=c(0,1))+ labs(x=&quot;Realized SRMR&quot;, y=NULL) + annotate(&quot;text&quot;, x = 0.75, y = 3, label = paste0(&quot;Pr(SRMR &lt;= 0.08)= &quot;, round(mean(plot.dat$srmr.rpv), 2))) + theme_bw()+theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) p1 + p2 "],["model-comparison.html", "10.3 Model Comparison", " 10.3 Model Comparison Bayes Factors (BF) Conditional predictive ordinate (CPO) Information criteria Entropy "],["item-response-theory.html", "Chapter 11 Item Response Theory", " Chapter 11 Item Response Theory Item response theory (IRT) builds models for item (stimuli that the measures collected) based on two broad classes of models Models for dichotomous (binary, 0/1) items and Models for polytomous (multi-category) items. "],["irt-models-for-dichotomous-data.html", "11.1 IRT Models for Dichotomous Data", " 11.1 IRT Models for Dichotomous Data First, for conventional dichotomous observed variables, an IRT model can be generally specified as follows. Let \\(x_{ij}\\) be the observed value from respondent \\(i\\) on observable (item) \\(j\\). When \\(x\\) is binary, the observed value can be \\(0\\) or \\(1\\). Some common IRT models for binary observed variables can be expressed as a version of \\[p(x_{ij} = 1 \\mid \\theta_i, d_j, a_j, c_j) = c_j + (1-c_j)F(a_j \\theta_i + d_j),\\] where, \\(\\theta_i\\) is the magnitude of the latent variable that individual \\(i\\) possesses. In educational measurement, \\(\\theta_i\\) commonly represents proficiency so that a higher \\(\\theta_i\\) means that individual has more of the trait, \\(d_j\\) is the item location or difficulty parameter. \\(d_j\\) is commonly transformed to be \\(d_j = -a_jb_j\\) so that the location parameter is easier to interpret in relation to the latent trait \\(\\theta_i\\), \\(a_j\\) is the item slope or discrimination parameter, \\(c_j\\) is the item lower asymptote or guessing parameter, \\(F(.)\\) is the link function to be specified that determines the form of the transformation between latent trait and the item response probability. The link is common chosen to be either the logistic link or the normal-ogive link. Common IRT models are the 1-PL, or one-parameter logistic model, which only uses one measurement parameter \\(d_j\\) per item, 2-PL, or two-parameter logistic model, which uses two measurement model parameters \\(a_j\\) and \\(d_j\\) per item, 3-PL, or three-parameter logistic model, which uses all three parameters as shown above. other models are also possible for binary item response formats but are omitted here. The above describes the functional form used to model why individual may have a greater or lesser likelihood of endorsing an item (have a \\(1\\) as a measure). We use the above model as the basis for defining the conditional probability of any response given the values of the parameters. The conditional probability is then commonly used as part of a marginal maximum likelihood (MML) approach to finding parameters values for the measurement model which maximize the likelihood. However, given that the values of the latent variable \\(\\theta_i\\) are also unknown, the distribution of \\(\\theta_i\\) is marginalized out of the likelihood function. However, in the Bayesian formulation, we can side-step some of these issues by the use of prior distributions. Starting with the general form of the likelihood function \\[p(\\mathbf{x}\\mid \\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = \\prod_{i=1}^np(\\mathbf{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{i=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\] where \\[x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j \\sim \\mathrm{Bernoulli}[p(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j)].\\] Developing a joint prior distribution for \\(p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega})\\) is not straightforward given the high dimensional aspect of the components. But, a common assumption is that the distribution for the latent variables (\\(\\boldsymbol{\\theta}\\)) is independent of the distribution for the measurement model parameters (\\(\\boldsymbol{\\omega}\\)). That is, we can separate the problem into independent priors \\[p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = p(\\boldsymbol{\\theta})p(\\boldsymbol{\\omega}).\\] For the latent variables, the prior distributuion is generally build by assuming that all individuals are also independent. The independence of observations leads to a joint prior that is a product of priors with a common distribution, \\[p(\\boldsymbol{\\theta}) = \\prod_{i=1}^np(\\theta_i\\mid \\boldsymbol{\\theta}_p),\\] where \\(\\boldsymbol{\\theta}_p\\) are the hyperparameters governing the common prior for the latent variable distribution. A common choice is that \\(\\theta_i \\sim \\mathrm{Normal}(\\mu_{\\theta} = 0, \\sigma^2_{\\theta}=1)\\) because the distribution is arbitrary. For the measurement model parameters, a bit more complex specification is generally needed. One simple approach would be to invoke an exchangeability assumption among items and among item parameters. This would essentially make all priors independent and simplify the specification to product of univariate priors over all measurement model parameters \\[p(\\boldsymbol{\\omega}) = \\prod_{j=1}^Jp(\\boldsymbol{\\omega}_j)=\\prod_{j=1}^Jp(d_j)p(a_j)p(c_j).\\] For for location parameter (\\(d_j\\)), a common prior distribution is an unbounded normal distribution. Because, the location can take on any value within the range of the latent variable which is also technically unbounded so we let \\[d_j \\sim \\mathrm{Normal}(\\mu_{d},\\sigma^2_d).\\] The choice of hyperparameters can be guided by prior research or set to a common relative diffuse value for all items such as \\(\\mu_{d}=0,\\sigma^2_d=10\\). The discrimination parameter governs the strength of the relationship between the latent variable and the probability of endorsing the item. This is similar in flavor to a factor loading in CFA. An issue with specifying a prior for the discrimination parameter is the indeterminacy with respect the the orientation of the latent variable. In CFA, we resolved the orientation indeterminacy issue by fixing one factor loading to 1. In IRT, we can do so by constraining the possible values of the discrimination parameters to be strictly positive. This forces each item to have the meaning of higher values on the latent variable directly (or at least proportionally) increase the probability of endorsing the item. We achieve this by using a prior such as \\[a_j \\sim \\mathrm{Normal}^{+}(\\mu_a,\\sigma^2_a).\\] The term \\(\\mathrm{Normal}^{+}(.)\\) means that the normal distribution is truncated at 0 so that only positive values are possible. Lastly, the guessing parameter \\(c_j\\) takes on values between \\([0,1]\\). A common choice for parameters bounded between 0 and 1 is a Beta prior, that is \\[c_j \\sim \\mathrm{Beta}(\\alpha_c, \\beta_c).\\] The hyperparameters \\(\\alpha_c\\) and \\(\\beta_c\\) determine the shape of the beta prior and affect the likelihood and magnitude of guessing parameters. "],["pl-lsat-example.html", "11.2 3-PL LSAT Example", " 11.2 3-PL LSAT Example In the Law School Admission Test (LSAT) example (p. 263-271), the data are from 1000 examinees responding to five items which is just a subset of the LSAT. We hypothesize that only one underlying latent variable is measured by these items. But that guessing is also plausible. The full 3-PL model we will use can be described in an equation as \\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{a}, \\boldsymbol{c} \\mid \\mathbf{x}) \\propto \\prod_{i=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)p(\\theta_i)p(d_j)p(a_j)p(c_j),\\] where \\[\\begin{align*} x_{ij}\\mid\\theta_i\\mid\\theta_i, d_j, a_j, c_j &amp;\\sim \\mathrm{Bernoulli}[p(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)],\\ \\mathrm{for}\\ i=1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\ p(\\theta_i\\mid\\theta_i, d_j, a_j, c_j) &amp;= c_j + (1-c_j)\\Phi(a_j\\theta_j + d_j),\\ \\mathrm{for}\\ i=1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\ \\theta_i &amp;\\sim \\mathrm{Normal}(0,1),\\ \\mathrm{for}\\ i = 1, \\cdots, 1000;\\\\ d_j &amp;\\sim \\mathrm{Normal}(0, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 5;\\\\ a_j &amp;\\sim \\mathrm{Normal}^{+}(1, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 5;\\\\ c_j &amp;\\sim \\mathrm{Beta}(5, 17),\\ \\mathrm{for}\\ j=1, \\cdots, 5. \\end{align*}\\] The above model can illustrated in a DAG as shown below. Figure 11.1: DAG for 3-PL IRT model for LSAT Example The path diagram for an IRT is essentially identical to the path diagram for a CFA model. This fact highlights an important feature of IRT/CFA in that the major conceptual difference between these approaches to is how we define the link between the latent variable the observed items. Figure 11.2: Path diagram for 3-PL IRT model For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 11.3: Model specification diagram for the 3-PL IRT model 11.2.1 LSAT Example Using JAGS jags.model.lsat &lt;- function(){ ######################################### # Specify the item response measurement model for the observables ######################################### for (i in 1:n){ for(j in 1:J){ P[i,j] &lt;- c[j]+(1-c[j])*phi(a[j]*theta[i]+d[j]) # 3P-NO expression x[i,j] ~ dbern(P[i,j]) # distribution for each observable } } ########################################## # Specify the (prior) distribution for the latent variables ########################################## for (i in 1:n){ theta[i] ~ dnorm(0, 1) # distribution for the latent variables } ########################################## # Specify the prior distribution for the measurement model parameters ########################################## for(j in 1:J){ d[j] ~ dnorm(0, .5) # Locations for observables a[j] ~ dnorm(1, .5); I(0,) # Discriminations for observables c[j] ~ dbeta(5,17) # Lower asymptotes for observables } } # closes the model # initial values start_values &lt;- list( list(&quot;d&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;a&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;c&quot;=c(0.20, 0.20, 0.20, 0.20, 0.20)), list(&quot;d&quot;=c(-3.00, -3.00, -3.00, -3.00, -3.00), &quot;a&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;c&quot;=c(0.50, 0.50, 0.50, 0.50, 0.50)), list(&quot;d&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;a&quot;=c(0.1, 0.1, 0.1, 0.1, 0.1), &quot;c&quot;=c(0.05, 0.05, 0.05, 0.05, 0.05)) ) # vector of all parameters to save param_save &lt;- c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;, &quot;theta&quot;) # dataset dat &lt;- read.table(&quot;data/LSAT.dat&quot;, header=T) mydata &lt;- list( n = nrow(dat), J = ncol(dat), x = as.matrix(dat) ) # fit model fit &lt;- jags( model.file=jags.model.lsat, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=20000, n.burnin = 6000, n.chains = 3, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 5000 ## Unobserved stochastic nodes: 1015 ## Total graph size: 31027 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmporGNNv/model41c4569b1970.txt&quot;, fit using jags, ## 3 chains, each with 20000 iterations (first 6000 discarded), n.thin = 14 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.471 0.181 0.175 0.346 0.450 0.569 0.881 1.002 1800 ## a[2] 0.584 0.230 0.289 0.431 0.537 0.674 1.161 1.007 300 ## a[3] 1.240 0.719 0.417 0.696 1.039 1.605 3.093 1.004 600 ## a[4] 0.508 0.214 0.234 0.379 0.473 0.589 1.023 1.001 3000 ## a[5] 0.417 0.154 0.154 0.313 0.404 0.505 0.765 1.001 3000 ## c[1] 0.239 0.092 0.086 0.170 0.232 0.298 0.435 1.004 670 ## c[2] 0.254 0.097 0.093 0.184 0.243 0.316 0.461 1.003 990 ## c[3] 0.262 0.080 0.102 0.202 0.266 0.321 0.407 1.006 450 ## c[4] 0.253 0.098 0.092 0.180 0.244 0.313 0.477 1.001 3000 ## c[5] 0.241 0.090 0.087 0.174 0.232 0.300 0.429 1.001 2600 ## d[1] 1.419 0.138 1.171 1.330 1.413 1.500 1.714 1.001 3000 ## d[2] 0.294 0.189 -0.185 0.214 0.329 0.419 0.555 1.009 770 ## d[3] -0.537 0.455 -1.688 -0.776 -0.414 -0.199 0.032 1.014 210 ## d[4] 0.512 0.160 0.121 0.435 0.538 0.617 0.746 1.001 3000 ## d[5] 1.024 0.117 0.787 0.955 1.030 1.099 1.238 1.002 1100 ## theta[1] -1.659 0.776 -3.317 -2.159 -1.631 -1.129 -0.190 1.001 3000 ## theta[2] -1.615 0.785 -3.232 -2.145 -1.602 -1.092 -0.115 1.001 3000 ## theta[3] -1.643 0.767 -3.199 -2.162 -1.619 -1.102 -0.225 1.001 2100 ## theta[4] -1.301 0.764 -2.862 -1.833 -1.289 -0.756 0.134 1.001 2800 ## theta[5] -1.285 0.780 -2.856 -1.790 -1.266 -0.733 0.133 1.002 1600 ## theta[6] -1.298 0.776 -2.833 -1.809 -1.276 -0.763 0.159 1.002 1300 ## theta[7] -1.299 0.768 -2.835 -1.799 -1.292 -0.781 0.158 1.002 1600 ## theta[8] -1.315 0.759 -2.913 -1.812 -1.282 -0.771 0.061 1.002 1900 ## theta[9] -1.311 0.776 -2.881 -1.838 -1.295 -0.794 0.141 1.001 3000 ## theta[10] -1.310 0.795 -2.922 -1.834 -1.290 -0.739 0.207 1.003 860 ## theta[11] -1.299 0.788 -2.898 -1.824 -1.275 -0.741 0.167 1.001 3000 ## theta[12] -0.921 0.775 -2.465 -1.419 -0.925 -0.385 0.575 1.002 1200 ## theta[13] -0.971 0.777 -2.592 -1.477 -0.948 -0.448 0.478 1.001 2500 ## theta[14] -0.939 0.773 -2.493 -1.474 -0.906 -0.398 0.477 1.001 3000 ## theta[15] -0.935 0.774 -2.495 -1.444 -0.923 -0.392 0.526 1.001 3000 ## theta[16] -0.954 0.790 -2.490 -1.494 -0.938 -0.405 0.542 1.001 3000 ## theta[17] -0.935 0.766 -2.449 -1.452 -0.924 -0.415 0.543 1.001 3000 ## theta[18] -0.937 0.775 -2.477 -1.444 -0.935 -0.407 0.554 1.001 3000 ## theta[19] -0.951 0.789 -2.576 -1.460 -0.926 -0.406 0.528 1.001 3000 ## theta[20] -0.950 0.786 -2.499 -1.472 -0.925 -0.395 0.542 1.003 820 ## theta[21] -0.949 0.774 -2.504 -1.468 -0.949 -0.425 0.496 1.001 3000 ## theta[22] -0.950 0.762 -2.510 -1.445 -0.925 -0.445 0.481 1.001 3000 ## theta[23] -1.447 0.868 -3.105 -2.036 -1.448 -0.856 0.257 1.002 1600 ## theta[24] -0.983 0.877 -2.668 -1.578 -0.999 -0.377 0.694 1.002 1700 ## theta[25] -1.011 0.867 -2.731 -1.595 -1.021 -0.419 0.683 1.001 3000 ## theta[26] -1.052 0.914 -2.859 -1.671 -1.034 -0.444 0.711 1.002 1300 ## theta[27] -1.012 0.894 -2.724 -1.617 -1.009 -0.391 0.718 1.002 1700 ## theta[28] -0.549 0.881 -2.277 -1.150 -0.559 0.072 1.172 1.001 3000 ## theta[29] -0.551 0.877 -2.240 -1.147 -0.533 0.051 1.109 1.001 2300 ## theta[30] -0.541 0.908 -2.325 -1.155 -0.518 0.079 1.182 1.002 1200 ## theta[31] -0.557 0.873 -2.290 -1.135 -0.557 0.022 1.106 1.001 3000 ## theta[32] -1.318 0.777 -2.916 -1.813 -1.300 -0.793 0.145 1.001 3000 ## theta[33] -0.937 0.798 -2.557 -1.445 -0.929 -0.370 0.590 1.001 2100 ## theta[34] -0.947 0.791 -2.522 -1.495 -0.919 -0.394 0.527 1.002 1900 ## theta[35] -0.933 0.776 -2.469 -1.451 -0.908 -0.382 0.476 1.001 2000 ## theta[36] -0.952 0.782 -2.483 -1.469 -0.953 -0.411 0.554 1.002 1400 ## theta[37] -0.965 0.796 -2.580 -1.493 -0.943 -0.411 0.527 1.001 3000 ## theta[38] -0.933 0.791 -2.516 -1.456 -0.906 -0.400 0.572 1.002 1300 ## theta[39] -0.951 0.799 -2.522 -1.482 -0.941 -0.394 0.572 1.001 3000 ## theta[40] -0.948 0.798 -2.551 -1.479 -0.917 -0.422 0.581 1.001 3000 ## theta[41] -0.556 0.783 -2.134 -1.064 -0.527 -0.014 0.859 1.001 2900 ## theta[42] -0.596 0.791 -2.170 -1.118 -0.579 -0.071 0.931 1.001 3000 ## theta[43] -0.583 0.799 -2.190 -1.121 -0.568 -0.043 0.995 1.002 1400 ## theta[44] -0.562 0.784 -2.160 -1.063 -0.538 -0.032 0.893 1.001 3000 ## theta[45] -0.545 0.793 -2.197 -1.058 -0.520 -0.018 0.972 1.002 1900 ## theta[46] -0.546 0.780 -2.121 -1.052 -0.536 -0.027 0.979 1.002 1500 ## theta[47] -0.581 0.799 -2.214 -1.083 -0.555 -0.044 0.957 1.001 3000 ## theta[48] -0.572 0.784 -2.208 -1.068 -0.532 -0.052 0.898 1.001 2400 ## theta[49] -0.596 0.790 -2.233 -1.108 -0.568 -0.072 0.854 1.001 3000 ## theta[50] -0.538 0.797 -2.207 -1.067 -0.514 0.005 0.963 1.001 3000 ## theta[51] -0.572 0.785 -2.189 -1.048 -0.554 -0.064 0.925 1.002 1500 ## theta[52] -0.564 0.781 -2.135 -1.074 -0.550 -0.028 0.903 1.002 1700 ## theta[53] -0.581 0.777 -2.216 -1.104 -0.541 -0.052 0.881 1.001 3000 ## theta[54] -0.555 0.784 -2.121 -1.061 -0.524 -0.001 0.900 1.001 2600 ## theta[55] -0.534 0.801 -2.176 -1.052 -0.506 0.014 0.942 1.001 3000 ## theta[56] -0.573 0.794 -2.165 -1.097 -0.550 -0.033 0.896 1.001 3000 ## theta[57] -0.536 0.889 -2.381 -1.145 -0.527 0.078 1.163 1.001 3000 ## theta[58] -0.532 0.885 -2.344 -1.126 -0.522 0.075 1.161 1.001 3000 ## theta[59] -0.534 0.897 -2.333 -1.128 -0.514 0.073 1.204 1.001 3000 ## theta[60] -0.532 0.916 -2.318 -1.160 -0.531 0.114 1.212 1.001 2500 ## theta[61] -0.516 0.901 -2.262 -1.147 -0.505 0.102 1.194 1.001 3000 ## theta[62] -0.035 0.908 -1.810 -0.652 -0.032 0.590 1.710 1.001 3000 ## theta[63] -0.045 0.905 -1.896 -0.642 -0.025 0.556 1.729 1.001 3000 ## theta[64] -0.032 0.913 -1.867 -0.665 -0.010 0.602 1.706 1.001 3000 ## theta[65] -0.031 0.908 -1.850 -0.656 -0.015 0.605 1.684 1.001 3000 ## theta[66] 0.006 0.903 -1.794 -0.573 0.027 0.611 1.707 1.001 3000 ## theta[67] -0.029 0.894 -1.800 -0.610 -0.011 0.565 1.715 1.003 850 ## theta[68] -0.035 0.914 -1.893 -0.664 -0.001 0.589 1.708 1.002 1900 ## theta[69] 0.008 0.923 -1.807 -0.620 0.025 0.655 1.716 1.001 3000 ## theta[70] -0.042 0.891 -1.787 -0.627 -0.027 0.564 1.668 1.001 3000 ## theta[71] -0.030 0.896 -1.809 -0.627 0.017 0.578 1.693 1.001 3000 ## theta[72] -0.046 0.914 -1.900 -0.642 -0.025 0.573 1.703 1.001 3000 ## theta[73] -0.017 0.909 -1.812 -0.629 0.034 0.611 1.647 1.001 3000 ## theta[74] -0.041 0.923 -1.982 -0.629 -0.018 0.604 1.647 1.001 2700 ## theta[75] -0.036 0.927 -1.937 -0.653 0.011 0.603 1.687 1.001 3000 ## theta[76] -0.008 0.912 -1.884 -0.616 0.037 0.600 1.675 1.001 3000 ## theta[77] -1.206 0.744 -2.692 -1.703 -1.192 -0.687 0.210 1.001 2600 ## theta[78] -1.202 0.775 -2.741 -1.711 -1.194 -0.673 0.265 1.002 1800 ## theta[79] -1.206 0.772 -2.758 -1.710 -1.191 -0.661 0.257 1.001 3000 ## theta[80] -1.186 0.769 -2.753 -1.704 -1.164 -0.657 0.253 1.001 3000 ## theta[81] -1.205 0.788 -2.817 -1.742 -1.193 -0.651 0.266 1.001 3000 ## theta[82] -1.187 0.786 -2.797 -1.707 -1.155 -0.637 0.234 1.003 740 ## theta[83] -1.197 0.757 -2.726 -1.703 -1.182 -0.683 0.269 1.001 3000 ## theta[84] -1.181 0.751 -2.724 -1.668 -1.155 -0.659 0.207 1.001 3000 ## theta[85] -1.182 0.780 -2.734 -1.710 -1.168 -0.648 0.313 1.001 3000 ## theta[86] -1.226 0.766 -2.768 -1.740 -1.185 -0.677 0.179 1.001 2700 ## theta[87] -0.847 0.752 -2.397 -1.334 -0.846 -0.325 0.585 1.001 3000 ## theta[88] -0.858 0.750 -2.370 -1.349 -0.846 -0.324 0.552 1.001 3000 ## theta[89] -0.832 0.757 -2.395 -1.319 -0.805 -0.302 0.558 1.002 1900 ## theta[90] -0.832 0.753 -2.352 -1.337 -0.793 -0.315 0.576 1.001 3000 ## theta[91] -0.832 0.752 -2.339 -1.308 -0.814 -0.323 0.582 1.001 3000 ## theta[92] -0.839 0.758 -2.387 -1.331 -0.836 -0.334 0.632 1.002 1600 ## theta[93] -0.847 0.772 -2.428 -1.347 -0.833 -0.315 0.595 1.001 3000 ## theta[94] -0.835 0.765 -2.408 -1.319 -0.825 -0.314 0.629 1.001 3000 ## theta[95] -0.829 0.762 -2.403 -1.331 -0.805 -0.316 0.604 1.001 2600 ## theta[96] -0.845 0.746 -2.369 -1.329 -0.839 -0.339 0.529 1.002 2000 ## [ reached getOption(&quot;max.print&quot;) -- omitted 905 rows ] ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 2749.6 and DIC = 7270.9 ## DIC is an estimate of expected predictive error (lower deviance is better). round(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% &quot;theta&quot;, ], 3) ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.471 0.181 0.175 0.346 0.450 0.569 0.881 1.002 1800 ## a[2] 0.584 0.230 0.289 0.431 0.537 0.674 1.161 1.007 300 ## a[3] 1.240 0.719 0.417 0.696 1.039 1.605 3.093 1.004 600 ## a[4] 0.508 0.214 0.234 0.379 0.473 0.589 1.023 1.001 3000 ## a[5] 0.417 0.154 0.154 0.313 0.404 0.505 0.765 1.001 3000 ## c[1] 0.239 0.092 0.086 0.170 0.232 0.298 0.435 1.004 670 ## c[2] 0.254 0.097 0.093 0.184 0.243 0.316 0.461 1.003 990 ## c[3] 0.262 0.080 0.102 0.202 0.266 0.321 0.407 1.006 450 ## c[4] 0.253 0.098 0.092 0.180 0.244 0.313 0.477 1.001 3000 ## c[5] 0.241 0.090 0.087 0.174 0.232 0.300 0.429 1.001 2600 ## d[1] 1.419 0.138 1.171 1.330 1.413 1.500 1.714 1.001 3000 ## d[2] 0.294 0.189 -0.185 0.214 0.329 0.419 0.555 1.009 770 ## d[3] -0.537 0.455 -1.688 -0.776 -0.414 -0.199 0.032 1.014 210 ## d[4] 0.512 0.160 0.121 0.435 0.538 0.617 0.746 1.001 3000 ## d[5] 1.024 0.117 0.787 0.955 1.030 1.099 1.238 1.002 1100 ## deviance 4521.341 74.150 4359.677 4477.858 4525.361 4572.117 4651.660 1.002 3000 # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) # the below two plots are too big to be useful given the 1000 observations. #R2jags::traceplot(jags.mcmc) # gelman-rubin-brook #gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;d[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;a[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;c[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) + plot_title 11.2.1.1 Posterior Predicted Distributions Here, we want to compare the observed and expected posterior predicted distributions. Statistical functions of interest are the (1) standardized model-based covariance (SMBC) and (2) the standardized generalized discrepancy measure (SGDDM). For (1), the SMBC is \\[SMBC_{jj^\\prime}=\\frac{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))^2}\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}}\\] In R, the functions below can be used to compute these qualtities. calculate.SGDDM &lt;- function(data.matrix, expected.value.matrix){ J.local = ncol(data.matrix) SMBC.matrix &lt;- calculate.SMBC.matrix(data.matrix, expected.value.matrix) SGDDM = sum(abs((lower.tri(SMBC.matrix, diag=FALSE))*SMBC.matrix))/((J.local*(J.local-1))/2) SGDDM } # closes calculate.SGDDM calculate.SMBC.matrix &lt;- function(data.matrix, expected.value.matrix){ N.local &lt;- nrow(data.matrix) MBC.matrix &lt;- (t(data.matrix-expected.value.matrix) %*% (data.matrix-expected.value.matrix))/N.local MBStddevs.matrix &lt;- diag(sqrt(diag(MBC.matrix))) #SMBC.matrix &lt;- solve(MBStddevs.matrix) %*% MBC.matrix %*% solve(MBStddevs.matrix) J.local &lt;- ncol(data.matrix) SMBC.matrix &lt;- matrix(NA, nrow=J.local, ncol=J.local) for(j in 1:J.local){ for(jj in 1:J.local){ SMBC.matrix[j,jj] &lt;- MBC.matrix[j,jj]/(MBStddevs.matrix[j,j]*MBStddevs.matrix[jj,jj]) } } SMBC.matrix } # closes calculate.MBC.matrix Next, we will use the functions above among other basic data wrangling to construct a full posterior predictive distribution analysis to probe our resulting posterior. # Data wrangle the results/posterior draws for use datv1 &lt;- plot.data %&gt;% pivot_longer( cols = `a[1]`:`a[5]`, values_to = &quot;a&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, a) datv2 &lt;- plot.data %&gt;% pivot_longer( cols = `c[1]`:`c[5]`, values_to = &quot;c&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, c) datv3 &lt;- plot.data %&gt;% pivot_longer( cols = `d[1]`:`d[5]`, values_to = &quot;d&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, d) datv4 &lt;- plot.data %&gt;% pivot_longer( cols = `theta[1]`:`theta[999]`, values_to = &quot;theta&quot;, names_to = &quot;person&quot; ) %&gt;% select(chain, iter, person, theta) dat_long &lt;- full_join(datv1, datv2) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;, &quot;item&quot;) dat_long &lt;- full_join(dat_long, datv3) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;, &quot;item&quot;) dat_long &lt;- full_join(dat_long, datv4) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;) dat1 &lt;- dat dat1$person &lt;- paste0(&quot;theta[&quot;,1:nrow(dat), &quot;]&quot;) datvl &lt;- dat1 %&gt;% pivot_longer( cols=contains(&quot;item&quot;), names_to = &quot;item&quot;, values_to = &quot;x&quot; ) %&gt;% mutate( item = substr(item, 6, 100) ) dat_long &lt;- left_join(dat_long, datvl) ## Joining, by = c(&quot;item&quot;, &quot;person&quot;) # compute expected prob ilogit &lt;- function(x){exp(x)/(1+exp(x))} dat_long &lt;- dat_long %&gt;% as_tibble()%&gt;% mutate( x.exp = c + (1-c)*ilogit(a*(theta - d)), x.dif = x - x.exp ) dat_long$x.ppd &lt;- apply( dat_long, 1, FUN=function(x){ rbern(1, as.numeric(x[10])) } ) itermin &lt;- min(dat_long$iter) # used for subseting # figure 11.4 d &lt;- dat_long %&gt;% group_by(chain, iter, person) %&gt;% summarise(raw.score = sum(x), raw.score.ppd = sum(x.ppd)) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;. You can override using the `.groups` argument. di &lt;- d[ d$chain ==1 &amp; d$iter == itermin, ] %&gt;% group_by(raw.score) %&gt;% summarise(count = n()) dii &lt;- d %&gt;% group_by(chain, iter, raw.score.ppd)%&gt;% summarise(raw.score = n()) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;. You can override using the `.groups` argument. # overall fit of observed scores ggplot()+ geom_boxplot(data=dii, aes(y=raw.score, x= raw.score.ppd, group=raw.score.ppd))+ geom_point(data=di, aes(x=raw.score, y=count), color=&quot;red&quot;, size=2)+ labs(x=&quot;Raw Score&quot;, y=&quot;Number of Examinees&quot;)+ scale_x_continuous(breaks=0:5)+ theme_classic() # by item d &lt;- dat_long %&gt;% group_by(chain, iter, person) %&gt;% mutate(raw.score = sum(x), raw.score.ppd = sum(x.ppd)) di &lt;- d[ d$chain ==1 &amp; d$iter == itermin, ] %&gt;% group_by(raw.score, item) %&gt;% summarise(p.correct = mean(x)) ## `summarise()` has grouped output by &#39;raw.score&#39;. You can override using the `.groups` argument. dii &lt;- d %&gt;% group_by(chain, iter, raw.score.ppd, item)%&gt;% summarise(p.correct = mean(x.ppd)) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;, &#39;raw.score.ppd&#39;. You can override using the `.groups` argument. ggplot()+ geom_boxplot(data=dii, aes(y= p.correct, x= raw.score.ppd, group=raw.score.ppd))+ geom_point(data=di, aes(x=raw.score, y=p.correct), color=&quot;red&quot;, size=2)+ facet_wrap(.~item)+ labs(x=&quot;Raw Score&quot;, y=&quot;Number of Examinees&quot;)+ theme_classic() # computing standardized model summary statistics # objects for results J &lt;- 5 n.chain &lt;- 3 n.iters &lt;- length(unique(dat_long$iter)) n.iters.PPMC &lt;- n.iters*n.chain realized.SMBC.array &lt;- array(NA, c(n.iters.PPMC, J, J)) postpred.SMBC.array &lt;- array(NA, c(n.iters.PPMC, J, J)) realized.SGDDM.vector &lt;- array(NA, c(n.iters.PPMC)) postpred.SGDDM.vector &lt;- array(NA, c(n.iters.PPMC)) ii &lt;- i &lt;- c &lt;- 1 # iteration condiitons iter.cond &lt;- unique(dat_long$iter) Xobs &lt;- as.matrix(dat[,-6]) for(i in 1:length(iter.cond)){ for(c in 1:3){ cc &lt;- iter.cond[i] Xexp &lt;- dat_long[dat_long$chain==c &amp; dat_long$iter==cc , ] %&gt;% pivot_wider( id_cols = person, names_from = &quot;item&quot;, values_from = &quot;x.exp&quot;, names_prefix = &quot;item&quot; ) %&gt;% ungroup()%&gt;% select(item1:item5)%&gt;% as.matrix() Xppd &lt;- dat_long[dat_long$chain==c &amp; dat_long$iter==cc , ] %&gt;% pivot_wider( id_cols = person, names_from = &quot;item&quot;, values_from = &quot;x.ppd&quot;, names_prefix = &quot;item&quot; ) %&gt;% ungroup()%&gt;% select(item1:item5)%&gt;% as.matrix() # compute realized values realized.SMBC.array[ii, ,] &lt;- calculate.SMBC.matrix(Xobs, Xexp) realized.SGDDM.vector[ii] &lt;- calculate.SGDDM(Xobs, Xexp) # compute PPD values postpred.SMBC.array[ii, ,] &lt;- calculate.SMBC.matrix(Xppd, Xexp) postpred.SGDDM.vector[ii] &lt;- calculate.SGDDM(Xppd, Xexp) ii &lt;- ii + 1 } } Next, generate plots to help summarize and describe the posterior predictor distributions of these statistics. plot.dat.ppd &lt;- data.frame( real = realized.SGDDM.vector, ppd = postpred.SGDDM.vector ) ggplot(plot.dat.ppd, aes(x=real, y=ppd))+ geom_point()+ geom_abline(intercept = 0, slope=1)+ lims(x=c(0,0.5), y=c(0, 0.5)) # transform smbc into plotable format ddim &lt;- dim(postpred.SMBC.array) plot.dat.ppd &lt;- as.data.frame(matrix(0, nrow=ddim[1]*ddim[2]*ddim[3], ncol=4)) colnames(plot.dat.ppd) &lt;- c(&quot;itemj&quot;, &quot;itemjj&quot;, &quot;real&quot;, &quot;ppd&quot;) ii &lt;- i &lt;- j &lt;- jj &lt;- 1 for(i in 1:ddim[1]){ for(j in 1:ddim[2]){ for(jj in 1:ddim[3]){ plot.dat.ppd[ii, 1] &lt;- j plot.dat.ppd[ii, 2] &lt;- jj plot.dat.ppd[ii, 3] &lt;- realized.SMBC.array[i, j, jj] plot.dat.ppd[ii, 4] &lt;- postpred.SMBC.array[i, j, jj] ii &lt;- ii + 1 } } } plot.dat.ppd &lt;- plot.dat.ppd %&gt;% filter(itemj &lt; itemjj) %&gt;% mutate( cov = paste0(&quot;cov(&quot;, itemj, &quot;, &quot;, itemjj,&quot;)&quot;) ) ggplot(plot.dat.ppd, aes(x=real, y=ppd))+ geom_point(alpha=0.25)+ geom_density2d(adjust=2)+ geom_abline(intercept = 0, slope=1)+ facet_wrap(.~cov)+ lims(x=c(-1,1), y=c(-1,1))+ theme_classic() "],["irt-models-for-polytomous-data.html", "11.3 IRT Models for Polytomous Data", " 11.3 IRT Models for Polytomous Data A commonly used IRT model for polytomous items is the graded response model (GRM). Below is one way of describing the model. Let \\(x_{ij}\\) be the observed response to item \\(j\\) from examinee \\(i\\) that may take on values 1, 2, , \\(K_j\\), where \\(K_j\\) is the number of possible responses/outcomes for item \\(j\\). In many applications, the number of response options is constant across items, though this need not be the case. The GRM using conditional probability statements about the probability of a response being at or above a specific category and obtaining the probability for each category as a difference of two such conditional probabilities. That is \\[P(x_{ij} = k \\mid \\theta_i, \\boldsymbol{d}_j,a_j) = P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j) - P(x_{ij \\geq k+1 \\mid \\theta_i, d_{j(k+1)},a_j),\\] where \\(\\boldsymbol{d}_j\\) is the collection of location/threshold parameters for item \\(j\\). The GRM takes on a structure similar to the 2-PL for any one category \\[P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j)=F(a_j\\theta_i + d_{jk}).\\] The conditional probability of observed responses may be modeled similarly as we have used for dichotomous responses but with a few important differences. The conditional distribution of the data is \\[p(\\boldsymbol{x}\\mid \\boldsymbol{\\theta},\\boldsymbol{\\omega}) = \\prod_{i=1}^np(\\boldsymbol{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{i=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\] where each \\(x_{ij}\\) is specified as a categorical random variable (or multinomial). A categorical random variable is a generalization of the Bernoulli distribution which is defined be the collection of category response probabilities \\[x_{ij} \\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)).\\] The above helps form the likelihood of the observed data. Next, the prior distribution is described because what the structure should be is not necessarily obvious. First, the prior for the latent ability follows the same logic from the dichotomous model. We employ an exchangeability assumption to specify independent priors for each respondent with a normally distribution prior. Next, the measurement model parameters priors are described. We again can assume exchangeability and arrive at a common but independent prior across items, and assume that the priors for the location and discrimination parameters are independent. These assumptions may not be tenable in theory, but they are practically useful. The priors for discrimination stay the same as the dichotomous model. The priors for the location parameters are a bit more involved. For the location parameters, the first location parameter \\(d_{j1}\\) specifies the probability of responding a 1 or greater which is a certainty if they gave a response. Therefore, the probability would be 1. We set \\(d_{j1} = -\\inf\\) and then set a normal prior for \\(d_{j2}\\sim \\mathrm{Normal}(\\mu_{d2},\\sigma^2_{d2})\\). The priors for the remaining location parameters (\\(d_{3}-d_{k}\\)) can be specified as truncated normal distributions. That is, the location of the next threshold is constrained to be larger than the previous threshold and is formally \\[d_{jk} \\sim \\mathrm{Normal}^{&gt;d_{j(k-1)}}(\\mu_{d_k},\\sigma^2_{d_k},\\ \\mathrm{for}\\ k=3, ...,K_j.\\] The posterior distribution for the GRM can be parameterized as follows. The model as described below is very general and can accommodate varying number of thresholds per item but is constrained to only 1 latent factor. \\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{a}\\mid \\mathbf{x}) \\propto \\prod_{i=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j)p(\\theta_i)p(a_j)\\prod_{k=2}^{K_j}p(d_{jk}),\\] where \\[\\begin{align*} x_{ij}\\mid\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;\\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\mathbf{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= \\left(P(x_{ij}=1\\mid\\theta_i, \\boldsymbol{d}_j, a_j), \\cdots, P(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j)\\right),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ P(x_{ij}=k\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq k\\mid\\theta_i,d_{jk}, a_j) - P(x_{ij}\\geq k+1\\mid\\theta_i, d_{j(k+1)}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k = 1,\\cdots,K_j-1;\\\\ P(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq K_j\\mid\\theta_i,d_{jK_j}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ P(x_{ij}\\geq k\\mid\\theta_i, d_{jk}, a_j) &amp;= F(a_j\\theta_j + d_{jk}),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k=2,\\cdots,K_j;\\\\ P(x_{ij}\\geq 1\\mid\\theta_i, d_{j1}, a_j) &amp;= 1,\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\theta_i \\mid \\mu_{\\theta}, \\sigma^2_{\\theta} &amp;\\sim \\mathrm{Normal}(\\mu_{\\theta}, \\sigma^2_{\\theta}),\\ \\mathrm{for}\\ i = 1, \\cdots, n;\\\\ a_j \\mid \\mu_{a}, \\sigma^2_{a} &amp;\\sim \\mathrm{Normal}^{+}(\\mu_{a}, \\sigma^2_{a}),\\ \\mathrm{for}\\ j=1, \\cdots, J;\\\\ d_{j2}\\mid\\mu_{j2}, \\sigma^2_{j2} &amp;\\sim \\mathrm{Normal}(\\mu_{j2}, \\sigma^2_{j2} ),\\ \\mathrm{for}\\ j=1, \\cdots, J;\\ \\mathrm{and}\\\\ d_{jk}\\mid\\mu_{d_{jk}},\\sigma^2_{d_{jk}} &amp;\\sim \\mathrm{Normal}^{&gt;d_{j(k-1)}}(\\mu_{d_{jk}},\\sigma^2_{d_{jk}}),\\ \\mathrm{for}\\ j=1, \\cdots, J,\\ k=3, ...,K_j. \\end{align*}\\] "],["grm-peer-interactions-example.html", "11.4 GRM Peer Interactions Example", " 11.4 GRM Peer Interactions Example The book uses an example of Peer Interactions from 500 responses to seven items. All the responses are coded from 1 to 5 on an agreement Likert-type scale. A DAG for the GRM corresponding to these data is shown below. Figure 11.4: DAG for the for Peer Interactions GRM analysis The path diagram version is substantially simpler and identical to the path diagram for the 3-PL and factor analysis diagrams. Highlighting the similarity in substantive modeling of polytomous items to dichotomous items. Figure 11.5: Path diagram for the Peer Interactions GRM analysis For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 11.6: Model specification diagram for the Peer Interactions GRM analysis 11.4.1 Peer Interactions Example using JAGS "],["final-notes.html", "11.5 Final Notes", " 11.5 Final Notes A fully Bayesian approach to psychometric modeling helps highlight the major similarities between factor analytic frameworks and the item response theory perspective. rm(list=ls()) "],["references.html", "References", " References Kelley, T. L. 1923. Statistical Method. New York, NY: Macmillan. "]]
=======
[["index.html", "Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy Chapter 1 Getting Started/Overview Chapter", " Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy R. Noah Padgett 2021-04-22 Chapter 1 Getting Started/Overview Chapter This online book is meant to serve a computation reference for the text Bayesian Psychometric Modeling by Roy Levy and Robert Mislevy. I hope that having a detailed computation guide using Stan is of interest to someone. To reference the book, I will frequently use BPM as a shorthand for the title of the text. Throughout this book, I have incorporated elaborations on the code to help set up the examples and provide more details when possible. The authors provided an accompanying website, where the examples are shown using WinBUGS and Netica. The later is used in Chapter 14 for estimation of Bayesian networks. I wanted to deeply dive into this text so rerunning all the examples in a different language provides an excellent excuse to do so. Also, since I can go into more detail in this format than they could, I have not restricted myself by cutting short of the analysis. Meaning that I have done my best to fully (or sufficient for the example) analyze the posteriors to see what issues may pop up and how I resolved them. "],["software.html", "1.1 Software", " 1.1 Software We have used R to call Stan. So we have relied on rstan heavily. NEED TO UPDATE - NEEDS A LOT MORE INFORMATION "],["overview-of-assessment-and-psychometric-modeling.html", "1.2 Overview of Assessment and Psychometric Modeling", " 1.2 Overview of Assessment and Psychometric Modeling Chapter 1 of BPM provided an excellent overview of topics related to psychometric modeling. I wanted to highlight some particularly important pieces to keep in mind while modeling. [We] view assessment as an instance of evidentiary reasoning. (p. 3) This idea forms the basis for this text. Providing evidence in support of inferences, claims, decisions, etc. is a major stance of the probabilistic modeling used by Levy and MisLevy. Observed data provide some evidence, but not all data is evidence. This gets to their point that data are grounds to help provide evidence, but they also recognize that evidence does not depend solely on data. What these data represents is a major factor in helping to decide whether evidence has been gathered. Inferences depends on the data and the claim being made. They used excellent examples of the relationships between data, claims, and potential alternative explanations. These examples on pages 5-9 should be read. In summary, the idea is to use our claim to make predictions about what data we should observe. However, we have to use data to make inferences about our claims. The reversal is in line with the relationship between deductive reasoning and inductive reasoning. A models is a simplified version of a real world scenario, wherein [relevant] features of the problem at hand are represented, while [less relevant features] are suppressed. They took that stance that they acknowledge that any model they come up with is wrong, but they aim to develop a useful model to help solve a problem at hand. The models they are developing help describe the world according to the analyst. The according to analyst is important since this implies that the analyst is in control of how the model represents the world. The inclusion of the analyst as an active participant in the model building process instead of a passive observer is a realistic representation of the problem of assessment and modeling building. They highlighed 3 goals of modeling (p. 11) represent the relationships among the relevant entities; provide the machinery for making inferences about what is unknown based on what is known; and offers machanisms for effectively communicating results to take actions in the real world. Probability is interpreted as an approach to describing uncertainty about beliefs. This is called the epistemic interpretation of probability. Traditionally, probability in assessment has been interpreted in the frequency of an event. For example, if you flip a coin 100 times we could use a probability of 0.5 to represents the proportion of times the coin would land on heads. This would lead us to expect the number heads to occur to be approximately 50. In the epistemic interpretation, we could describe that we believe that the coin is fair meaning would would place equal weight to heads and tails when flipped. However, if we had a belief that the coin favors heads we could reflect this in the probability we assign to each event. This epistemic interpretation aligns with (1) above where we aim to provide evidence through assessment. Context of assessment should be incorporated into modeling whenever possible. Context (when, where, how long, how much, with whom, etc.) should be considered (at least) as part of the assessment and included in modeling/decision making. Without such details, the analyst may overlook an important consideration in making a decision. Evidence-Centered Design A framework for describing assessments and the context around the assessment. Three properties of ECD are helps us understand the argumentation behind the use of particular psychometric models; helps us through the assessment development process that might lead to such models; and does not require the use of such models. "],["looking-forward.html", "1.3 Looking Forward", " 1.3 Looking Forward The remainder of this online accompanying text to BPM is organized as follows. Chapters 2-6 round of the Foundational information which includes a introduction of Bayesian inference (Chp 2), a discussion of the conceptual issues in Bayesian inferences (chp 3), a dive into normal distribution models (chp 4), a near 50,000 ft view of estimation with markov chain Monte Carlo (MCMC, chp 5), and introducing notation for regression modeling (chp 6). Next, we turn our attention to the meat of the book with section 2 which is the remainder of the text, chapters 7-14. These chapters move from basic psychometric modeling (chp 7) to classical test theory (chp 8), factor analysis (chp 9), item response theory (chp 11), latent class analysis (chp 13), and networks (chp 14). Other modeling issues and topics are discussed such as model comparison (chp 10) and missing data (chp 12). Throughout all these chapters I will go through all the example analyses using Stan instead of WinBUGS so that potential differences can be compared and discussed. "],["chp2.html", "Chapter 2 Introduction to Bayesian Inference", " Chapter 2 Introduction to Bayesian Inference Chapter 2 is focused on introducing the fundamentals of Bayesian modeling. I will briefly reiterate some of these concepts, but Levy and Mislevy did an excellent job introducing the basic concepts so I defer to them. A few points I would like to highlight are The concept of likelihood is fundamental to Bayesian methods and frequentist methods as well. The likelihood function (denoted \\(p(\\mathbf{x} \\mid \\theta)\\) or equivalently \\(L(\\theta \\mid \\mathbf{x})\\)) is fundamental as this conditional probability describes our beliefs about the data generating process. Another way of thinking about the likelihood function is as the data model. The data model decribes how parameters of interest relates to the observed data. This key concept is used in frequentist methods (e.g., maximum likelihood estimation) to obtain point estimates of model parameters. A fundamental difference between maximum likelihood and Bayesian estimation is how we use the likelihood function to construct interval estimates of parameters (see next point). Interval estimates in Bayesian methods do not rely on the idea of repeated sampling. In frequentist analyses, the construction of interval estimates around maximum likelihood estimators is dependent on utilizing repeated sampling paradigm. The interval estimate around the MLE is referred to the sampling distribution of the parameter estimator. BPM discusses these features of maximum likelihood well on p. 26. In Bayesian methods, an interval estimate is constructed based on the distribution of the parameter and not the parameter estimator. This distinction makes Bayesian intervals based on the likely values of the parameter based on our prior beliefs and observed data. Bayes Theorem Bayes theorem is the underlying engine of all Bayesian methods. We use Bayes theorm to decompose conditional probabilities so that they can work for us. As an analyst, we interested in the plausible values of the parameters based on the observed data. This can be expressed as a conditional probability (\\(p(\\theta \\mid \\mathbf{x})\\)). Bayes theorm states that \\[\\begin{equation} \\begin{split} p(\\theta \\mid \\mathbf{x}) &amp;= \\frac{p(\\mathbf{x}, \\theta)}{p(\\mathbf{x})}\\\\ &amp;= \\frac{p(\\mathbf{x}\\mid \\theta)p(\\theta)}{p(\\mathbf{x})}.\\\\ \\end{split} \\tag{2.1} \\end{equation}\\] The distinction between frequentist and Bayesian approaches is more than treating model parameters as random. A different way to stating the difference between frequentist and Bayesian approaches is based on what is being conditioned on to make inferences. In a classic frequentist hypothesis testing scenario, the model parameters are conditioned on to calculate the probability of the observed data (i.e., \\(\\mathrm{Pr}(data \\mid \\theta)\\)). This implies that the data are treated as random variables, but this does not exclude the fact the \\(\\theta\\) can be a collection of parameters that have random components (e.g., random intercepts in HLM). However, in a Bayesian model, the model parameters are the object of interest and the data are conditioned on (i.e., \\(\\mathrm{Pr}(\\theta \\mid data)\\)). This implies that the data are treated as a fixed entity that is used to construct inferences. This is how BPM related Bayesian inference to inductive reasoning. The inductive reasoning comes from taking observations and trying to making claims about the general. "],["beta-binomial-example.html", "2.1 Beta-binomial Example", " 2.1 Beta-binomial Example Here I go through the the first example from BPM. The example is a relatively simple beta-binomial model. Which is a way of modeling the number of occurrences of a bernoulli process. For example, suppose we were interested in the number of times a coin landed on heads. Here, we have a set number of coin flips (say \\(J\\)) and we are interested in the number of times the coin landed on heads (call this outcome \\(y\\)). We can model this structure letting \\(y\\) be a binomial random variable which we can express this as \\[y\\sim\\mathrm{Binomial}(\\theta, J)\\] where \\(\\theta\\) is the probability of heads on any given coin toss. As part of the Bayesian modeling I need to specify my prior belief as to the likely values of \\(\\theta\\). The probability \\(\\theta\\) lies in the interval \\([0, 1]\\). A nice probability distribution on this range is the beta distribution. That is, I can model my belief as to the likely values of the probability of heads by saying that \\(\\theta\\) is beta distributed which can be expressed as \\[\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\]. The two parameters for the beta distribution are representative of the shape the distribution will take. When \\(\\alpha = \\beta\\) the distribution is symmetrical, and when \\(\\alpha = \\beta=1\\) the beta distribution is flat or uniform over \\([0,1]\\). When a distribution is uniform I mean that all values are equally likely over the range of possible values which can be described as having the belief that all values are equally plausible. This model can be represented in a couple different ways. One way is as a directed acyclic graph (DAG). A DAG representation is very similar to path models in general structural equation modeling. The directed nature of the diagram highlights how observed variables (e.g., \\(y\\)) are modeled by unknown parameters \\(\\theta\\). All observed or explicitly defined variables/values are in rectangles while any latent variable or model parameter are in circles. DAG representation of model for the beta-binomal model is Figure 2.1: Directed Acyclic Graph (DAG) for the beta-binomial model I have given an alternative DAG representation that includes all relevant details. In terms of a DAG, I prefer this representation as all the assumed model components are made explicit. However, in more complex models this approach will likely lead to very dense and possible unuseful representations. Figure 2.2: DAG with explicit representation for all beta-binomial model components Yet another alternative representation is what I call a model specification chart. This takes on a similar feel as a DAG in that the flow of model parameters can be shown, but with the major difference that I use the distributional notation explicitly. Figure 2.3: Model specification diagram for beta-binomial model I will stick with these last two representations as much as possible. 2.1.1 Computation using Stan Now, Im finally getting to the analysis part. I have done my best to be descriptive of what the Stan code represents and how it works (in a general how to use this sense). I highly recommend a look at the example analysis by the development team to help see their approach as well (see here Stan analysis). model_beta_binomial &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y; real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { y ~ binomial(J, theta); theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_binomial, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: db495166b911389af4867f0120ac5e81. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.39 0.52 0.59 0.66 0.79 1553 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:11:47 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. I have downloaded the .bug file from the text website and I will load it into R for viewing. First, lets take a look at the model described by BPM on p. 39. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) } # data statement list(J = 10, y = 7, alpha = 6, beta = 6) Next, we want to use the above model. Using OpensBUGS through R can be a little clunky as I had to create objects with the filepaths of the data and model code then get R to read those in through the function openbugs. Otherwise, the code is similar to style to the code used for calling Stan. # model code model.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Binomial/Binomial Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 3.6 1.1 2.6 2.8 3.2 4.0 6.7 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 4.0 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) ################################# # Prior distribution ################################# theta ~ dbeta(alpha, beta) } # data mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpyqRSeb/model31246f711351.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.589 0.101 0.390 0.519 0.591 0.663 0.781 1.001 2000 ## deviance 3.583 1.091 2.643 2.764 3.193 4.027 6.585 1.001 1900 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.6 and DIC = 4.2 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],["beta-bernoulli-example.html", "2.2 Beta-Bernoulli Example", " 2.2 Beta-Bernoulli Example For this next example, I use the same data as the previous model. But now, instead of treating the individual events as part of a whole and sum over the successes, I will treat the model in a more hierarchical manner. A hierarchical model here simply implies that Ill be using the same probability function for all individual observations. We express this by saying that the observations depend on the index (\\(j=1, 2, ..., J\\)) but that the parameter of interest does not vary across \\(j\\). Two DAG representations similar to the previous examples are shown below. The major difference in these representations from the previous example is the inclusion of a plate that represents the observations depend on the index \\(j\\). Figure 2.4: DAG for the beta-bernoulli model Figure 2.5: DAG with explicit representation for all beta-bernoulli model components In my favor representation, this model can be expressed as Figure 2.6: Model specification diagram for beta-bernoulli model We will use the same \\(\\mathrm{Beta}(\\alpha, \\beta)\\) prior for \\(\\theta\\) as in the previous example. The model code changes to the following, 2.2.1 Computation using Stan model_beta_bernoulli &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y[J]; //declare observations as an integer vector of length J real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { for(j in 1:J){ y[j] ~ bernoulli(theta); } theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,1,1,1), alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_bernoulli, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: 7a99cbb09826cf6efe5d323426433fa9. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.38 0.52 0.59 0.66 0.78 1434 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:17:02 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. First, lets take a look at the model described by BPM on p. 41. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } } # data statement list(J=10, y=c(1,0,1,0,1,1,1,1,0,1), alpha=6, beta=6) The code is similar to style to the code used for calling Stan. However youll notice a difference in how a probability distribution is referenced. # model code model.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Bernoulli/Bernoulli Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 13.2 1.1 12.2 12.3 12.7 13.5 16.3 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 13.6 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) } # data mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,NA,1,1), alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 9 ## Unobserved stochastic nodes: 2 ## Total graph size: 14 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpyqRSeb/model312454083d2a.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.569 0.104 0.359 0.499 0.570 0.640 0.765 1.004 700 ## deviance 12.218 0.970 11.458 11.550 11.852 12.507 14.973 1.003 790 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.5 and DIC = 12.7 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],["conceptual-issues-in-bayesian-inference.html", "Chapter 3 Conceptual Issues in Bayesian Inference", " Chapter 3 Conceptual Issues in Bayesian Inference This chapter was conceptual so there was no code. "],["normal-distribution-models.html", "Chapter 4 Normal Distribution Models", " Chapter 4 Normal Distribution Models This chapter was mainly analytic derivations, but there was one section that did code so I show that in JAGS and Stan. "],["stan-model-for-mean-and-variance-unknown.html", "4.1 Stan Model for mean and variance unknown", " 4.1 Stan Model for mean and variance unknown The model for mean and variance unknown for normal sampling. Figure 4.1: DAG with for mean and variance unknown: Variance parameterization Or, alternatively, Figure 4.2: Model specification diagram for normal model model_normal &lt;- &#39; data { int N; real x[N]; real mu0; real sigma0; real alpha0; real beta0; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { x ~ normal(mu, sigma); mu ~ normal(mu0, sigma0); sigma ~ inv_gamma(alpha0, beta0); } &#39; # data must be in a list mydata &lt;- list( N = 10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92), mu0 = 75, sigma0 = 50, alpha0 = 5, beta0 = 150 ) # start values start_values &lt;- function(){ list(mu=50, sigma=5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_normal, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 682d92f82066fa7e19436da3c3fccc69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu 84.06 0.05 4.81 74.39 81.05 84.09 87.08 93.55 7913 1 ## sigma 14.82 0.04 3.87 9.13 12.08 14.20 16.87 24.03 7794 1 ## lp__ -52.88 0.01 1.07 -55.73 -53.30 -52.57 -52.12 -51.83 5949 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:39:35 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;mu&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;mu&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot posterior &lt;- as.data.frame(posterior) p &lt;- ggplot(posterior, aes(x=mu, y=sigma))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), sd(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(60.01, 120, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sig &lt;- function(x){extraDistr::dinvgamma(x, 5, 150)} x.sig &lt;- seq(0.01, 60, 0.01) prior.sig &lt;- data.frame(sigma=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=posterior, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=posterior, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sigma, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],["jags-model-for-mean-and-variance-unknown-precision-parameterization.html", "4.2 JAGS Model for mean and variance unknown (precision parameterization)", " 4.2 JAGS Model for mean and variance unknown (precision parameterization) The model for mean and variance unknown for normal sampling. Figure 4.3: DAG with for mean and variance unknown: Precision parameterization Or, alternatively, Figure 4.4: Model specification diagram for normal model with precision parameterization Now for the computation using JAGS # model code jags.model &lt;- function(){ ############################################# # Conditional distribution for the data ############################################# for(i in 1:n){ x[i] ~ dnorm(mu, tau) # conditional distribution of the data } # closes loop over subjects ############################################# # Define the prior distributions for the unknown parameters # The mean of the data (mu) # The variance (sigma.squared) and precision (tau) of the data ############################################# mu ~ dnorm(mu.mu, tau.mu) # prior distribution for mu mu.mu &lt;- 75 # mean of the prior for mu sigma.squared.mu &lt;- 50 # variance of the prior for mu tau.mu &lt;- 1/sigma.squared.mu # precision of the prior for mu tau ~ dgamma(alpha, beta) # precision of the data sigma.squared &lt;- 1/tau # variance of the data sigma &lt;- pow(sigma.squared, 0.5) # taking square root nu.0 &lt;- 10 # hyperparameter for prior for tau sigma.squared.0 &lt;- 30 # hyperparameter for prior for tau alpha &lt;- nu.0/2 # hyperparameter for prior for tau beta &lt;- nu.0*sigma.squared.0/2 # hyperparameter for prior for tau } # data mydata &lt;- list( n=10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92)) # starting values start_values &lt;- function(){ list(&quot;mu&quot;=75, &quot;tau&quot;=0.1) } # vector of all parameters to save param_save &lt;- c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 10 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpc1TPTO/model171464e22ff9.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## mu 83.247 2.173 78.841 81.847 83.258 84.724 87.398 1.001 12000 ## sigma 7.171 1.223 5.264 6.312 7.000 7.854 10.023 1.002 3600 ## tau 0.021 0.007 0.010 0.016 0.020 0.025 0.036 1.002 3600 ## deviance 71.189 1.818 69.392 69.893 70.641 71.915 76.075 1.001 12000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.7 and DIC = 72.8 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;mu&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;tau&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot p &lt;- ggplot(plot.data, aes(x=mu, y=tau))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), 1/var(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(70.01, 100, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_tau &lt;- function(x){dgamma(x, 5, 150)} x.tau &lt;- seq(0.0001, 0.06, 0.0001) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],["markov-chain-monte-carlo-estimation.html", "Chapter 5 Markov Chain Monte Carlo Estimation", " Chapter 5 Markov Chain Monte Carlo Estimation This chapter on MCMC methods gives an introduction to some of the common and basic sampling approaches for Bayesian methods. These methods in Gibbs Sampling Metropolis Sampling Metropolis-Hastings and some notes on how these approaches are related. The most important take away for me was their section on practical issues in MCMC methods. These practical aspects of estimation that should be noted are: Assessing convergence - making sure enough iterations have been used including the potential scale reduction factor (\\(\\hat{R}\\)), Serial dependence - where the samples drawn from the posterior are autocorrelated. This means that within a chain the draws are dependent but with enough draws and thinning all samples are sufficiently independent, Mixing - that different chains search/sample from the same parameter space but different chains can sometimes get stuck sampling one part of the parameter space that is not the same as the other chains. Lastly, a major take away from this chapter is that MCMC methods help to approximate the posterior distribution. The distribution is the solution of a full Bayesian analysis and not a point estimate. "],["regression.html", "Chapter 6 Regression", " Chapter 6 Regression For the regression models, we have built up what the DAG could look like. These representations are shown below. Figure 6.1: DAG a simple regression model with 1 predictor Figure 6.2: DAG for a regression with \\(J\\) predictors Figure 6.3: Expanded DAG representation for regression with hyperparameters included Next, we gave a general representation of how the model specification diagram could be constructed. Figure 6.4: Model specification diagram for a linear regression model "],["stan-model-for-regression-model.html", "6.1 Stan Model for Regression Model", " 6.1 Stan Model for Regression Model model_reg &lt;- &#39; data { int N; real x1[N]; real x2[N]; real y[N]; } parameters { real beta[3]; real&lt;lower=0&gt; tau; } transformed parameters { real&lt;lower=0&gt; sigma; sigma = 1/sqrt(tau); } model { for(i in 1:N){ y[i] ~ normal(beta[1] + beta[2]*x1[i] + beta[3]*x2[i], sigma); } beta ~ normal(0, 100); tau ~ gamma(1, 1); } generated quantities { real varerror; real vary; real Rsquared; real error[N]; for(i in 1:N){ error[i] = y[i] - (beta[1] + beta[2]*x1[i] + beta[3]*x2[i]); } varerror = variance(error); vary = variance(y); Rsquared = 1 - (varerror/vary); } &#39; # data must be in a list dat &lt;- read.table(&quot;data/Chp4_Reg_Chapter_Tests.dat&quot;, header=T) mydata &lt;- list( N=nrow(dat), x1=dat$Ch1Test, x2=dat$Ch2Test, y =dat$Ch3Test ) # start values start_values &lt;- function(){ list(sigma=1, beta=c(0,0,0)) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_reg, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 66db47b16bda4d720cafb6a9769da243. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## beta[1] -2.50 0.02 1.94 -6.24 -3.81 -2.51 -1.18 1.31 7938 ## beta[2] 0.65 0.00 0.17 0.33 0.54 0.66 0.76 0.97 7389 ## beta[3] 0.38 0.00 0.10 0.19 0.32 0.38 0.45 0.59 8465 ## tau 0.28 0.00 0.06 0.18 0.24 0.28 0.32 0.41 9068 ## sigma 1.91 0.00 0.20 1.57 1.77 1.89 2.03 2.34 9035 ## varerror 3.65 0.00 0.15 3.51 3.55 3.61 3.71 4.07 5481 ## vary 8.79 0.00 0.00 8.79 8.79 8.79 8.79 8.79 2 ## Rsquared 0.58 0.00 0.02 0.54 0.58 0.59 0.60 0.60 5481 ## error[1] -0.26 0.02 1.41 -3.04 -1.22 -0.25 0.70 2.46 8023 ## error[2] 1.71 0.01 0.51 0.72 1.37 1.71 2.05 2.70 8555 ## error[3] -1.06 0.01 0.57 -2.16 -1.44 -1.06 -0.68 0.05 8104 ## error[4] -3.21 0.01 0.76 -4.69 -3.71 -3.22 -2.71 -1.73 7925 ## error[5] -2.79 0.01 0.52 -3.82 -3.14 -2.80 -2.44 -1.77 9333 ## error[6] 0.44 0.00 0.42 -0.39 0.16 0.44 0.72 1.25 9732 ## error[7] -1.95 0.00 0.40 -2.72 -2.21 -1.95 -1.68 -1.17 9812 ## error[8] -6.22 0.00 0.39 -6.98 -6.47 -6.22 -5.96 -5.46 10494 ## error[9] 3.40 0.00 0.34 2.74 3.18 3.40 3.62 4.06 11553 ## error[10] 4.02 0.00 0.31 3.40 3.81 4.02 4.22 4.63 12213 ## error[11] -0.75 0.00 0.35 -1.44 -0.99 -0.75 -0.51 -0.06 10525 ## error[12] 0.48 0.01 0.49 -0.47 0.16 0.48 0.81 1.43 9240 ## error[13] 2.48 0.01 0.49 1.53 2.16 2.48 2.81 3.43 9240 ## error[14] -0.72 0.01 0.69 -2.05 -1.18 -0.72 -0.26 0.63 9071 ## error[15] 0.36 0.00 0.30 -0.23 0.17 0.36 0.56 0.95 14290 ## error[16] 0.36 0.00 0.30 -0.23 0.17 0.36 0.56 0.95 14290 ## error[17] 0.98 0.00 0.27 0.44 0.80 0.98 1.16 1.52 16668 ## error[18] 1.98 0.00 0.27 1.44 1.80 1.98 2.16 2.52 16668 ## error[19] -0.40 0.00 0.28 -0.95 -0.59 -0.40 -0.22 0.16 15786 ## error[20] 1.60 0.00 0.28 1.05 1.41 1.60 1.78 2.16 15786 ## error[21] 1.21 0.00 0.32 0.58 1.00 1.21 1.43 1.86 13045 ## error[22] 2.21 0.00 0.32 1.58 2.00 2.21 2.43 2.86 13045 ## error[23] 0.83 0.00 0.39 0.07 0.57 0.83 1.09 1.59 11242 ## error[24] 0.83 0.00 0.39 0.07 0.57 0.83 1.09 1.59 11242 ## error[25] 3.01 0.01 0.88 1.31 2.42 3.01 3.60 4.75 8649 ## error[26] -1.37 0.01 0.78 -2.89 -1.90 -1.37 -0.84 0.18 8741 ## error[27] -0.91 0.00 0.44 -1.75 -1.20 -0.91 -0.61 -0.05 10068 ## error[28] 0.09 0.00 0.44 -0.75 -0.20 0.09 0.39 0.95 10068 ## error[29] 2.09 0.00 0.44 1.25 1.80 2.09 2.39 2.95 10068 ## error[30] -1.67 0.00 0.32 -2.29 -1.88 -1.67 -1.46 -1.05 13262 ## error[31] -0.67 0.00 0.32 -1.29 -0.88 -0.67 -0.46 -0.05 13262 ## error[32] -0.06 0.00 0.29 -0.63 -0.25 -0.06 0.14 0.53 15416 ## error[33] 0.94 0.00 0.29 0.37 0.75 0.94 1.14 1.53 15416 ## error[34] 1.94 0.00 0.29 1.37 1.75 1.94 2.14 2.53 15416 ## error[35] -1.44 0.00 0.30 -2.04 -1.64 -1.44 -1.24 -0.83 15538 ## error[36] -0.44 0.00 0.30 -1.04 -0.64 -0.44 -0.24 0.17 15538 ## error[37] 1.56 0.00 0.30 0.96 1.36 1.56 1.76 2.17 15538 ## error[38] 0.17 0.00 0.35 -0.50 -0.05 0.17 0.40 0.86 13787 ## error[39] 1.17 0.00 0.35 0.50 0.95 1.17 1.40 1.86 13787 ## error[40] -0.21 0.00 0.41 -1.01 -0.48 -0.21 0.06 0.60 12081 ## error[41] -1.33 0.00 0.43 -2.15 -1.61 -1.33 -1.04 -0.49 10069 ## error[42] 0.91 0.00 0.37 0.19 0.66 0.90 1.15 1.63 11958 ## error[43] -3.48 0.00 0.38 -4.22 -3.74 -3.48 -3.23 -2.73 12463 ## error[44] -2.48 0.00 0.38 -3.22 -2.74 -2.48 -2.23 -1.73 12463 ## error[45] -1.86 0.00 0.41 -2.68 -2.14 -1.86 -1.59 -1.03 12255 ## error[46] -0.86 0.00 0.41 -1.68 -1.14 -0.86 -0.59 -0.03 12255 ## error[47] -0.86 0.00 0.41 -1.68 -1.14 -0.86 -0.59 -0.03 12255 ## error[48] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## error[49] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## error[50] 0.14 0.00 0.41 -0.68 -0.14 0.14 0.41 0.97 12255 ## lp__ -59.40 0.02 1.44 -63.00 -60.11 -59.07 -58.35 -57.61 5286 ## Rhat ## beta[1] 1 ## beta[2] 1 ## beta[3] 1 ## tau 1 ## sigma 1 ## varerror 1 ## vary 1 ## Rsquared 1 ## error[1] 1 ## error[2] 1 ## error[3] 1 ## error[4] 1 ## error[5] 1 ## error[6] 1 ## error[7] 1 ## error[8] 1 ## error[9] 1 ## error[10] 1 ## error[11] 1 ## error[12] 1 ## error[13] 1 ## error[14] 1 ## error[15] 1 ## error[16] 1 ## error[17] 1 ## error[18] 1 ## error[19] 1 ## error[20] 1 ## error[21] 1 ## error[22] 1 ## error[23] 1 ## error[24] 1 ## error[25] 1 ## error[26] 1 ## error[27] 1 ## error[28] 1 ## error[29] 1 ## error[30] 1 ## error[31] 1 ## error[32] 1 ## error[33] 1 ## error[34] 1 ## error[35] 1 ## error[36] 1 ## error[37] 1 ## error[38] 1 ## error[39] 1 ## error[40] 1 ## error[41] 1 ## error[42] 1 ## error[43] 1 ## error[44] 1 ## error[45] 1 ## error[46] 1 ## error[47] 1 ## error[48] 1 ## error[49] 1 ## error[50] 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 01:57:59 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## &#39;pars&#39; not specified. Showing first 10 parameters by default. ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;beta&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;beta&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p2 &lt;- ggs_grb(ggs(fit, family = &quot;sigma&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 + p2 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;beta&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p2 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;sigma&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 + p2 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;Rsquared&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot fit.lm &lt;- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat)) MLE &lt;- c(fit.lm$coefficients[,1], fit.lm$sigma**2, fit.lm$r.squared) prior_beta &lt;- function(x){dnorm(x, 0, 1000)} x.beta &lt;- seq(-10, 4.99, 0.01) prior.beta &lt;- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta)) prior_sig &lt;- function(x){dgamma(x, 1, 1)} x.sig &lt;- seq(0.01, 2.5, 0.01) prior.sig &lt;- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`beta[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=Rsquared, color=&quot;Posterior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 268 rows containing non-finite values (stat_density). ## Warning: Removed 1399 row(s) containing missing values (geom_path). ## Warning: Removed 1399 row(s) containing missing values (geom_path). "],["jags-model-for-regression-model.html", "6.2 JAGS Model for Regression Model", " 6.2 JAGS Model for Regression Model # model code jags.model &lt;- function(){ ############################################ # Prior distributions ############################################ beta.0 ~ dnorm(0, .001) # prior for the intercept beta.1 ~ dnorm(0, .001) # prior for coefficient 1 beta.2 ~ dnorm(0, .001) # prior for coefficient 2 tau.e ~ dgamma(1, 1) # prior for the error precision sigma.e &lt;- 1/sqrt(tau.e) # standard deviation of the errors ############################################ # Conditional distribution of the data # Via a regression model ############################################ for(i in 1:n){ y.prime[i] &lt;- beta.0 + beta.1*x1[i] + beta.2*x2[i] y[i] ~ dnorm(y.prime[i], tau.e) } ############################################ # Calculate R-squared ############################################ for(i in 1:n){ error[i] &lt;- y[i] - y.prime[i] } var.error &lt;- sd(error[])*sd(error[]) var.y &lt;- sd(y[])*sd(y[]) R.squared &lt;- 1 - (var.error/var.y) } # data dat &lt;- read.table(&quot;data/Chp4_Reg_Chapter_Tests.dat&quot;, header=T) mydata &lt;- list( n=nrow(dat), x1=dat$Ch1Test, x2=dat$Ch2Test, y =dat$Ch3Test ) # starting values start_values &lt;- function(){ list(&quot;tau.e&quot;=0.01, &#39;beta.0&#39;=0, &quot;beta.1&quot;=0, &quot;beta.2&quot;=0) } # vector of all parameters to save param_save &lt;- c(&quot;tau.e&quot;, &quot;beta.0&quot;, &quot;beta.1&quot;, &quot;beta.2&quot;, &quot;R.squared&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 4 ## Total graph size: 262 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmp6LGnyB/model44a8e65278.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## R.squared 0.584 0.020 0.538 0.578 0.590 0.596 0.601 1.037 7100 ## beta.0 -2.513 1.908 -6.295 -3.771 -2.506 -1.259 1.249 1.001 12000 ## beta.1 0.656 0.164 0.337 0.543 0.659 0.767 0.983 1.001 12000 ## beta.2 0.382 0.103 0.181 0.313 0.382 0.451 0.583 1.001 7900 ## tau.e 0.282 0.057 0.182 0.243 0.278 0.318 0.404 1.001 9400 ## deviance 207.595 2.887 204.084 205.498 206.954 208.944 214.745 1.001 6200 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.2 and DIC = 211.8 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;beta.0&quot;, &quot;beta.1&quot;, &quot;beta.2&quot;, &quot;tau.e&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;R.squared&quot;), prob = 0.8) + plot_title # Expanded Posterior Plot fit.lm &lt;- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat)) MLE &lt;- c(fit.lm$coefficients[,1], 1/fit.lm$sigma**2, fit.lm$r.squared) prior_beta &lt;- function(x){dnorm(x, 0, 1000)} x.beta &lt;- seq(-5, 4.99, 0.01) prior.beta &lt;- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta)) prior_tau &lt;- function(x){dgamma(x, 1, 1)} x.tau &lt;- seq(0.01, 0.50, 0.01) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.0, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.1, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=beta.2, color=&quot;Posterior&quot;))+ geom_line(data=prior.beta, aes(x=beta, y=dens.beta, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0, 1))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau.e, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=R.squared, color=&quot;Posterior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.5, 0.65))+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 235 rows containing non-finite values (stat_density). ## Warning: Removed 899 row(s) containing missing values (geom_path). ## Warning: Removed 4 rows containing non-finite values (stat_density). ## Warning: Removed 899 row(s) containing missing values (geom_path). ## Warning: Removed 39 rows containing non-finite values (stat_density). "],["canonical-bayesian-psychometric-modeling.html", "Chapter 7 Canonical Bayesian Psychometric Modeling", " Chapter 7 Canonical Bayesian Psychometric Modeling This chapter provides an overview of the purposes that psychometric modeling serve and how a Bayesian approach can fit into this purpose. Other information was introduced such as directed acyclic graph (DAG) representations of basic models and notation for future chapters. A canonical (conventional) psychometric analysis focuses on scoring and calibration. Scoring refers to arriving at a representation for an examinee based on their performance on an assessment, and Calibration refers to arriving at a representation for the measurement model parameters (and possibly hyper-parameters). In a research or operational setting, these two core components can be the focus separately or together. When the focus is on the simultaneous assessment of measurement model parameters and person parameters, there may need to be concession made with how to conduct both. In one sense, we would like to estimate all the parameters simultaneous so that uncertainty in measurement model parameters is reflected in person parameter (ability) estimates. However, traditional psychometric analysis tends to proceed by 1) estimating the measurement model parameters only by first integrating the person parameter distribution out of the likelihood function, then 2) using the now estimated measurement model parameters as fixed quantities, we estimate the person parameters (factor scores). This process results in the uncertainty in the person parameters being decreased. One potential benefit of a Bayesian approach is that both calibration and scoring can be done simultaneously. This may not always be of interest in the current application so the traditional approach may still be done. A canonical psychometric model can be expressed as a DAG using a model similar to the path models traditionally used in SEM. We expressed two forms of a model shown in the book below. Figure 7.1: DAG for canonical psychometric modeling Figure 7.2: Expanded DAG to include measurement model parameters "],["classical-test-theory.html", "Chapter 8 Classical Test Theory", " Chapter 8 Classical Test Theory The traditional model specification for CTT is \\[X = T + E,\\] where \\(X\\) is the observed test/measure score, \\(T\\) is the truce score we wish to make inferences about, and \\(E\\) is the error. The true scores have population mean \\(\\mu_T\\) and variance \\(\\sigma^2_T\\). The errors for any individual are expected to be 0 on average, \\(\\mathbb{E}(E_i)=0\\) with variance \\(\\sigma^2_E\\). The errors are uncorrelated with the true score in the population, that is \\[\\mathbb{COV}(T, E) = \\sigma_{TE} = \\rho_{TE}\\sigma_{T}\\sigma_E = 0.\\] Some implications associated with the CTT model are: The population mean of observed scores is the same as the true scores \\[\\mu_x = \\mu_T.\\] The observed score variance can be decomposed into \\[\\begin{align*} \\sigma^2_X &amp;= \\sigma^2_T + \\sigma^2_E + 2\\sigma_{TE}\\\\ &amp;= \\sigma^2_T + \\sigma^2_E. \\end{align*}\\] We can define the reliability in terms of the ratio of true score variance to observed score variance, that is \\[\\rho = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E}.\\] An interesting approach to deriving estimates of true scores is to flip the traditional CTT model around so that we define the true score as a function of the observed score. This uses Kelleys formula (Kelley 1923), \\[\\begin{align*} \\hat{T}_i &amp;= \\rho x_i + (1-\\rho)\\mu_x\\\\ &amp;= \\mu_x + \\rho (x_i - \\mu_x), \\end{align*}\\] where \\(\\mu_x\\) is the mean of the observed scores and \\(\\hat{T}_i\\) is the estimated true score of individual \\(i\\). This is an interesting formula since theres the notion about how to incorporate uncertainty into the estimation of the true score. The higher the uncertainty (lower the reliability) the less we weight the observed score and more we rely on the population mean as our estimate. This has a very Bayesian feel to is, because its nearly identical to how we derive the posterior mean in a conjugate normal model (see p.158). "],["example-1-known-measurement-model-parameters-with-1-measure.html", "8.1 Example 1 - Known measurement model parameters with 1 measure", " 8.1 Example 1 - Known measurement model parameters with 1 measure Here, we will discuss a simple CTT example where we assume that the measurement model parameters are known. This means we assume a value for \\(\\mu_t\\), \\(\\sigma^2_T\\), and \\(\\sigma^2_E\\). We would nearly always need to estimate these quantities to provide an informed decision as to what these parameters should be. This example using 3 observations (individuals) with 1 measure per individual. The DAG for this model is shown below. Figure 8.1: Simple CTT model with 1 measure and known measurement parameters With a simply model specification using normal distributions as the underly probability functions. Figure 8.2: Model specification diagram for the known parameters CTT model "],["example-1-stan.html", "8.2 Example 1 - Stan", " 8.2 Example 1 - Stan model_ctt1 &lt;- &#39; data { int N; real x[N]; real muT; real sigmaT; real sigmaE; } parameters { real T[N]; } model { for(i in 1:N){ x[i] ~ normal(T[i], sigmaE); T[i] ~ normal(muT, sigmaT); } } &#39; # data must be in a list mydata &lt;- list( N=3, x=c(70, 80, 96), muT = 80, sigmaT = 6, #sqrt(36) sigmaE = 4 # sqrt(16) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt1, # model code to be compiled data = mydata, # my data chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: f5d98d6088830a6665ad87ae5eaa93c8. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 73.06 0.03 3.33 66.58 70.84 73.03 75.31 79.59 15634 1 ## T[2] 80.00 0.03 3.31 73.61 77.75 80.00 82.23 86.45 15102 1 ## T[3] 91.09 0.03 3.37 84.55 88.80 91.08 93.35 97.73 16506 1 ## lp__ -4.93 0.01 1.24 -8.17 -5.48 -4.61 -4.03 -3.53 7836 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:26:32 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;,&quot;T[2]&quot;,&quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- mydata$x prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-1-jags.html", "8.3 Example 1 - JAGS", " 8.3 Example 1 - JAGS # model code jags.model.ctt1 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY MODEL # WITH KNOWN HYPERPARAMETERS # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # KNOWN HYPERPARAMETERS ############################################ mu.T &lt;- 80 # Mean of the true scores sigma.squared.T &lt;- 36 # Variance of the true scores sigma.squared.E &lt;- 16 # Variance of the errors tau.T &lt;- 1/sigma.squared.T # Precision of the true scores tau.E &lt;- 1/sigma.squared.E # Precision of the errors ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:n) { T[i] ~ dnorm(mu.T, tau.T) # Distribution of true scores x[i] ~ dnorm(T[i], tau.E) # Distribution of observables } } # data mydata &lt;- list( n=3, x=c(70, 80, 96) ) # starting values start_values &lt;- function(){ list(&quot;T&quot;=c(80,80,80)) } # vector of all parameters to save param_save &lt;- c(&quot;T&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt1, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3 ## Unobserved stochastic nodes: 3 ## Total graph size: 13 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpQ7YNVi/model49f46422dfa.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## T[1] 73.117 3.319 66.487 70.918 73.108 75.357 79.648 1.001 9200 ## T[2] 80.044 3.329 73.532 77.825 80.032 82.300 86.535 1.002 3500 ## T[3] 90.991 3.335 84.418 88.774 90.974 93.255 97.499 1.001 12000 ## deviance 18.083 2.990 14.240 15.841 17.479 19.645 25.392 1.001 12000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.5 and DIC = 22.6 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;, &quot;T[2]&quot;, &quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- mydata$x prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-2-known-measurement-model-with-multiple-measures.html", "8.4 Example 2 - Known Measurement Model with Multiple Measures", " 8.4 Example 2 - Known Measurement Model with Multiple Measures Here, the only thing that changes from the first example is that now we have multiple observations per individual. We can think of this as if we could administer a test (or parallel tests) repeatedly without learning occuring. The DAG and model-specification change to: Figure 8.3: Simple CTT model with known measurement parameters and multiple measures Figure 8.4: Model specification diagram for the known parameters CTT model and multiple measures "],["example-2-stan.html", "8.5 Example 2 - Stan", " 8.5 Example 2 - Stan model_ctt2 &lt;- &#39; data { int N; int J; matrix[N, J] X; real muT; real sigmaT; real sigmaE; } parameters { real T[N]; } model { for(i in 1:N){ T[i] ~ normal(muT, sigmaT); for(j in 1:J){ X[i, j] ~ normal(T[i], sigmaE); } } } &#39; # data must be in a list mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T), muT = 80, sigmaT = 6, #sqrt(36) sigmaE = 4 # sqrt(16) ) # initial values start_values &lt;- function(){ list(T=c(80,80,80,80,80,80,80,80,80,80)) } # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt2, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 5eefd59584a6c5be6da9aa620ec6167e. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 76.89 0.01 1.72 73.49 75.73 76.90 78.07 80.21 31183 1 ## T[2] 79.09 0.01 1.72 75.71 77.94 79.08 80.25 82.46 30038 1 ## T[3] 82.03 0.01 1.72 78.68 80.87 82.02 83.20 85.41 31248 1 ## T[4] 75.05 0.01 1.71 71.69 73.89 75.05 76.21 78.36 30749 1 ## T[5] 72.64 0.01 1.73 69.28 71.46 72.64 73.83 76.01 31455 1 ## T[6] 88.47 0.01 1.71 85.11 87.31 88.47 89.63 91.84 30882 1 ## T[7] 77.24 0.01 1.70 73.90 76.09 77.25 78.38 80.61 32344 1 ## T[8] 80.55 0.01 1.69 77.27 79.43 80.54 81.67 83.84 29137 1 ## T[9] 80.18 0.01 1.71 76.88 79.02 80.18 81.34 83.60 32971 1 ## T[10] 89.01 0.01 1.71 85.64 87.84 89.00 90.17 92.37 29607 1 ## lp__ -23.47 0.03 2.24 -28.69 -24.74 -23.14 -21.83 -20.11 6988 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:34:50 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit, family = &quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit, family=&quot;T&quot;)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = paste0(&quot;T[&quot;,1:10,&quot;]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- rowMeans(mydata$X) prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + plot_layout(guides=&quot;collect&quot;) "],["example-2-jags.html", "8.6 Example 2 - JAGS", " 8.6 Example 2 - JAGS # model code jags.model.ctt2 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY # WITH KNOWN # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # KNOWN HYPERPARAMETERS ############################################ mu.T &lt;- 80 # Mean of the true scores sigma.squared.T &lt;- 36 # Variance of the true scores sigma.squared.E &lt;- 16 # Variance of the errors tau.T &lt;- 1/sigma.squared.T # Precision of the true scores tau.E &lt;- 1/sigma.squared.E # Precision of the errors ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:N) { T[i] ~ dnorm(mu.T, tau.T) # Distribution of true scores for(j in 1:J){ x[i, j] ~ dnorm(T[i], tau.E) # Distribution of observables } } } # data mydata &lt;- list( N = 10, J = 5, x = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # starting values start_values &lt;- function(){ list(&quot;T&quot;=rep(80,10)) } # vector of all parameters to save param_save &lt;- c(&quot;T&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 10 ## Total graph size: 68 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpq8pU6a/model4d046a677c1.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## T[1] 76.888 1.720 73.515 75.728 76.892 78.032 80.294 1.001 12000 ## T[2] 79.064 1.709 75.655 77.935 79.083 80.206 82.408 1.001 11000 ## T[3] 82.045 1.725 78.681 80.880 82.046 83.204 85.450 1.001 12000 ## T[4] 75.053 1.729 71.659 73.884 75.063 76.228 78.436 1.001 12000 ## T[5] 72.673 1.689 69.365 71.570 72.651 73.805 75.988 1.001 7300 ## T[6] 88.407 1.714 85.006 87.274 88.393 89.578 91.730 1.001 12000 ## T[7] 77.246 1.710 73.864 76.083 77.266 78.417 80.576 1.001 12000 ## T[8] 80.535 1.712 77.170 79.390 80.521 81.703 83.841 1.001 12000 ## T[9] 80.178 1.722 76.833 78.988 80.190 81.330 83.543 1.001 12000 ## T[10] 88.952 1.725 85.559 87.763 88.955 90.114 92.331 1.001 12000 ## deviance 269.701 4.423 262.951 266.485 269.038 272.243 280.007 1.001 11000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 9.8 and DIC = 279.5 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;T[1]&quot;, &quot;T[2]&quot;, &quot;T[3]&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- rowMeans(mydata$X) ## Error in rowMeans(mydata$X): &#39;x&#39; must be an array of at least two dimensions prior_t &lt;- function(x){dnorm(x, 80, 6)} x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=x.t, dens.t = prior_t(x.t)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.t, aes(x=tr, y=dens.t, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + plot_layout(guides=&quot;collect&quot;) "],["example-3-unknown-measurement-model-with-multiple-measures.html", "8.7 Example 3 - Unknown Measurement Model with Multiple Measures", " 8.7 Example 3 - Unknown Measurement Model with Multiple Measures Here, we finally get to the (more) realistic case when we dont have as much prior knowledge about the measurement model parameters (namely, variances). The structure relies on hierarchically specifying priors to induce conditional independence. The DAG and model-specification change to: Figure 8.5: Simple CTT model with unknown measurement parameters Figure 8.6: Model specification diagram for the unknown measurement model parameters "],["example-3-stan.html", "8.8 Example 3 - Stan", " 8.8 Example 3 - Stan model_ctt3 &lt;- &#39; data { int N; int J; matrix[N, J] X; } parameters { real T[N]; real muT; real&lt;lower=0&gt; sigmaT; real&lt;lower=0&gt; sigmaE; } model { for(i in 1:N){ T[i] ~ normal(muT, sigmaT); for(j in 1:J){ X[i, j] ~ normal(T[i], sigmaE); } } muT ~ normal(80, 10); sigmaT ~ inv_gamma(1, 6); sigmaE ~ inv_gamma(1, 4); } generated quantities { real rho; real rhocomp; rho = square(sigmaT)/(square(sigmaT) + square(sigmaE)); rhocomp = J*rho/((J-1)*rho + 1); } &#39; # data must be in a list mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # initial values start_values &lt;- function(){ list(T=c(80,80,80,80,80,80,80,80,80,80), muT=80, sigmaT=10, sigmaE=5) } # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_ctt3, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 8af61d5f99903650a9f4417c76fd9a69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## T[1] 76.87 0.01 1.53 73.87 75.86 76.86 77.90 79.90 ## T[2] 79.08 0.01 1.54 76.09 78.06 79.08 80.11 82.08 ## T[3] 82.04 0.01 1.51 79.09 81.03 82.04 83.04 85.03 ## T[4] 75.02 0.01 1.52 72.05 74.01 75.02 76.02 78.06 ## T[5] 72.60 0.01 1.52 69.64 71.58 72.60 73.61 75.60 ## T[6] 88.52 0.01 1.54 85.49 87.49 88.54 89.56 91.53 ## T[7] 77.23 0.01 1.54 74.25 76.21 77.21 78.26 80.28 ## T[8] 80.55 0.01 1.51 77.56 79.55 80.54 81.53 83.53 ## T[9] 80.19 0.01 1.51 77.26 79.18 80.18 81.19 83.14 ## T[10] 89.07 0.01 1.55 85.97 88.06 89.09 90.11 92.06 ## muT 80.10 0.02 1.99 76.17 78.84 80.10 81.34 84.04 ## sigmaT 6.03 0.01 1.68 3.67 4.87 5.73 6.86 10.04 ## sigmaE 3.50 0.00 0.40 2.82 3.21 3.46 3.74 4.39 ## rho 0.72 0.00 0.11 0.49 0.66 0.73 0.80 0.90 ## rhocomp 0.92 0.00 0.04 0.83 0.90 0.93 0.95 0.98 ## lp__ -115.06 0.04 2.86 -121.63 -116.72 -114.66 -112.98 -110.62 ## n_eff Rhat ## T[1] 25673 1 ## T[2] 29294 1 ## T[3] 27260 1 ## T[4] 27424 1 ## T[5] 23635 1 ## T[6] 22939 1 ## T[7] 30949 1 ## T[8] 25073 1 ## T[9] 28241 1 ## T[10] 25912 1 ## muT 17119 1 ## sigmaT 16441 1 ## sigmaE 16557 1 ## rho 19364 1 ## rhocomp 18382 1 ## lp__ 5897 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:41:05 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## &#39;pars&#39; not specified. Showing first 10 parameters by default. ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;T&quot;, &quot;muT&quot;, &quot;sigmaT&quot;, &quot;sigmaE&quot;, &quot;rho&quot;, &quot;rhocomp&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion p1 &lt;- ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) p1 # autocorrelation p1 &lt;- ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) p1 # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;T[&quot;,1:10,&quot;]&quot;), &quot;muT&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigmaT&quot;, &quot;sigmaE&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;rho&quot;, &quot;rhocomp&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot MLE &lt;- c(rowMeans(mydata$X), mean(mydata$X)) prior_mu &lt;- function(x){dnorm(x, 80, 10)} x.mu&lt;- seq(50.1, 100, 0.1) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sigt &lt;- function(x){dinvgamma(x, 1, 6)} x.sigt&lt;- seq(.1, 15, 0.1) prior.sigt &lt;- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_t &lt;- function(x){ mu &lt;- rnorm(1, 80, 10) sig &lt;- rinvgamma(1, 1, 4) rnorm(x, mu, sig) } x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=prior_t(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; plot.data &lt;- as.data.frame(plot.data) p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`muT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[11], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaE`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=&quot;collect&quot;) "],["example-3-jags.html", "8.9 Example 3 - JAGS", " 8.9 Example 3 - JAGS # model code jags.model.ctt3 &lt;- function(){ ############################################ # CLASSICAL TEST THEORY MODEL # WITH UnkNOWN HYPERPARAMETERS # TRUE SCORE MEAN, TRUE SCORE VARIANCE # ERROR VARIANCE ############################################ ############################################ # PRIOR DISTRIBUTIONS FOR HYPERPARAMETERS ############################################ muT ~ dnorm(80,.01) # Mean of the true scores tau.T ~ dgamma(1, 36) # Precision of the true scores tau.E ~ dgamma(1, 16) # Precision of the errors sigma.squared.T &lt;- 1/tau.T # Variance of the true scores sigma.squared.E &lt;- 1/tau.E # Variance of the errors # get SD for summarizing sigmaT &lt;- pow(sigma.squared.T, 0.5) sigmaE &lt;- pow(sigma.squared.E, 0.5) ############################################ # MODEL FOR TRUE SCORES AND OBSERVABLES ############################################ for (i in 1:N) { T[i] ~ dnorm(muT, tau.T) # Distribution of true scores for(j in 1:J){ X[i,j] ~ dnorm(T[i], tau.E) # Distribution of observables } } ############################################ # RELIABILITY ############################################ rho &lt;- sigma.squared.T/(sigma.squared.T+sigma.squared.E) rhocomp &lt;- J*rho/((J-1)*rho+1) } # data mydata &lt;- list( N = 10, J = 5, X = matrix( c(80, 77, 80, 73, 73, 83, 79, 78, 78, 77, 85, 77, 88, 81, 80, 76, 76, 76, 78, 67, 70, 69, 73, 71, 77, 87, 89, 92, 91, 87, 76, 75, 79, 80, 75, 86, 75, 80, 80, 82, 84, 79, 79, 77, 82, 96, 85, 91, 87, 90), ncol=5, nrow=10, byrow=T) ) # initial values start_values &lt;- list( list(&quot;T&quot;=c(60,85,80,95,74,69,91,82,87,78), &quot;muT&quot;=80, &quot;tau.E&quot;=0.06, &quot;tau.T&quot;=0.023), list(&quot;T&quot;=c(63, 79, 74, 104, 80, 71, 95, 72, 80, 82), &quot;muT&quot;=100, &quot;tau.E&quot;=0.09, &quot;tau.T&quot;=0.05), list(&quot;T&quot;=c(59, 86, 88, 89, 76, 65, 94, 72, 95, 84), &quot;muT&quot;=70, &quot;tau.E&quot;=0.03, &quot;tau.T&quot;=0.001), list(&quot;T&quot;=c(60, 87, 90, 91, 77, 74, 95, 76, 83, 87), &quot;muT&quot;=90, &quot;tau.E&quot;=0.01, &quot;tau.T&quot;=0.1) ) # vector of all parameters to save param_save &lt;- c(&quot;T&quot;,&quot;muT&quot;,&quot;sigmaT&quot;,&quot;sigmaE&quot;, &quot;rho&quot;, &quot;rhocomp&quot;) # fit model fit &lt;- jags( model.file=jags.model.ctt2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Warning in jags.model(model.file, data = data, inits = init.values, n.chains = n.chains, : Unused ## variable &quot;X&quot; in data ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 60 ## Total graph size: 68 ## ## Deleting model ## Error in setParameters(init.values[[i]], i): Error in node tau.E ## Cannot set value of non-variable node print(fit) ## Inference for Stan model: 8af61d5f99903650a9f4417c76fd9a69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## T[1] 76.87 0.01 1.53 73.87 75.86 76.86 77.90 79.90 25673 1 ## T[2] 79.08 0.01 1.54 76.09 78.06 79.08 80.11 82.08 29294 1 ## T[3] 82.04 0.01 1.51 79.09 81.03 82.04 83.04 85.03 27260 1 ## T[4] 75.02 0.01 1.52 72.05 74.01 75.02 76.02 78.06 27424 1 ## T[5] 72.60 0.01 1.52 69.64 71.58 72.60 73.61 75.60 23635 1 ## T[6] 88.52 0.01 1.54 85.49 87.49 88.54 89.56 91.53 22939 1 ## T[7] 77.23 0.01 1.54 74.25 76.21 77.21 78.26 80.28 30949 1 ## T[8] 80.55 0.01 1.51 77.56 79.55 80.54 81.53 83.53 25073 1 ## T[9] 80.19 0.01 1.51 77.26 79.18 80.18 81.19 83.14 28241 1 ## T[10] 89.07 0.01 1.55 85.97 88.06 89.09 90.11 92.06 25912 1 ## muT 80.10 0.02 1.99 76.17 78.84 80.10 81.34 84.04 17119 1 ## sigmaT 6.03 0.01 1.68 3.67 4.87 5.73 6.86 10.04 16441 1 ## sigmaE 3.50 0.00 0.40 2.82 3.21 3.46 3.74 4.39 16557 1 ## rho 0.72 0.00 0.11 0.49 0.66 0.73 0.80 0.90 19364 1 ## rhocomp 0.92 0.00 0.04 0.83 0.90 0.93 0.95 0.98 18382 1 ## lp__ -115.06 0.04 2.86 -121.63 -116.72 -114.66 -112.98 -110.62 5897 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:41:05 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) ## Error in xy.coords(x, y, xlabel, ylabel, log = log, recycle = TRUE): &#39;list&#39; object cannot be coerced to type &#39;double&#39; # gelman-rubin-brook gelman.plot(jags.mcmc) ## Error in gelman.preplot(x, bin.width = bin.width, max.bins = max.bins, : Insufficient iterations to produce Gelman-Rubin plot # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) ## Error in jags.mcmc[[1]]: this S4 class is not subsettable plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) ## Error in y[, var.cols] &lt;- x: number of items to replace is not a multiple of replacement length colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;T[&quot;,1:10,&quot;]&quot;), &quot;muT&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero mcmc_areas( plot.data, pars = c(&quot;sigmaT&quot;, &quot;sigmaE&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero mcmc_areas( plot.data, pars = c(&quot;rho&quot;, &quot;rhocomp&quot;), prob = 0.8) + plot_title ## Error in x[, j, ] &lt;- a[chain == j, , drop = FALSE]: replacement has length zero # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(rowMeans(mydata$X), mean(mydata$X)) prior_mu &lt;- function(x){dnorm(x, 80, 10)} x.mu&lt;- seq(50.1, 100, 0.1) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sigt &lt;- function(x){dinvgamma(x, 1, 6)} x.sigt&lt;- seq(.1, 15, 0.1) prior.sigt &lt;- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_t &lt;- function(x){ mu &lt;- rnorm(1, 80, 10) sig &lt;- rinvgamma(1, 1, 4) rnorm(x, mu, sig) } x.t&lt;- seq(50.1, 100, 0.1) prior.t &lt;- data.frame(tr=prior_t(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[1]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[5]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`T[10]`, color=&quot;Posterior&quot;))+ geom_density(data=prior.t,aes(x=tr,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[10], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`muT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[11], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaT`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`sigmaE`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=&quot;Prior&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=&quot;collect&quot;) ## Error in FUN(X[[i]], ...): object &#39;muT&#39; not found "],["confirmatory-factor-analysis.html", "Chapter 9 Confirmatory Factor Analysis", " Chapter 9 Confirmatory Factor Analysis The full Bayesian specification of a general CFA model for all associated unknowns is as follows. This includes probability statements, notation, parameters, likelihood, priors, and hyperparameters. The observed data is defined as the \\(n\\times J\\) matrix \\(\\mathbf{X}\\) for the \\(J\\) observed measures. The CFA model parameters are defined as \\[\\begin{align*} \\mathbf{x}_i &amp;= \\tau + \\Lambda\\xi_i + \\varepsilon_i\\\\ \\Sigma (\\mathbf{x}) &amp;= \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi \\end{align*}\\] \\(\\Xi\\) is the \\(n\\times M\\) matrix of latent variable scores on the \\(M\\) latent variables for the \\(n\\) respondents/subjects. For an single subject, \\(\\xi_i\\) represents the vector of scores on the latent variable(s). Values (location, scale, orientation, etc.) or \\(\\xi_i\\) are conditional on (1) \\(\\kappa\\), the \\(M\\times 1\\) vector of latent variable means, and (2) \\(\\Phi\\), the \\(M\\times M\\) covariance matrix of variable variables; \\(\\tau\\) is the \\(J\\times 1\\) vector of observed variable intercepts which is the expected value for the observed measures when the latent variable(s) are all \\(0\\); \\(\\Lambda\\) is the \\(J\\times M\\) matrix of factor loadings where the \\(j\\)th row and \\(m\\)th column represents the factor loading of the \\(j\\)th observed variable on the \\(m\\)th latent variable; \\(\\delta_i\\) is the \\(J\\times 1\\) vector of errors, where \\(E(\\delta_i)=\\mathbf{0}\\) with \\(\\mathrm{var}(\\delta_i)=\\Psi\\) which is the \\(J\\times J\\) error covariance matrix. \\[\\begin{align*} p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi\\mid \\mathbf{X}) &amp;\\propto p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)\\\\ &amp;= p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi) p(\\Xi\\mid\\kappa, \\Phi) p(\\kappa) p(\\Phi) p(\\tau) p(\\Lambda) p(\\Psi)\\\\ &amp;= \\prod_{i=1}^{n}\\prod_{j=1}^J\\prod_{m=1}^M p(x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj}) p(\\xi_i\\mid\\kappa, \\Phi) p(\\kappa_m) p(\\Phi) p(\\tau_j) p(\\lambda_j) p(\\psi_{jj}) \\end{align*}\\] where \\[\\begin{align*} x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj} &amp;\\sim \\mathrm{Normal}(\\tau_j+\\xi_i\\lambda^{\\prime}_j, \\psi_{jj}),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\xi_i\\mid\\kappa, \\Phi &amp;\\sim \\mathrm{Normal}(\\kappa, \\Phi),\\ \\mathrm{for}\\ i=1, \\cdots, n;\\\\ \\kappa_m &amp;\\sim \\mathrm{Normal}(\\mu_{\\kappa},\\sigma^2_{\\kappa}),\\ \\mathrm{for}\\ m = 1, \\cdots, M;\\\\ \\Phi &amp;\\sim \\mathrm{Inverse-Wishart}(\\Phi_0, d);\\\\ \\tau_j &amp;\\sim \\mathrm{Normal}(\\mu_{\\tau},\\sigma^2_{\\tau}),\\ \\mathrm{for}\\ j = 1, \\cdots, J;\\\\ \\lambda_{j,m} &amp;\\sim \\mathrm{Normal}(\\mu_{\\lambda}, \\sigma^2_{\\lambda}),\\ \\mathrm{for}\\ j = 1, \\cdots, J,\\ m = 1, \\cdots, M;\\\\ \\psi_{jj} &amp;\\sim \\mathrm{Inverse-Gamma}(\\nu_{\\psi}/2, \\nu_{\\psi}\\psi_0/2),\\ \\mathrm{for}\\ j=1, \\cdots, J. \\end{align*}\\] With the hyperparameters that are supplied by the analyst being defined as \\(\\mu_{\\kappa}\\) is the prior mean for the latent variable, \\(\\sigma^2_{\\kappa}\\) is the prior variance for the latent variable, \\(\\Phi_0\\) is the prior expectation for the covariance matrix among latent variables, \\(d\\) represents a dispersion parameter reflecting the magnitude of our beliefs about \\(\\Phi_0\\), \\(\\mu_{\\tau}\\) is the prior mean for the intercepts which reflects our knowledge about the location of the observed variables, \\(\\sigma^2_{\\tau}\\) is a measure of how much weight we want to give to the prior mean, \\(\\mu_{\\lambda}\\) is the prior mean for the factor loadings which can vary over items and latent variables, \\(\\sigma^2_{\\lambda}\\) is the measure of dispersion for the the factor loadings, where lower variances indicate a stronger belief about the values for the loadings, \\(\\nu_{\\psi}\\) is the measure of location for the gamma prior indicating our expectation for the magnitude of the error variance, \\(\\psi_0\\) is our uncertainty with respect to the location we selected for the variance, and Alternatively, we could place a prior on \\(\\Psi\\) instead of the individual residual variances. This would mean we would be placing a prior on the error-covariance matrix similar to how we specified a prior for latent variance covariance matrix. "],["single-latent-variable-model.html", "9.1 Single Latent Variable Model", " 9.1 Single Latent Variable Model Here we consider the model in section 9.3 which is a CFA model with 1 latent variable and 5 observed indicators. The graphical representation of these factor models get pretty complex pretty quickly, but for this example I have reproduced a version of Figure 9.3b, shown below. Figure 9.1: DAG for CFA model with 1 latent variable However, as the authors noted, the path diagram tradition of conveying models is also very useful in discussing and describing the model, which I give next. Figure 9.2: Path diagram for CFA model with 1 latent variable For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 9.3: Model specification diagram for the CFA model with 1 latent factor "],["jags-single-latent-variable.html", "9.2 JAGS - Single Latent Variable", " 9.2 JAGS - Single Latent Variable # model code jags.model.cfa1 &lt;- function(){ ######################################## # Specify the factor analysis measurement model for the observables ############################################## for (i in 1:n){ for(j in 1:J){ mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # model implied expectation for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ################################## # Specify the (prior) distribution for the latent variables #################################### for (i in 1:n){ ksi[i] ~ dnorm(kappa, inv.phi) # distribution for the latent variables } ###################################### # Specify the prior distribution for the parameters that govern the latent variables ################################### kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(.1, .1, .1, .1, .1), &quot;lambda&quot;=c(NA, 0, 0, 0, 0), &quot;inv.phi&quot;=1, &quot;inv.psi&quot;=c(1, 1, 1, 1, 1)), list(&quot;tau&quot;=c(3, 3, 3, 3, 3), &quot;lambda&quot;=c(NA, 3, 3, 3, 3), &quot;inv.phi&quot;=2, &quot;inv.psi&quot;=c(2, 2, 2, 2, 2)), list(&quot;tau&quot;=c(5, 5, 5, 5, 5), &quot;lambda&quot;=c(NA, 6, 6, 6, 6), &quot;inv.phi&quot;=.5, &quot;inv.psi&quot;=c(.5, .5, .5, .5, .5)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,2:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa1, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 3, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpu2Wtns/model27604c204620.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 7500 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda[2] 0.732 0.046 0.645 0.700 0.730 0.762 0.826 1.003 1000 ## lambda[3] 0.421 0.038 0.349 0.395 0.420 0.447 0.499 1.004 560 ## lambda[4] 1.056 0.067 0.931 1.010 1.055 1.100 1.195 1.004 600 ## lambda[5] 0.989 0.061 0.878 0.947 0.986 1.028 1.114 1.004 690 ## phi 0.433 0.043 0.353 0.403 0.430 0.460 0.523 1.005 530 ## psi[1] 0.373 0.028 0.322 0.354 0.372 0.392 0.429 1.001 7500 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.191 0.212 1.001 7500 ## psi[3] 0.180 0.012 0.158 0.171 0.179 0.188 0.205 1.001 7500 ## psi[4] 0.378 0.030 0.323 0.357 0.376 0.397 0.440 1.001 7500 ## psi[5] 0.266 0.022 0.225 0.251 0.265 0.281 0.311 1.001 3600 ## tau[1] 3.334 0.040 3.256 3.306 3.333 3.361 3.411 1.001 7500 ## tau[2] 3.898 0.029 3.842 3.879 3.898 3.917 3.956 1.001 7500 ## tau[3] 4.596 0.023 4.551 4.581 4.597 4.612 4.640 1.001 7500 ## tau[4] 3.034 0.041 2.952 3.006 3.034 3.061 3.115 1.001 7500 ## tau[5] 3.713 0.037 3.642 3.689 3.713 3.738 3.787 1.001 7500 ## deviance 3380.640 42.695 3301.046 3350.905 3379.767 3409.341 3467.116 1.001 7500 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 911.6 and DIC = 4292.2 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;lambda[&quot;, 2:5, &quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title "],["stan-single-latent-variable.html", "9.3 Stan - Single Latent Variable", " 9.3 Stan - Single Latent Variable model_cfa1 &lt;- &#39; data { int N; int J; matrix[N, J] X; } parameters { real ksi[N]; //latent variable values real tau[J]; //intercepts real load[J-1]; //factor loadings real&lt;lower=0&gt; psi[J]; //residual variance //real kappa; // factor means real&lt;lower=0&gt; phi; // factor variances } transformed parameters { real lambda[J]; lambda[1] = 1; lambda[2:J] = load; } model { real kappa; kappa = 0; // likelihood for data for(i in 1:N){ for(j in 1:J){ X[i, j] ~ normal(tau[j] + ksi[i]*lambda[j], psi[j]); } } // prior for latent variable parameters ksi ~ normal(kappa, phi); phi ~ inv_gamma(5, 10); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); for(j in 1:(J-1)){ load[j] ~ normal(1, 10); } } &#39; # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( N = 500, J = 5, X = as.matrix(dat) ) # initial values start_values &lt;- list( list(tau = c(.1,.1,.1,.1,.1), lambda=c(0, 0, 0, 0, 0), phi = 1, psi=c(1, 1, 1, 1, 1)), list(tau = c(3,3,3,3,3), lambda=c(3, 3, 3, 3, 3), phi = 2, psi=c(.5, .5, .5, .5, .5)), list(tau = c(5, 5, 5, 5, 5), lambda=c(6, 6, 6, 6, 6), phi = 2, psi=c(2, 2, 2, 2, 2)) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa1, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 1, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## Inference for Stan model: 2d84cef0cbb0e7ac36bc8c7a29bca5b9. ## 3 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## lambda[2] 0.81 0 0.05 0.72 0.78 0.81 0.85 0.92 1355 1 ## lambda[3] 0.47 0 0.04 0.40 0.45 0.47 0.50 0.56 2095 1 ## lambda[4] 1.10 0 0.07 0.97 1.05 1.10 1.15 1.26 1625 1 ## lambda[5] 1.06 0 0.07 0.93 1.01 1.06 1.10 1.20 1355 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 2162 1 ## tau[2] 3.90 0 0.03 3.84 3.88 3.90 3.92 3.95 1730 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 2974 1 ## tau[4] 3.03 0 0.04 2.95 3.01 3.03 3.06 3.11 2063 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 1678 1 ## psi[1] 0.60 0 0.02 0.55 0.58 0.60 0.61 0.64 6962 1 ## psi[2] 0.36 0 0.02 0.33 0.35 0.36 0.37 0.40 4102 1 ## psi[3] 0.37 0 0.01 0.35 0.37 0.37 0.38 0.40 8740 1 ## psi[4] 0.60 0 0.02 0.56 0.59 0.60 0.62 0.65 7053 1 ## psi[5] 0.48 0 0.02 0.44 0.47 0.48 0.49 0.52 5340 1 ## phi 0.60 0 0.03 0.54 0.58 0.60 0.63 0.67 1326 1 ## ksi[1] -0.23 0 0.22 -0.68 -0.38 -0.23 -0.08 0.21 18662 1 ## ksi[8] 0.85 0 0.23 0.41 0.70 0.85 1.01 1.30 12002 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 02:49:16 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ## Warning: Removed 50 row(s) containing missing values (geom_path). ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning: Removed 150 rows containing missing values (geom_bar). ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot colnames(dat) &lt;- paste0(&quot;x&quot;,1:5) lav.mod &lt;- &#39; xi =~ 1*x1 + x2 + x3 + x4 + x5 xi ~~ xi x1 ~ 1 x2 ~ 1 x3 ~ 1 x4 ~ 1 x5 ~ 1 &#39; lav.fit &lt;- lavaan::cfa(lav.mod, data=dat) MLE &lt;- lavaan::parameterEstimates(lav.fit) prior_tau &lt;- function(x){dnorm(x, 3, 10)} x.tau&lt;- seq(1, 5, 0.01) prior.tau &lt;- data.frame(tau=x.tau, dens.mtau = prior_tau(x.tau)) prior_lambda &lt;- function(x){dnorm(x, 1, 10)} x.lambda&lt;- seq(0, 2, 0.01) prior.lambda &lt;- data.frame(lambda=x.lambda, dens.lambda = prior_lambda(x.lambda)) prior_sig &lt;- function(x){dinvgamma(x, 5, 10)} x.sig&lt;- seq(.01, 1, 0.01) prior.sig &lt;- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_ksi &lt;- function(x){ mu &lt;- 0 sig &lt;- rinvgamma(1, 5, 10) rnorm(x, mu, sig) } x.ksi&lt;- seq(-5, 5, 0.01) prior.ksi &lt;- data.frame(ksi=prior_ksi(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; # get stan samples plot.data &lt;- as.data.frame(plot.data) # make plotting pieces p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). # phi p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`phi`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[6,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) # psi p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[12,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[13,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[14,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[15,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[16,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(guides = &quot;collect&quot;) "],["blavaan-single-latent-variable.html", "9.4 Blavaan - Single Latent Variable", " 9.4 Blavaan - Single Latent Variable # model model_cfa_blavaan &lt;- &quot; f1 =~ 1*PI + AD + IGC + FI + FC &quot; dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) fit &lt;- blavaan::bcfa(model_cfa_blavaan, data=dat) ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.001 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 1: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 1: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 1: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 1: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 1: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 1: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 1: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 1: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 1: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 1: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 1: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 5.473 seconds (Warm-up) ## Chain 1: 8.591 seconds (Sampling) ## Chain 1: 14.064 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.001 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 2: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 2: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 2: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 2: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 2: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 2: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 2: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 2: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 2: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 2: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 2: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 5.892 seconds (Warm-up) ## Chain 2: 9.185 seconds (Sampling) ## Chain 2: 15.077 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 3: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 3: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 3: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 3: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 3: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 3: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 3: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 3: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 3: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 3: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 3: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 5.008 seconds (Warm-up) ## Chain 3: 8.132 seconds (Sampling) ## Chain 3: 13.14 seconds (Total) ## Chain 3: ## Computing posterior predictives... summary(fit) ## blavaan (0.3-15) results of 1000 samples after 500 adapt/burnin iterations ## ## Number of observations 500 ## ## Number of missing patterns 1 ## ## Statistic MargLogLik PPP ## Value -2196.709 0.000 ## ## Latent Variables: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 =~ ## PI 1.000 NA ## AD 0.847 0.054 0.748 0.96 1.000 normal(0,10) ## IGC 0.494 0.039 0.424 0.575 1.000 normal(0,10) ## FI 1.130 0.076 0.984 1.289 1.000 normal(0,10) ## FC 1.090 0.070 0.958 1.237 1.000 normal(0,10) ## ## Intercepts: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 3.333 0.038 3.258 3.408 1.001 normal(0,32) ## .AD 3.898 0.027 3.846 3.95 1.002 normal(0,32) ## .IGC 4.596 0.021 4.555 4.638 1.002 normal(0,32) ## .FI 3.034 0.041 2.951 3.114 1.002 normal(0,32) ## .FC 3.712 0.037 3.64 3.784 1.001 normal(0,32) ## f1 0.000 NA ## ## Variances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 0.353 0.026 0.304 0.407 1.002 gamma(1,.5)[sd] ## .AD 0.120 0.012 0.097 0.146 0.999 gamma(1,.5)[sd] ## .IGC 0.133 0.009 0.116 0.152 1.000 gamma(1,.5)[sd] ## .FI 0.362 0.030 0.306 0.423 0.999 gamma(1,.5)[sd] ## .FC 0.223 0.021 0.185 0.267 1.000 gamma(1,.5)[sd] ## f1 0.337 0.040 0.263 0.419 1.000 gamma(1,.5)[sd] plot(fit) ## Two Latent Variable Model Here we consider the model in section 9.4 which is a CFA model with 2 latent variables and 5 observed indicators. The graphical representation of these factor models get pretty complex pretty quickly, but for this example I have reproduced a version of Figure 9.4, shown below. Figure 9.4: DAG for CFA model with 2 latent variables However, as the authors noted, the path diagram tradition of conveying models is also very useful in discussing and describing the model, which I give next. Figure 9.5: Path Diagram for CFA model with 2 latent variables For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 9.6: Model specification diagram for the CFA model with 2 latent factors "],["jags-two-latent-variable.html", "9.5 JAGS - Two Latent Variable", " 9.5 JAGS - Two Latent Variable # model code jags.model.cfa2 &lt;- function(){ ################### # Specify the factor analysis measurement model for the observables #################### for (i in 1:n){ # expected value for each examinee for each observable mu[i,1] &lt;- tau[1] + lambda[1,1]*ksi[i,1] mu[i,2] &lt;- tau[2] + lambda[2,1]*ksi[i,1] mu[i,3] &lt;- tau[3] + lambda[3,1]*ksi[i,1] mu[i,4] &lt;- tau[4] + lambda[4,2]*ksi[i,2] mu[i,5] &lt;- tau[5] + lambda[5,2]*ksi[i,2] for(j in 1:J){ x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ###################################################################### # Specify the (prior) distribution for the latent variables ###################################################################### for (i in 1:n){ ksi[i, 1:M] ~ dmnorm(kappa[], inv.phi[,]) # distribution for the latent variables } ###################################################################### # Specify the prior distribution for the parameters that govern the latent variables ###################################################################### for(m in 1:M){ kappa[m] &lt;- 0 # Means of latent variables } inv.phi[1:M,1:M] ~ dwish(dxphi.0[ , ], d); # prior for precision matrix for the latent variables phi[1:M,1:M] &lt;- inverse(inv.phi[ , ]); # the covariance matrix for the latent vars phi.0[1,1] &lt;- 1; phi.0[1,2] &lt;- .3; phi.0[2,1] &lt;- .3; phi.0[2,2] &lt;- 1; d &lt;- 2; for (m in 1:M){ for (mm in 1:M){ dxphi.0[m,mm] &lt;- d*phi.0[m,mm]; } } ###################################################################### # Specify the prior distribution for the measurement model parameters ###################################################################### for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1,1] &lt;- 1.0 # loading fixed to 1.0 lambda[4,2] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:3){ lambda[j,1] ~ dnorm(1, .1) # prior distribution for the remaining loadings } lambda[5,2] ~ dnorm(1, .1) # prior distribution for the remaining loadings } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, M =2, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01), lambda= structure( .Data= c( NA, 2.00E+00, 2.00E+00, NA, NA, NA, NA, NA, NA, 2.00E+00), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.00E+00, 0.00E+00, 0.00E+00, 1.00E+00), .Dim=c(2, 2)), inv.psi=c(1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00)), list(tau=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), lambda= structure( .Data= c( NA, 5.00E-01, 5.00E-01, NA, NA, NA, NA, NA, NA, 5.00E-01), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.33E+00, -6.67E-01, -6.67E-01, 1.33E+00), .Dim=c(2, 2)), inv.psi=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)) , list(tau=c(5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00), lambda= structure( .Data= c( NA, 1.00E+00, 1.00E+00, NA, NA, NA, NA, NA, NA, 1.00E+00), .Dim=c(5, 2)), inv.phi= structure( .Data= c(1.96E+00, -1.37E+00, -1.37E+00, 1.96E+00), .Dim=c(2, 2)), inv.psi=c(5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau&quot;, &quot;lambda[2,1]&quot;,&quot;lambda[3,1]&quot;,&quot;lambda[5,2]&quot;, &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa2, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 3, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 514 ## Total graph size: 9035 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/Rtmpu2Wtns/model27604325135c.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 7500 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda[2,1] -0.192 1.385 -2.439 -2.002 0.735 0.786 0.873 15.478 3 ## lambda[3,1] -0.153 0.883 -1.599 -1.293 0.429 0.471 0.542 13.567 3 ## lambda[5,2] 0.928 0.145 0.815 0.880 0.915 0.955 1.043 1.008 590 ## phi[1,1] 0.273 0.161 0.042 0.059 0.353 0.397 0.467 7.702 3 ## phi[2,1] 0.215 0.231 -0.129 -0.097 0.353 0.389 0.448 10.412 3 ## phi[1,2] 0.215 0.231 -0.129 -0.097 0.353 0.389 0.448 10.412 3 ## phi[2,2] 0.492 0.065 0.391 0.459 0.494 0.529 0.601 1.002 1900 ## psi[1] 0.558 0.277 0.315 0.355 0.383 0.898 1.039 9.987 3 ## psi[2] 0.173 0.159 0.144 0.160 0.170 0.180 0.202 1.034 1600 ## psi[3] 0.169 0.221 0.141 0.157 0.166 0.175 0.194 1.225 16 ## psi[4] 0.344 0.130 0.283 0.319 0.339 0.360 0.411 1.012 740 ## psi[5] 0.246 0.158 0.203 0.228 0.242 0.257 0.290 1.018 1600 ## tau[1] 3.333 0.041 3.250 3.306 3.332 3.360 3.413 1.007 7500 ## tau[2] 3.897 0.028 3.841 3.878 3.897 3.916 3.951 1.001 4700 ## tau[3] 4.596 0.022 4.551 4.581 4.596 4.611 4.639 1.001 7500 ## tau[4] 3.034 0.041 2.953 3.006 3.034 3.061 3.116 1.002 1800 ## tau[5] 3.713 0.036 3.643 3.689 3.713 3.738 3.783 1.002 2700 ## deviance 3340.837 253.362 3081.261 3163.988 3220.955 3591.817 3741.760 3.914 3 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 8675.6 and DIC = 12016.4 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;lambda[2,1]&quot;,&quot;lambda[3,1]&quot;,&quot;lambda[5,2]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title ## Error: Some &#39;pars&#39; don&#39;t match parameter names: phi FALSE "],["stan-two-latent-variable.html", "9.6 Stan - Two Latent Variable", " 9.6 Stan - Two Latent Variable 9.6.1 Inverse-Wishart Prior Using Stan based on a nearly identical model structure presented in the text. model_cfa_2factor &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; matrix[M, M] phi0; } parameters { matrix[M, M] phi; // latent variable covaraince matrix matrix[N, M] ksi; //latent variable values real lambda[J]; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); // prior for ksi ksi[i] ~ multi_normal(rep_vector(0, M), phi); } // latent variable variance matrix phi ~ inv_wishart(2, phi0); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat, phi0 = matrix(c(1, .3, .3, 1), ncol=2) ) # # initial values start_values &lt;- list( list( phi= structure( .Data= c(1, 0.30, 0.30, 1), .Dim=c(2, 2)), tau = c(3, 3, 3, 3, 3), lambda= c(1, 1, 1, 1, 1), psi=c(.5, .5, .5, .5, .5) ), list( phi= structure( .Data= c(1, 0, 0, 1), .Dim=c(2, 2)), tau = c(5, 5, 5, 5, 5), lambda= c(1, .7, .7, 1, .7), psi=c(2, 2, 2, 2, 2) ), list( phi= structure( .Data= c(1, 0.10, 0.10, 1), .Dim=c(2, 2)), tau = c(1, 1, 1, 1, 1), lambda= c(1, 1.3, 1.3, 1, 1.3), psi=c(1, 1, 1, 1, 1) ) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa_2factor, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) control = list(adapt_delta = 0.9, max_treedepth = 12), refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found ## Warning: There were 14910 divergent transitions after warmup. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## to find out why this is a problem and how to eliminate them. ## Warning: There were 90 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 12. See ## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded ## Warning: Examine the pairs() plot to diagnose sampling problems ## Warning: The largest R-hat is 3.8, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 9cba7ccbea0f9cb6bfe744cf6be59b64. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## lambda[1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## lambda[2] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## lambda[3] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## lambda[4] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## lambda[5] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 ## tau[1] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[2] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[3] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[4] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## tau[5] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 ## psi[1] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[2] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[3] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[4] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## psi[5] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 ## phi[1,1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 5 ## phi[1,2] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 ## phi[2,1] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 ## phi[2,2] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 ## ksi[1,1] 0.81 1.01 1.24 -0.91 -0.91 1.43 1.93 1.93 2 ## ksi[1,2] 0.62 0.72 0.89 -0.57 -0.57 0.85 1.56 1.56 2 ## ksi[8,1] 0.14 0.62 0.76 -0.84 -0.84 0.27 1.00 1.00 2 ## ksi[8,2] -0.12 0.17 0.20 -0.34 -0.34 -0.17 0.15 0.15 2 ## Rhat ## lambda[1] 3.76 ## lambda[2] 200082.02 ## lambda[3] 110868.47 ## lambda[4] 3.24 ## lambda[5] 94909.59 ## tau[1] 971982.86 ## tau[2] 1168347.77 ## tau[3] 1446723.31 ## tau[4] 842172.13 ## tau[5] 1190014.95 ## psi[1] 277869.53 ## psi[2] 256094.75 ## psi[3] 333243.02 ## psi[4] 272051.82 ## psi[5] 201375.90 ## phi[1,1] 1.62 ## phi[1,2] 103038.69 ## phi[2,1] 103035.55 ## phi[2,2] 2.68 ## ksi[1,1] 536377.12 ## ksi[1,2] 706233.82 ## ksi[8,1] 519297.43 ## ksi[8,2] 159190.73 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 04:23:50 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot( fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) 9.6.2 LKJ Cholesky Parameterization Because I had such massive problems with the above, I search for how people estimate CFA models in Stan. I found that most people use the LKJ Cholesky parameterization. Some helpful pages that I used to help get this to work. Stan Users Guide on Factor Covaraince Parameterization Michael DeWitt - Confirmatory Factor Analysis in Stan Rick Farouni - Fitting a Bayesian Factor Analysis Model in Stan model_cfa2 &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; } parameters { cholesky_factor_corr[M] L; // Cholesky decomp of // corr mat of random slopes vector[M] A; // Vector of factor variances matrix[N, M] ksi; //latent variable values vector[J] lambda; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } transformed parameters { matrix[M, M] A0; vector[M] S; A0 = diag_pre_multiply(A, L); S = sqrt(A); } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); } // latent variable parameters A ~ inv_gamma(5, 10); L ~ lkj_corr_cholesky(M); for(i in 1:N){ ksi[i] ~ multi_normal_cholesky(rep_vector(0, M), A0); } // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); // factor loading patterns lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } generated quantities { matrix[M, M] R; matrix[M, M] phi; R = tcrossprod(L); phi = quad_form_diag(R, S); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa2, # model code to be compiled data = mydata, # my data #init = init_fun, #start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = ## TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;R&quot;, &quot;A&quot;, &quot;A0&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 09d22e8b8116ca5395ea85ab1df26f50. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 16739 1 ## lambda[2] 0.87 0 0.06 0.76 0.83 0.87 0.91 0.99 2092 1 ## lambda[3] 0.52 0 0.04 0.44 0.49 0.52 0.54 0.60 3029 1 ## lambda[4] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 16022 1 ## lambda[5] 0.96 0 0.05 0.86 0.92 0.96 1.00 1.07 2956 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 4249 1 ## tau[2] 3.90 0 0.03 3.84 3.88 3.90 3.92 3.95 3252 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 4964 1 ## tau[4] 3.03 0 0.04 2.95 3.01 3.03 3.06 3.11 3855 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 3324 1 ## psi[1] 0.61 0 0.02 0.56 0.59 0.61 0.62 0.65 10217 1 ## psi[2] 0.32 0 0.02 0.28 0.31 0.32 0.33 0.36 3488 1 ## psi[3] 0.36 0 0.01 0.33 0.35 0.36 0.36 0.38 11296 1 ## psi[4] 0.57 0 0.03 0.52 0.55 0.57 0.58 0.62 7958 1 ## psi[5] 0.42 0 0.03 0.37 0.41 0.42 0.44 0.47 4437 1 ## R[1,1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## R[1,2] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2366 1 ## R[2,1] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2366 1 ## R[2,2] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## A[1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## A[2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 3536 1 ## A0[1,1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## A0[1,2] 0.00 NaN 0.00 0.00 0.00 0.00 0.00 0.00 NaN NaN ## A0[2,1] 0.61 0 0.04 0.53 0.58 0.60 0.63 0.68 4887 1 ## A0[2,2] 0.36 0 0.04 0.29 0.34 0.36 0.39 0.43 1869 1 ## phi[1,1] 0.60 0 0.04 0.53 0.58 0.60 0.62 0.67 2717 1 ## phi[1,2] 0.56 0 0.03 0.49 0.54 0.56 0.58 0.62 3882 1 ## phi[2,1] 0.56 0 0.03 0.49 0.54 0.56 0.58 0.62 3882 1 ## phi[2,2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 3536 1 ## ksi[1,1] -0.22 0 0.23 -0.67 -0.38 -0.22 -0.07 0.24 24053 1 ## ksi[1,2] -0.36 0 0.28 -0.91 -0.55 -0.37 -0.18 0.18 22191 1 ## ksi[8,1] 0.91 0 0.23 0.46 0.75 0.91 1.06 1.38 19097 1 ## ksi[8,2] 0.88 0 0.27 0.34 0.70 0.88 1.07 1.42 23361 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Apr 05 04:46:37 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi[1,1]&quot;, &quot;phi[1,2]&quot;, &quot;phi[2,2]&quot;), prob = 0.8) + plot_title "],["blavaan-two-latent-variables.html", "9.7 Blavaan - Two Latent Variables", " 9.7 Blavaan - Two Latent Variables # model model_cfa2_blavaan &lt;- &quot; f1 =~ 1*PI + AD + IGC f2 =~ 1*FI + FC f1 ~~ f2 &quot; dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) fit &lt;- blavaan::bcfa(model_cfa2_blavaan, data=dat) ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.001 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 1: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 1: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 1: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 1: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 1: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 1: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 1: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 1: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 1: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 1: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 1: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 6.738 seconds (Warm-up) ## Chain 1: 13.97 seconds (Sampling) ## Chain 1: 20.708 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 2: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 2: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 2: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 2: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 2: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 2: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 2: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 2: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 2: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 2: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 2: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 7.142 seconds (Warm-up) ## Chain 2: 13.648 seconds (Sampling) ## Chain 2: 20.79 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;stanmarg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1500 [ 0%] (Warmup) ## Chain 3: Iteration: 150 / 1500 [ 10%] (Warmup) ## Chain 3: Iteration: 300 / 1500 [ 20%] (Warmup) ## Chain 3: Iteration: 450 / 1500 [ 30%] (Warmup) ## Chain 3: Iteration: 501 / 1500 [ 33%] (Sampling) ## Chain 3: Iteration: 650 / 1500 [ 43%] (Sampling) ## Chain 3: Iteration: 800 / 1500 [ 53%] (Sampling) ## Chain 3: Iteration: 950 / 1500 [ 63%] (Sampling) ## Chain 3: Iteration: 1100 / 1500 [ 73%] (Sampling) ## Chain 3: Iteration: 1250 / 1500 [ 83%] (Sampling) ## Chain 3: Iteration: 1400 / 1500 [ 93%] (Sampling) ## Chain 3: Iteration: 1500 / 1500 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 6.865 seconds (Warm-up) ## Chain 3: 12.483 seconds (Sampling) ## Chain 3: 19.348 seconds (Total) ## Chain 3: ## Computing posterior predictives... summary(fit) ## blavaan (0.3-15) results of 1000 samples after 500 adapt/burnin iterations ## ## Number of observations 500 ## ## Number of missing patterns 1 ## ## Statistic MargLogLik PPP ## Value -2174.086 0.000 ## ## Latent Variables: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 =~ ## PI 1.000 NA ## AD 0.959 0.069 0.833 1.101 1.000 normal(0,10) ## IGC 0.563 0.046 0.478 0.656 1.000 normal(0,10) ## f2 =~ ## FI 1.000 NA ## FC 1.007 0.062 0.888 1.137 1.000 normal(0,10) ## ## Covariances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## f1 ~~ ## f2 0.312 0.035 0.249 0.384 1.000 lkj_corr(1) ## ## Intercepts: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 3.334 0.038 3.258 3.41 1.000 normal(0,32) ## .AD 3.898 0.027 3.846 3.95 1.001 normal(0,32) ## .IGC 4.597 0.021 4.556 4.636 1.000 normal(0,32) ## .FI 3.035 0.039 2.959 3.111 1.001 normal(0,32) ## .FC 3.714 0.035 3.646 3.782 1.001 normal(0,32) ## f1 0.000 NA ## f2 0.000 NA ## ## Variances: ## Estimate Post.SD pi.lower pi.upper Rhat Prior ## .PI 0.382 0.030 0.327 0.446 0.999 gamma(1,.5)[sd] ## .AD 0.077 0.013 0.052 0.103 1.000 gamma(1,.5)[sd] ## .IGC 0.116 0.009 0.1 0.135 0.999 gamma(1,.5)[sd] ## .FI 0.322 0.029 0.268 0.38 0.999 gamma(1,.5)[sd] ## .FC 0.151 0.024 0.104 0.199 1.000 gamma(1,.5)[sd] ## f1 0.311 0.041 0.234 0.394 1.000 gamma(1,.5)[sd] ## f2 0.465 0.051 0.372 0.569 1.000 gamma(1,.5)[sd] plot(fit) "],["indeterminacy-in-one-factor-cfa.html", "9.8 Indeterminacy in One Factor CFA", " 9.8 Indeterminacy in One Factor CFA # model code jags.model.cfa.ind &lt;- function(){ ###################################################################### # Specify the factor analysis measurement model for the observables ###################################################################### for (i in 1:n){ for(j in 1:J){ mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # model implied expectation for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # distribution for each observable } } ###################################################################### # Specify the (prior) distribution for the latent variables ###################################################################### for (i in 1:n){ ksi[i] ~ dnorm(kappa, inv.phi) # distribution for the latent variables } ###################################################################### # Specify the prior distribution for the parameters that govern the latent variables ###################################################################### kappa &lt;- 0 # Mean of factor 1 inv.phi &lt;-1 # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ###################################################################### # Specify the prior distribution for the measurement model parameters ###################################################################### for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } for (j in 1:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # initial values start_values &lt;- list( list(&quot;tau&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;lambda&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;inv.psi&quot;=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)), list(&quot;tau&quot;=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00), &quot;lambda&quot;=c(-3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00), &quot;inv.psi&quot;=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)) ) # vector of all parameters to save # exclude fixed lambda since it throws an error in # in the GRB plot param_save &lt;- c(&quot;tau[1]&quot;, &quot;lambda[1]&quot;, &quot;phi&quot;, &quot;psi[1]&quot;, &quot;ksi[8]&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa.ind, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=5000, n.burnin = 2500, n.chains = 2, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpAH8d3B/model2a0046052851.txt&quot;, fit using jags, ## 2 chains, each with 5000 iterations (first 2500 discarded) ## n.sims = 5000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## ksi[8] -0.009 1.414 -2.045 -1.363 0.054 1.322 2.055 8.057 2 ## lambda[1] -0.002 0.597 -0.652 -0.591 -0.017 0.588 0.649 21.322 2 ## phi 1.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000 1 ## psi[1] 0.382 0.029 0.329 0.362 0.381 0.401 0.440 1.005 400 ## tau[1] 3.333 0.039 3.257 3.308 3.333 3.359 3.408 1.001 5000 ## deviance 3384.995 63.370 3300.938 3353.736 3383.034 3413.631 3473.403 1.001 5000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 2008.3 and DIC = 5393.3 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) "],["model-evaluation.html", "Chapter 10 Model Evaluation", " Chapter 10 Model Evaluation The goal of evaluating the model is to determine if the inferences suggested by the model are reasonable based ones content area knowledge. The text and (BDA3) say that this evaluation of reasonableness is really a form of prior, that is Gelman et al. (2013) argued tha when analysts deem that the posterior distribution and inferences are unreasonable, what they are really expressing is that there is additional information available that was not included in the analysis. Having some knowledge about what the posterior distribution should look like can be extremely helpful in developing how the model takes shape (e.g., setting reasonable boundaries on parameters). Three aspects/questions we aim to address as part of the logic of model checking: What is going on in our data, possibly in relation to the model? What the model has to say about what should be going on? How what is going on in the data compared to what the model says should be going on? Model evaluation is accomplished through Residual analysis, Posterior predictive distributions, and Model comparisons. "],["residual-analysis.html", "10.1 Residual Analysis", " 10.1 Residual Analysis A residual is a key component of analysis in many statistical procedures. Here, we describe how this important feature can be used in Bayesian psychometric analysis. At a fundamental level, a residual is one component in the data-model relationship, that is \\[DATA = MODEL + RESIDUALS.\\] This view highlights the three major pieces we are going to use in the model evaluation process. We can rewrite the above to focus on the residuals as well. In factor analysis (CFA in particular), we tend to be interested in the residuals of the covariances among variables because our factor model is a hypothesis for how the observed variables are interrelated. In the observed data, we capture this relationship with the observed sample covariance matrix \\(\\mathbf{S}\\), which for variable \\(j\\) and \\(k\\) is usually computed as \\[s_{jk} = \\frac{\\sum_{\\forall i (x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)}}{n-1}.\\] And, the whole covariance matrix is \\(\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\prime}\\mathbf{X}\\) where \\(\\mathbf{X}\\) is in centered form. In CFA, we can get the model implied covariance matrix by using the estimated parameters to get \\(\\Sigma(\\theta)\\), that is \\[\\Sigma(\\theta) = \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi.\\] Then, to get the residual matrix we simple find the difference scores (\\(\\mathbf{E}\\)) between these matrices \\[\\mathbf{E} = \\mathbf{S} - \\Sigma(\\theta).\\] Although, we would probably want to rescale \\(\\mathbf{S}\\) and \\(\\Sigma(\\theta)\\) to be correlation matrices because the scale of the covariances and variances are likely not of primary interest. This means we can use the residuals correlations instead. Generating the residual correlations is accomplished using the posterior predictive distribution. "],["posterior-predictive-distributions.html", "10.2 Posterior Predictive Distributions", " 10.2 Posterior Predictive Distributions The posterior predictive distribution is used heavily in model evaluation. (look at Bayes notes) Basically, the posterior predictive distribution is the what values of the observed data (\\(Y\\)) are mostly likely given the posterior distribution. 10.2.1 Example of posterior predictive distribution of correlations In this example, we use the correlations of the observed variable as the function of interest. # model code jags.model.cfa &lt;- function(){ # # Specify the factor analysis measurement # model for the observables # for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;) # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, parameters.to.save = param_save, n.iter=15000, n.burnin = 5000, n.chains = 1, # for simplicity n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpUNZc83/model3bfc8bd33b6.txt&quot;, fit using jags, ## 1 chains, each with 15000 iterations (first 5000 discarded) ## n.sims = 10000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.729 0.045 0.645 0.698 0.727 0.759 0.822 ## lambda[3] 0.420 0.038 0.349 0.395 0.420 0.446 0.495 ## lambda[4] 1.052 0.065 0.929 1.007 1.051 1.096 1.182 ## lambda[5] 0.985 0.059 0.874 0.944 0.983 1.024 1.104 ## phi 0.435 0.042 0.357 0.405 0.433 0.462 0.523 ## psi[1] 0.373 0.028 0.321 0.354 0.372 0.391 0.431 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.191 0.211 ## psi[3] 0.180 0.012 0.157 0.171 0.179 0.187 0.204 ## psi[4] 0.377 0.030 0.323 0.356 0.376 0.396 0.440 ## psi[5] 0.266 0.022 0.225 0.251 0.265 0.280 0.310 ## tau[1] 3.334 0.040 3.254 3.306 3.334 3.360 3.413 ## tau[2] 3.897 0.029 3.841 3.878 3.897 3.917 3.952 ## tau[3] 4.596 0.023 4.552 4.581 4.596 4.611 4.640 ## tau[4] 3.034 0.041 2.955 3.006 3.034 3.061 3.114 ## tau[5] 3.713 0.037 3.641 3.688 3.713 3.738 3.786 ## deviance 3379.345 42.258 3298.273 3350.553 3379.160 3407.319 3463.922 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 892.9 and DIC = 4272.2 ## DIC is an estimate of expected predictive error (lower deviance is better). plot(fit) # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;tau[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;, 1:5, &quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # compute model implied covariance/correlations # for each iterations out.mat &lt;- matrix(ncol=10,nrow=nrow(plot.data)) colnames(out.mat) &lt;- c(&quot;r12&quot;, &quot;r13&quot;, &quot;r14&quot;, &quot;r15&quot;, &quot;r23&quot;, &quot;r24&quot;, &quot;r25&quot;, &quot;r34&quot;,&quot;r35&quot;, &quot;r45&quot;) plot.data1 &lt;- cbind(plot.data,out.mat) # compute the model implied correlations for each iterations i &lt;- 1 for(i in 1:nrow(plot.data1)){ x &lt;- plot.data1[i,] x &lt;- unlist(x) lambda &lt;- matrix(x[4:8], ncol=1) phi &lt;- matrix(x[9], ncol=1) psi &lt;- diag(x[10:14], ncol=5, nrow=5) micov &lt;- lambda%*%phi%*%t(lambda)+psi D &lt;- diag(sqrt(diag(micov)), ncol=5, nrow=5) Dinv &lt;- solve(D) micor &lt;- Dinv%*%micov%*%Dinv outr &lt;- micor[lower.tri(micor)] # combine plot.data1[i,20:29] &lt;- outr } obs.mat &lt;- matrix(cor(dat)[lower.tri(cor(dat))],byrow=T, ncol=10,nrow=nrow(plot.data)) colnames(obs.mat) &lt;- c(&quot;ObsR12&quot;, &quot;ObsR13&quot;, &quot;ObsR14&quot;, &quot;ObsR15&quot;, &quot;ObsR23&quot;, &quot;ObsR24&quot;, &quot;ObsR25&quot;, &quot;ObsR34&quot;,&quot;ObsR35&quot;, &quot;ObsR45&quot;) plot.data1 &lt;- cbind(plot.data1,obs.mat) theme_set(theme_classic()) t1 &lt;- grid::textGrob(&#39;PI&#39;) t2 &lt;- grid::textGrob(&#39;AD&#39;) t3 &lt;- grid::textGrob(&#39;IGC&#39;) t4 &lt;- grid::textGrob(&#39;FI&#39;) t5 &lt;- grid::textGrob(&#39;FC&#39;) p12 &lt;- ggplot(plot.data1) + geom_density(aes(x=r12))+ geom_vline(aes(xintercept = ObsR12), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p13 &lt;- ggplot(plot.data1) + geom_density(aes(x=r13))+ geom_vline(aes(xintercept = ObsR13), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p14 &lt;- ggplot(plot.data1) + geom_density(aes(x=r14))+ geom_vline(aes(xintercept = ObsR14), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p15 &lt;- ggplot(plot.data1) + geom_density(aes(x=r15))+ geom_vline(aes(xintercept = ObsR15), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p23 &lt;- ggplot(plot.data1) + geom_density(aes(x=r23))+ geom_vline(aes(xintercept = ObsR23), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p24 &lt;- ggplot(plot.data1) + geom_density(aes(x=r24))+ geom_vline(aes(xintercept = ObsR24), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p25 &lt;- ggplot(plot.data1) + geom_density(aes(x=r25))+ geom_vline(aes(xintercept = ObsR25), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p34 &lt;- ggplot(plot.data1) + geom_density(aes(x=r34))+ geom_vline(aes(xintercept = ObsR34), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p35 &lt;- ggplot(plot.data1) + geom_density(aes(x=r35))+ geom_vline(aes(xintercept = ObsR35), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) p45 &lt;- ggplot(plot.data1) + geom_density(aes(x=r45))+ geom_vline(aes(xintercept = ObsR45), linetype=&quot;dashed&quot;)+ lims(x=c(0.25, 0.75)) + labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank()) layout &lt;- &#39; A#### BC### DEF## GHIJ# KLMNO &#39; wrap_plots(A=t1,C=t2,F=t3,J=t4,O=t5, B=p12,D=p13,G=p14,K=p15, E=p23,H=p24,L=p25, I=p34,M=p35, N=p45,design = layout) 10.2.2 PPD SRMR # model code jags.model.cfa &lt;- function(){ ######################################## # Specify the factor analysis measurement # model for the observables ######################################## for (i in 1:n){ for(j in 1:J){ # model implied expectation for each observable mu[i,j] &lt;- tau[j] + ksi[i]*lambda[j] # distribution for each observable x[i,j] ~ dnorm(mu[i,j], inv.psi[j]) # Posterior Predictive Distribution of x # needed for SRMR # set mean to 0 # x.ppd[i,j] ~ dnorm(0, inv.psi[j]) } } ######################################## # Specify the (prior) distribution for # the latent variables ######################################## for (i in 1:n){ # distribution for the latent variables ksi[i] ~ dnorm(kappa, inv.phi) } ######################################## # Specify the prior distribution for the # parameters that govern the latent variables ######################################## kappa &lt;- 0 # Mean of factor 1 inv.phi ~ dgamma(5, 10) # Precision of factor 1 phi &lt;- 1/inv.phi # Variance of factor 1 ######################################## # Specify the prior distribution for the # measurement model parameters ######################################## for(j in 1:J){ tau[j] ~ dnorm(3, .1) # Intercepts for observables inv.psi[j] ~ dgamma(5, 10) # Precisions for observables psi[j] &lt;- 1/inv.psi[j] # Variances for observables } lambda[1] &lt;- 1.0 # loading fixed to 1.0 for (j in 2:J){ lambda[j] ~ dnorm(1, .1) # prior distribution for the remaining loadings } } # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( n = 500, J = 5, x = as.matrix(dat) ) # vector of all parameters to save param_save &lt;- c(&quot;tau&quot;, paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;, &quot;psi&quot;)# &quot;Sigma&quot;, # fit model fit &lt;- jags( model.file=jags.model.cfa, data=mydata, #inits=start_values, parameters.to.save = param_save, n.iter=10000, n.burnin = 5000, n.chains = 1, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 2500 ## Unobserved stochastic nodes: 515 ## Total graph size: 8029 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpUNZc83/model3bfc6075631.txt&quot;, fit using jags, ## 1 chains, each with 10000 iterations (first 5000 discarded) ## n.sims = 5000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% ## lambda[1] 1.000 0.000 1.000 1.000 1.000 1.000 1.000 ## lambda[2] 0.728 0.045 0.643 0.696 0.727 0.757 0.819 ## lambda[3] 0.419 0.037 0.348 0.395 0.419 0.444 0.492 ## lambda[4] 1.051 0.066 0.927 1.006 1.049 1.096 1.183 ## lambda[5] 0.986 0.059 0.873 0.945 0.985 1.025 1.107 ## phi 0.436 0.043 0.359 0.406 0.433 0.462 0.527 ## psi[1] 0.373 0.028 0.320 0.354 0.372 0.391 0.433 ## psi[2] 0.183 0.014 0.157 0.173 0.182 0.192 0.211 ## psi[3] 0.180 0.012 0.158 0.171 0.179 0.188 0.205 ## psi[4] 0.377 0.030 0.321 0.356 0.375 0.396 0.437 ## psi[5] 0.265 0.022 0.225 0.250 0.264 0.279 0.311 ## tau[1] 3.332 0.040 3.254 3.305 3.332 3.360 3.411 ## tau[2] 3.898 0.029 3.843 3.878 3.898 3.917 3.953 ## tau[3] 4.596 0.023 4.551 4.581 4.596 4.611 4.640 ## tau[4] 3.033 0.042 2.951 3.005 3.033 3.062 3.113 ## tau[5] 3.712 0.037 3.640 3.688 3.712 3.737 3.786 ## deviance 3379.601 42.607 3298.725 3350.919 3379.051 3407.511 3466.042 ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 907.7 and DIC = 4287.3 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit)[[1]] # function for estimating SRMR SRMR.function &lt;- function(data.cov.matrix, mod.imp.cov.matrix){ J=nrow(data.cov.matrix) temp &lt;- matrix(NA, nrow=J, ncol=J) for(j in 1:J){ for(jprime in 1:j){ temp[j, jprime] &lt;- ((data.cov.matrix[j, jprime] - mod.imp.cov.matrix[j, jprime])/(data.cov.matrix[j, j] * data.cov.matrix[jprime, jprime]))^2 } } SRMR &lt;- sqrt((2*sum(temp,na.rm=TRUE))/(J*(J+1))) SRMR } # set up the parameters for # (1) model implied covariance # (2) PPD of x/covariance iter &lt;- nrow(jags.mcmc) srmr.realized &lt;- rep(NA, iter) srmr.ppd &lt;- rep(NA, iter) srmr.rpv &lt;- rep(NA, iter) jags.mcmc &lt;- cbind(jags.mcmc, srmr.realized, srmr.ppd, srmr.rpv) N &lt;- 500; J &lt;- 5; M &lt;- 1 cov.x &lt;- cov(mydata$x) i &lt;- 1 for(i in 1:iter){ # set up parameters x &lt;- jags.mcmc[i,] lambda &lt;- matrix(x[2:6], ncol=M, nrow=J) phi &lt;- matrix(x[7], ncol=M, nrow=M) psi &lt;- diag(x[8:12], ncol=J, nrow=J) # estimate model implied covariance matrix cov.imp &lt;- lambda%*%phi%*%t(lambda) + psi # get posterior predicted observed x x.ppd &lt;- mvtnorm::rmvnorm(N, mean=rep(0, J), sigma=cov.imp) # compute posterior predictied covariance matrix cov.ppd &lt;- cov(x.ppd) # estimate SRMR values jags.mcmc[i,18] &lt;- SRMR.function(cov.x, cov.imp)# srmr realized jags.mcmc[i,19] &lt;- SRMR.function(cov.ppd, cov.imp)# srmr ppd # posterior predicted p-value of realized SRMR being &lt;= 0.08. jags.mcmc[i,20] &lt;- ifelse(jags.mcmc[i,18]&lt;=0.08, 1, 0) } plot.dat &lt;- as.data.frame(jags.mcmc) p1 &lt;- ggplot(plot.dat, aes(x=srmr.realized, y=srmr.ppd))+ geom_point()+ geom_abline(slope=1, intercept = 0)+ lims(x=c(0,1),y=c(0,1))+ labs(x=&quot;Realized SRMR&quot;,y=&quot;Posterior Predicted SRMR&quot;) + theme_bw()+theme(panel.grid = element_blank()) p2 &lt;- ggplot(plot.dat, aes(x=srmr.realized))+ geom_density()+ lims(x=c(0,1))+ labs(x=&quot;Realized SRMR&quot;, y=NULL) + annotate(&quot;text&quot;, x = 0.75, y = 3, label = paste0(&quot;Pr(SRMR &lt;= 0.08)= &quot;, round(mean(plot.dat$srmr.rpv), 2))) + theme_bw()+theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) p1 + p2 "],["model-comparison.html", "10.3 Model Comparison", " 10.3 Model Comparison Bayes Factors (BF) Conditional predictive ordinate (CPO) Information criteria Entropy "],["item-response-theory.html", "Chapter 11 Item Response Theory", " Chapter 11 Item Response Theory Item response theory (IRT) builds models for item (stimuli that the measures collected) based on two broad classes of models Models for dichotomous (binary, 0/1) items and Models for polytomous (multi-category) items. "],["irt-models-for-dichotomous-data.html", "11.1 IRT Models for Dichotomous Data", " 11.1 IRT Models for Dichotomous Data First, for conventional dichotomous observed variables, an IRT model can be generally specified as follows. Let \\(x_{ij}\\) be the observed value from respondent \\(i\\) on observable (item) \\(j\\). When \\(x\\) is binary, the observed value can be \\(0\\) or \\(1\\). Some common IRT models for binary observed variables can be expressed as a version of \\[p(x_{ij} = 1 \\mid \\theta_i, d_j, a_j, c_j) = c_j + (1-c_j)F(a_j \\theta_i + d_j),\\] where, \\(\\theta_i\\) is the magnitude of the latent variable that individual \\(i\\) possesses. In educational measurement, \\(\\theta_i\\) commonly represents proficiency so that a higher \\(\\theta_i\\) means that individual has more of the trait, \\(d_j\\) is the item location or difficulty parameter. \\(d_j\\) is commonly transformed to be \\(d_j = -a_jb_j\\) so that the location parameter is easier to interpret in relation to the latent trait \\(\\theta_i\\), \\(a_j\\) is the item slope or discrimination parameter, \\(c_j\\) is the item lower asymptote or guessing parameter, \\(F(.)\\) is the link function to be specified that determines the form of the transformation between latent trait and the item response probability. The link is common chosen to be either the logistic link or the normal-ogive link. Common IRT models are the 1-PL, or one-parameter logistic model, which only uses one measurement parameter \\(d_j\\) per item, 2-PL, or two-parameter logistic model, which uses two measurement model parameters \\(a_j\\) and \\(d_j\\) per item, 3-PL, or three-parameter logistic model, which uses all three parameters as shown above. other models are also possible for binary item response formats but are omitted here. The above describes the functional form used to model why individual may have a greater or lesser likelihood of endorsing an item (have a \\(1\\) as a measure). We use the above model as the basis for defining the conditional probability of any response given the values of the parameters. The conditional probability is then commonly used as part of a marginal maximum likelihood (MML) approach to finding parameters values for the measurement model which maximize the likelihood. However, given that the values of the latent variable \\(\\theta_i\\) are also unknown, the distribution of \\(\\theta_i\\) is marginalized out of the likelihood function. However, in the Bayesian formulation, we can side-step some of these issues by the use of prior distributions. Starting with the general form of the likelihood function \\[p(\\mathbf{x}\\mid \\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = \\prod_{i=1}^np(\\mathbf{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{i=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\] where \\[x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j \\sim \\mathrm{Bernoulli}[p(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j)].\\] Developing a joint prior distribution for \\(p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega})\\) is not straightforward given the high dimensional aspect of the components. But, a common assumption is that the distribution for the latent variables (\\(\\boldsymbol{\\theta}\\)) is independent of the distribution for the measurement model parameters (\\(\\boldsymbol{\\omega}\\)). That is, we can separate the problem into independent priors \\[p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = p(\\boldsymbol{\\theta})p(\\boldsymbol{\\omega}).\\] For the latent variables, the prior distributuion is generally build by assuming that all individuals are also independent. The independence of observations leads to a joint prior that is a product of priors with a common distribution, \\[p(\\boldsymbol{\\theta}) = \\prod_{i=1}^np(\\theta_i\\mid \\boldsymbol{\\theta}_p),\\] where \\(\\boldsymbol{\\theta}_p\\) are the hyperparameters governing the common prior for the latent variable distribution. A common choice is that \\(\\theta_i \\sim \\mathrm{Normal}(\\mu_{\\theta} = 0, \\sigma^2_{\\theta}=1)\\) because the distribution is arbitrary. For the measurement model parameters, a bit more complex specification is generally needed. One simple approach would be to invoke an exchangeability assumption among items and among item parameters. This would essentially make all priors independent and simplify the specification to product of univariate priors over all measurement model parameters \\[p(\\boldsymbol{\\omega}) = \\prod_{j=1}^Jp(\\boldsymbol{\\omega}_j)=\\prod_{j=1}^Jp(d_j)p(a_j)p(c_j).\\] For for location parameter (\\(d_j\\)), a common prior distribution is an unbounded normal distribution. Because, the location can take on any value within the range of the latent variable which is also technically unbounded so we let \\[d_j \\sim \\mathrm{Normal}(\\mu_{d},\\sigma^2_d).\\] The choice of hyperparameters can be guided by prior research or set to a common relative diffuse value for all items such as \\(\\mu_{d}=0,\\sigma^2_d=10\\). The discrimination parameter governs the strength of the relationship between the latent variable and the probability of endorsing the item. This is similar in flavor to a factor loading in CFA. An issue with specifying a prior for the discrimination parameter is the indeterminacy with respect the the orientation of the latent variable. In CFA, we resolved the orientation indeterminacy issue by fixing one factor loading to 1. In IRT, we can do so by constraining the possible values of the discrimination parameters to be strictly positive. This forces each item to have the meaning of higher values on the latent variable directly (or at least proportionally) increase the probability of endorsing the item. We achieve this by using a prior such as \\[a_j \\sim \\mathrm{Normal}^{+}(\\mu_a,\\sigma^2_a).\\] The term \\(\\mathrm{Normal}^{+}(.)\\) means that the normal distribution is truncated at 0 so that only positive values are possible. Lastly, the guessing parameter \\(c_j\\) takes on values between \\([0,1]\\). A common choice for parameters bounded between 0 and 1 is a Beta prior, that is \\[c_j \\sim \\mathrm{Beta}(\\alpha_c, \\beta_c).\\] The hyperparameters \\(\\alpha_c\\) and \\(\\beta_c\\) determine the shape of the beta prior and affect the likelihood and magnitude of guessing parameters. "],["pl-lsat-example.html", "11.2 3-PL LSAT Example", " 11.2 3-PL LSAT Example In the Law School Admission Test (LSAT) example (p. 263-271), the data are from 1000 examinees responding to five items which is just a subset of the LSAT. We hypothesize that only one underlying latent variable is measured by these items. But that guessing is also plausible. The full 3-PL model we will use can be described in an equation as \\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{a}, \\boldsymbol{c} \\mid \\mathbf{x}) \\propto \\prod_{i=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)p(\\theta_i)p(d_j)p(a_j)p(c_j),\\] where \\[\\begin{align*} x_{ij}\\mid\\theta_i\\mid\\theta_i, d_j, a_j, c_j &amp;\\sim \\mathrm{Bernoulli}[p(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)],\\ \\mathrm{for}\\ i=1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\ p(\\theta_i\\mid\\theta_i, d_j, a_j, c_j) &amp;= c_j + (1-c_j)\\Phi(a_j\\theta_j + d_j),\\ \\mathrm{for}\\ i=1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\ \\theta_i &amp;\\sim \\mathrm{Normal}(0,1),\\ \\mathrm{for}\\ i = 1, \\cdots, 1000;\\\\ d_j &amp;\\sim \\mathrm{Normal}(0, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 5;\\\\ a_j &amp;\\sim \\mathrm{Normal}^{+}(1, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 5;\\\\ c_j &amp;\\sim \\mathrm{Beta}(5, 17),\\ \\mathrm{for}\\ j=1, \\cdots, 5. \\end{align*}\\] The above model can illustrated in a DAG as shown below. Figure 11.1: DAG for 3-PL IRT model for LSAT Example The path diagram for an IRT is essentially identical to the path diagram for a CFA model. This fact highlights an important feature of IRT/CFA in that the major conceptual difference between these approaches to is how we define the link between the latent variable the observed items. Figure 11.2: Path diagram for 3-PL IRT model For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 11.3: Model specification diagram for the 3-PL IRT model "],["lsat-example-jags.html", "11.3 LSAT Example - JAGS", " 11.3 LSAT Example - JAGS jags.model.lsat &lt;- function(){ ######################################### # Specify the item response measurement model for the observables ######################################### for (i in 1:n){ for(j in 1:J){ P[i,j] &lt;- c[j]+(1-c[j])*phi(a[j]*theta[i]+d[j]) # 3P-NO expression x[i,j] ~ dbern(P[i,j]) # distribution for each observable } } ########################################## # Specify the (prior) distribution for the latent variables ########################################## for (i in 1:n){ theta[i] ~ dnorm(0, 1) # distribution for the latent variables } ########################################## # Specify the prior distribution for the measurement model parameters ########################################## for(j in 1:J){ d[j] ~ dnorm(0, .5) # Locations for observables a[j] ~ dnorm(1, .5); I(0,) # Discriminations for observables c[j] ~ dbeta(5,17) # Lower asymptotes for observables } } # closes the model # initial values start_values &lt;- list( list(&quot;d&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;a&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;c&quot;=c(0.20, 0.20, 0.20, 0.20, 0.20)), list(&quot;d&quot;=c(-3.00, -3.00, -3.00, -3.00, -3.00), &quot;a&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;c&quot;=c(0.50, 0.50, 0.50, 0.50, 0.50)), list(&quot;d&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;a&quot;=c(0.1, 0.1, 0.1, 0.1, 0.1), &quot;c&quot;=c(0.05, 0.05, 0.05, 0.05, 0.05)) ) # vector of all parameters to save param_save &lt;- c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;, &quot;theta&quot;) # dataset dat &lt;- read.table(&quot;data/LSAT.dat&quot;, header=T) mydata &lt;- list( n = nrow(dat), J = ncol(dat), x = as.matrix(dat) ) # fit model fit &lt;- jags( model.file=jags.model.lsat, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=26000, n.burnin = 6000, n.chains = 3, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 5000 ## Unobserved stochastic nodes: 1015 ## Total graph size: 31027 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpW4ITDP/modelcd040fb7a9e.txt&quot;, fit using jags, ## 3 chains, each with 26000 iterations (first 6000 discarded), n.thin = 20 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.476 0.188 0.181 0.347 0.455 0.573 0.922 1.001 3000 ## a[2] 0.595 0.295 0.283 0.435 0.541 0.672 1.275 1.016 190 ## a[3] 1.253 0.734 0.427 0.706 1.025 1.625 3.146 1.006 370 ## a[4] 0.502 0.207 0.229 0.375 0.469 0.578 0.971 1.001 3000 ## a[5] 0.417 0.152 0.166 0.313 0.403 0.503 0.760 1.002 2200 ## c[1] 0.242 0.094 0.090 0.172 0.232 0.303 0.443 1.001 3000 ## c[2] 0.256 0.099 0.092 0.182 0.248 0.320 0.479 1.005 450 ## c[3] 0.262 0.082 0.099 0.201 0.268 0.322 0.406 1.003 770 ## c[4] 0.251 0.098 0.093 0.179 0.240 0.313 0.466 1.001 3000 ## c[5] 0.241 0.091 0.088 0.175 0.235 0.301 0.436 1.002 1800 ## d[1] 1.419 0.147 1.167 1.324 1.410 1.497 1.741 1.004 1500 ## d[2] 0.285 0.221 -0.214 0.202 0.330 0.420 0.552 1.045 250 ## d[3] -0.544 0.464 -1.698 -0.776 -0.429 -0.195 0.029 1.006 400 ## d[4] 0.515 0.157 0.147 0.438 0.537 0.619 0.746 1.001 2500 ## d[5] 1.024 0.115 0.784 0.954 1.032 1.104 1.234 1.003 780 ## theta[1] -1.608 0.775 -3.190 -2.101 -1.604 -1.073 -0.145 1.002 1600 ## theta[2] -1.623 0.778 -3.167 -2.166 -1.600 -1.054 -0.177 1.001 3000 ## theta[3] -1.625 0.768 -3.156 -2.156 -1.614 -1.083 -0.202 1.001 2100 ## theta[4] -1.307 0.773 -2.879 -1.810 -1.303 -0.774 0.180 1.001 3000 ## theta[5] -1.294 0.768 -2.832 -1.807 -1.297 -0.749 0.152 1.001 2300 ## theta[6] -1.303 0.766 -2.816 -1.811 -1.288 -0.772 0.163 1.003 830 ## theta[7] -1.287 0.767 -2.777 -1.796 -1.274 -0.770 0.197 1.002 1100 ## theta[8] -1.296 0.768 -2.877 -1.810 -1.264 -0.771 0.150 1.001 3000 ## theta[9] -1.315 0.768 -2.846 -1.815 -1.296 -0.791 0.155 1.001 3000 ## theta[10] -1.323 0.776 -2.911 -1.831 -1.307 -0.799 0.163 1.002 1200 ## theta[11] -1.295 0.769 -2.879 -1.805 -1.261 -0.762 0.128 1.001 3000 ## theta[12] -0.933 0.790 -2.530 -1.458 -0.922 -0.389 0.543 1.001 2300 ## theta[13] -0.948 0.766 -2.474 -1.458 -0.943 -0.433 0.545 1.001 3000 ## theta[14] -0.933 0.767 -2.442 -1.452 -0.910 -0.401 0.503 1.001 3000 ## theta[15] -0.962 0.764 -2.542 -1.478 -0.933 -0.409 0.457 1.003 900 ## theta[16] -0.940 0.774 -2.513 -1.446 -0.902 -0.398 0.497 1.001 3000 ## theta[17] -0.966 0.757 -2.436 -1.498 -0.968 -0.426 0.483 1.001 3000 ## theta[18] -0.935 0.778 -2.495 -1.448 -0.893 -0.396 0.505 1.001 3000 ## theta[19] -0.958 0.793 -2.540 -1.488 -0.932 -0.426 0.554 1.001 2500 ## theta[20] -0.965 0.761 -2.454 -1.462 -0.952 -0.455 0.503 1.001 2400 ## theta[21] -0.935 0.792 -2.566 -1.432 -0.900 -0.406 0.557 1.001 3000 ## theta[22] -0.960 0.775 -2.500 -1.489 -0.938 -0.408 0.470 1.003 910 ## theta[23] -1.448 0.826 -3.045 -1.990 -1.439 -0.883 0.179 1.001 3000 ## theta[24] -1.002 0.862 -2.666 -1.606 -1.002 -0.401 0.592 1.001 3000 ## theta[25] -0.991 0.877 -2.728 -1.579 -0.978 -0.425 0.715 1.001 3000 ## theta[26] -1.032 0.868 -2.675 -1.616 -1.042 -0.458 0.653 1.001 3000 ## theta[27] -1.007 0.867 -2.682 -1.598 -1.025 -0.403 0.702 1.001 3000 ## theta[28] -0.539 0.879 -2.253 -1.123 -0.550 0.037 1.152 1.002 1100 ## theta[29] -0.548 0.871 -2.265 -1.142 -0.532 0.059 1.085 1.001 3000 ## theta[30] -0.552 0.888 -2.316 -1.166 -0.537 0.080 1.105 1.001 3000 ## theta[31] -0.554 0.878 -2.313 -1.145 -0.537 0.049 1.122 1.001 3000 ## theta[32] -1.314 0.800 -2.967 -1.833 -1.292 -0.776 0.216 1.002 1400 ## theta[33] -0.927 0.779 -2.521 -1.422 -0.917 -0.404 0.486 1.001 3000 ## theta[34] -0.956 0.783 -2.591 -1.465 -0.941 -0.400 0.480 1.001 2500 ## theta[35] -0.947 0.786 -2.565 -1.467 -0.920 -0.383 0.523 1.002 1600 ## theta[36] -0.944 0.808 -2.539 -1.494 -0.932 -0.373 0.577 1.002 1000 ## theta[37] -0.943 0.769 -2.513 -1.437 -0.925 -0.421 0.557 1.001 3000 ## theta[38] -0.937 0.791 -2.543 -1.467 -0.913 -0.387 0.542 1.001 3000 ## theta[39] -0.956 0.787 -2.564 -1.480 -0.933 -0.413 0.529 1.001 3000 ## theta[40] -0.943 0.772 -2.525 -1.440 -0.948 -0.411 0.537 1.003 940 ## theta[41] -0.587 0.780 -2.148 -1.101 -0.579 -0.042 0.886 1.001 2600 ## theta[42] -0.589 0.794 -2.198 -1.103 -0.548 -0.057 0.874 1.004 630 ## theta[43] -0.582 0.789 -2.200 -1.098 -0.556 -0.034 0.890 1.001 3000 ## theta[44] -0.565 0.799 -2.131 -1.108 -0.535 -0.028 0.955 1.001 3000 ## theta[45] -0.579 0.804 -2.213 -1.113 -0.575 -0.018 0.946 1.001 3000 ## theta[46] -0.572 0.787 -2.183 -1.081 -0.567 -0.052 0.981 1.001 3000 ## theta[47] -0.576 0.794 -2.171 -1.114 -0.557 -0.040 0.955 1.002 2000 ## theta[48] -0.602 0.783 -2.195 -1.106 -0.578 -0.072 0.883 1.001 3000 ## theta[49] -0.577 0.784 -2.173 -1.122 -0.540 -0.036 0.856 1.001 3000 ## theta[50] -0.564 0.797 -2.179 -1.100 -0.540 -0.033 0.915 1.001 2600 ## theta[51] -0.552 0.794 -2.132 -1.079 -0.541 -0.010 0.944 1.001 3000 ## theta[52] -0.592 0.786 -2.231 -1.095 -0.574 -0.055 0.851 1.001 2300 ## theta[53] -0.559 0.776 -2.188 -1.046 -0.541 -0.027 0.920 1.001 3000 ## theta[54] -0.569 0.792 -2.203 -1.081 -0.550 -0.039 0.938 1.002 1100 ## theta[55] -0.582 0.798 -2.231 -1.105 -0.556 -0.053 0.916 1.001 3000 ## theta[56] -0.577 0.803 -2.227 -1.117 -0.565 -0.016 0.926 1.001 2500 ## theta[57] -0.540 0.889 -2.313 -1.138 -0.546 0.051 1.171 1.001 3000 ## theta[58] -0.516 0.896 -2.276 -1.115 -0.519 0.097 1.164 1.001 3000 ## theta[59] -0.535 0.908 -2.324 -1.140 -0.538 0.089 1.246 1.001 3000 ## theta[60] -0.530 0.930 -2.407 -1.149 -0.511 0.108 1.228 1.002 1800 ## theta[61] -0.555 0.900 -2.313 -1.156 -0.552 0.057 1.169 1.001 3000 ## theta[62] -0.026 0.914 -1.861 -0.638 0.015 0.604 1.689 1.001 3000 ## theta[63] -0.037 0.920 -1.886 -0.675 -0.019 0.577 1.703 1.001 3000 ## theta[64] -0.029 0.908 -1.841 -0.636 -0.013 0.601 1.700 1.001 3000 ## theta[65] -0.038 0.917 -1.944 -0.652 0.006 0.602 1.675 1.001 2000 ## theta[66] -0.029 0.900 -1.819 -0.625 -0.010 0.588 1.654 1.001 3000 ## theta[67] -0.026 0.898 -1.897 -0.608 -0.008 0.595 1.690 1.001 2100 ## theta[68] -0.029 0.918 -1.872 -0.641 -0.011 0.596 1.729 1.002 1600 ## theta[69] -0.039 0.929 -1.882 -0.666 -0.018 0.575 1.737 1.001 3000 ## theta[70] -0.007 0.909 -1.867 -0.608 0.037 0.630 1.650 1.001 3000 ## theta[71] -0.046 0.917 -1.853 -0.677 -0.021 0.585 1.712 1.001 3000 ## theta[72] -0.012 0.909 -1.844 -0.627 0.029 0.610 1.668 1.001 3000 ## theta[73] -0.019 0.902 -1.890 -0.632 0.015 0.610 1.643 1.001 3000 ## theta[74] -0.034 0.909 -1.888 -0.654 -0.009 0.590 1.707 1.001 3000 ## theta[75] 0.002 0.899 -1.872 -0.584 0.043 0.603 1.715 1.001 3000 ## theta[76] -0.020 0.915 -1.910 -0.642 0.034 0.607 1.673 1.001 3000 ## theta[77] -1.203 0.768 -2.773 -1.675 -1.162 -0.703 0.233 1.001 3000 ## theta[78] -1.195 0.774 -2.720 -1.719 -1.178 -0.661 0.249 1.002 1300 ## theta[79] -1.234 0.761 -2.720 -1.751 -1.213 -0.703 0.232 1.001 3000 ## theta[80] -1.217 0.780 -2.791 -1.741 -1.192 -0.680 0.246 1.001 3000 ## theta[81] -1.187 0.781 -2.775 -1.714 -1.153 -0.656 0.266 1.002 1100 ## theta[82] -1.200 0.764 -2.717 -1.707 -1.187 -0.666 0.244 1.001 3000 ## theta[83] -1.207 0.773 -2.802 -1.714 -1.202 -0.651 0.231 1.001 3000 ## theta[84] -1.193 0.758 -2.744 -1.689 -1.165 -0.660 0.189 1.001 3000 ## theta[85] -1.180 0.778 -2.750 -1.696 -1.166 -0.630 0.286 1.001 3000 ## theta[86] -1.184 0.759 -2.713 -1.686 -1.159 -0.657 0.320 1.001 2100 ## theta[87] -0.836 0.759 -2.348 -1.335 -0.844 -0.307 0.575 1.001 3000 ## theta[88] -0.833 0.764 -2.353 -1.345 -0.811 -0.318 0.638 1.001 3000 ## theta[89] -0.839 0.765 -2.368 -1.352 -0.820 -0.310 0.651 1.003 780 ## theta[90] -0.828 0.763 -2.395 -1.311 -0.808 -0.307 0.589 1.001 3000 ## theta[91] -0.838 0.756 -2.349 -1.345 -0.808 -0.312 0.597 1.001 3000 ## theta[92] -0.821 0.747 -2.337 -1.313 -0.802 -0.323 0.578 1.001 3000 ## theta[93] -0.853 0.765 -2.442 -1.374 -0.844 -0.320 0.539 1.001 3000 ## theta[94] -0.817 0.742 -2.342 -1.312 -0.793 -0.306 0.561 1.001 3000 ## theta[95] -0.835 0.763 -2.440 -1.336 -0.808 -0.303 0.574 1.001 3000 ## theta[96] -0.859 0.763 -2.383 -1.347 -0.839 -0.326 0.569 1.001 3000 ## [ reached getOption(&quot;max.print&quot;) -- omitted 905 rows ] ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 2773.5 and DIC = 7292.9 ## DIC is an estimate of expected predictive error (lower deviance is better). round(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% &quot;theta&quot;, ], 3) ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.476 0.188 0.181 0.347 0.455 0.573 0.922 1.001 3000 ## a[2] 0.595 0.295 0.283 0.435 0.541 0.672 1.275 1.016 190 ## a[3] 1.253 0.734 0.427 0.706 1.025 1.625 3.146 1.006 370 ## a[4] 0.502 0.207 0.229 0.375 0.469 0.578 0.971 1.001 3000 ## a[5] 0.417 0.152 0.166 0.313 0.403 0.503 0.760 1.002 2200 ## c[1] 0.242 0.094 0.090 0.172 0.232 0.303 0.443 1.001 3000 ## c[2] 0.256 0.099 0.092 0.182 0.248 0.320 0.479 1.005 450 ## c[3] 0.262 0.082 0.099 0.201 0.268 0.322 0.406 1.003 770 ## c[4] 0.251 0.098 0.093 0.179 0.240 0.313 0.466 1.001 3000 ## c[5] 0.241 0.091 0.088 0.175 0.235 0.301 0.436 1.002 1800 ## d[1] 1.419 0.147 1.167 1.324 1.410 1.497 1.741 1.004 1500 ## d[2] 0.285 0.221 -0.214 0.202 0.330 0.420 0.552 1.045 250 ## d[3] -0.544 0.464 -1.698 -0.776 -0.429 -0.195 0.029 1.006 400 ## d[4] 0.515 0.157 0.147 0.438 0.537 0.619 0.746 1.001 2500 ## d[5] 1.024 0.115 0.784 0.954 1.032 1.104 1.234 1.003 780 ## deviance 4519.440 74.520 4358.806 4472.146 4522.941 4571.929 4650.166 1.003 1100 # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) # the below two plots are too big to be useful given the 1000 observations. #R2jags::traceplot(jags.mcmc) # gelman-rubin-brook #gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) bayesplot::mcmc_acf(plot.data,pars = c(paste0(&quot;d[&quot;, 1:5, &quot;]&quot;))) bayesplot::mcmc_trace(plot.data,pars = c(paste0(&quot;d[&quot;, 1:5, &quot;]&quot;))) ggmcmc::ggs_grb(ggs(jags.mcmc), family=&quot;d&quot;) mcmc_areas(plot.data, pars = c(paste0(&quot;d[&quot;,1:5,&quot;]&quot;)), prob = 0.8) bayesplot::mcmc_acf(plot.data,pars = c(paste0(&quot;a[&quot;, 1:5, &quot;]&quot;))) bayesplot::mcmc_trace(plot.data,pars = c(paste0(&quot;a[&quot;, 1:5, &quot;]&quot;))) ggmcmc::ggs_grb(ggs(jags.mcmc), family=&quot;a&quot;) mcmc_areas( plot.data,pars = c(paste0(&quot;a[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) bayesplot::mcmc_acf(plot.data,pars = c(paste0(&quot;c[&quot;, 1:5, &quot;]&quot;))) bayesplot::mcmc_trace(plot.data,pars = c(paste0(&quot;c[&quot;, 1:5, &quot;]&quot;))) ggmcmc::ggs_grb(ggs(jags.mcmc), family=&quot;c&quot;) mcmc_areas(plot.data, pars = c(paste0(&quot;c[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) 11.3.1 Posterior Predicted Distributions Here, we want to compare the observed and expected posterior predicted distributions. Statistical functions of interest are the (1) standardized model-based covariance (SMBC) and (2) the standardized generalized discrepancy measure (SGDDM). For (1), the SMBC is \\[SMBC_{jj^\\prime}=\\frac{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))^2}\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}}\\] In R, the functions below can be used to compute these qualtities. calculate.SGDDM &lt;- function(data.matrix, expected.value.matrix){ J.local = ncol(data.matrix) SMBC.matrix &lt;- calculate.SMBC.matrix(data.matrix, expected.value.matrix) SGDDM = sum(abs((lower.tri(SMBC.matrix, diag=FALSE))*SMBC.matrix))/((J.local*(J.local-1))/2) SGDDM } # closes calculate.SGDDM calculate.SMBC.matrix &lt;- function(data.matrix, expected.value.matrix){ N.local &lt;- nrow(data.matrix) MBC.matrix &lt;- (t(data.matrix-expected.value.matrix) %*% (data.matrix-expected.value.matrix))/N.local MBStddevs.matrix &lt;- diag(sqrt(diag(MBC.matrix))) #SMBC.matrix &lt;- solve(MBStddevs.matrix) %*% MBC.matrix %*% solve(MBStddevs.matrix) J.local &lt;- ncol(data.matrix) SMBC.matrix &lt;- matrix(NA, nrow=J.local, ncol=J.local) for(j in 1:J.local){ for(jj in 1:J.local){ SMBC.matrix[j,jj] &lt;- MBC.matrix[j,jj]/(MBStddevs.matrix[j,j]*MBStddevs.matrix[jj,jj]) } } SMBC.matrix } # closes calculate.MBC.matrix Next, we will use the functions above among other basic data wrangling to construct a full posterior predictive distribution analysis to probe our resulting posterior. # Data wrangle the results/posterior draws for use datv1 &lt;- plot.data %&gt;% pivot_longer( cols = `a[1]`:`a[5]`, values_to = &quot;a&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, a) datv2 &lt;- plot.data %&gt;% pivot_longer( cols = `c[1]`:`c[5]`, values_to = &quot;c&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, c) datv3 &lt;- plot.data %&gt;% pivot_longer( cols = `d[1]`:`d[5]`, values_to = &quot;d&quot;, names_to = &quot;item&quot; ) %&gt;% mutate(item = substr(item, 3,3)) %&gt;% select(chain, iter, item, d) datv4 &lt;- plot.data %&gt;% pivot_longer( cols = `theta[1]`:`theta[999]`, values_to = &quot;theta&quot;, names_to = &quot;person&quot; ) %&gt;% select(chain, iter, person, theta) dat_long &lt;- full_join(datv1, datv2) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;, &quot;item&quot;) dat_long &lt;- full_join(dat_long, datv3) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;, &quot;item&quot;) dat_long &lt;- full_join(dat_long, datv4) ## Joining, by = c(&quot;chain&quot;, &quot;iter&quot;) dat1 &lt;- dat dat1$person &lt;- paste0(&quot;theta[&quot;,1:nrow(dat), &quot;]&quot;) datvl &lt;- dat1 %&gt;% pivot_longer( cols=contains(&quot;item&quot;), names_to = &quot;item&quot;, values_to = &quot;x&quot; ) %&gt;% mutate( item = substr(item, 6, 100) ) dat_long &lt;- left_join(dat_long, datvl) ## Joining, by = c(&quot;item&quot;, &quot;person&quot;) # compute expected prob ilogit &lt;- function(x){exp(x)/(1+exp(x))} dat_long &lt;- dat_long %&gt;% as_tibble()%&gt;% mutate( x.exp = c + (1-c)*ilogit(a*(theta - d)), x.dif = x - x.exp ) dat_long$x.ppd &lt;- apply( dat_long, 1, FUN=function(x){ rbern(1, as.numeric(x[10])) } ) # figure 11.4 d &lt;- dat_long %&gt;% group_by(chain, iter, person) %&gt;% summarise(raw.score = sum(x), raw.score.ppd = sum(x.ppd)) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;. You can override using the `.groups` argument. di &lt;- d %&gt;% filter(chain==1, iter==6001) %&gt;% group_by(raw.score) %&gt;% summarise(count = n()) dii &lt;- d %&gt;% group_by(chain, iter, raw.score.ppd)%&gt;% summarise(raw.score = n()) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;. You can override using the `.groups` argument. # overall fit of observed scores ggplot()+ geom_boxplot(data=dii, aes(y=raw.score, x= raw.score.ppd, group=raw.score.ppd))+ geom_point(data=di, aes(x=raw.score, y=count), color=&quot;red&quot;, size=2)+ labs(x=&quot;Raw Score&quot;, y=&quot;Number of Examinees&quot;)+ scale_x_continuous(breaks=0:5)+ theme_classic() # by item d &lt;- dat_long %&gt;% group_by(chain, iter, person) %&gt;% mutate(raw.score = sum(x), raw.score.ppd = sum(x.ppd)) di &lt;- d %&gt;% filter(chain==1, iter==6001) %&gt;% group_by(raw.score, item) %&gt;% summarise(p.correct = mean(x)) ## `summarise()` has grouped output by &#39;raw.score&#39;. You can override using the `.groups` argument. dii &lt;- d %&gt;% group_by(chain, iter, raw.score.ppd, item)%&gt;% summarise(p.correct = mean(x.ppd)) ## `summarise()` has grouped output by &#39;chain&#39;, &#39;iter&#39;, &#39;raw.score.ppd&#39;. You can override using the `.groups` argument. ggplot()+ geom_boxplot(data=dii, aes(y= p.correct, x= raw.score.ppd, group=raw.score.ppd))+ geom_point(data=di, aes(x=raw.score, y=p.correct), color=&quot;red&quot;, size=2)+ facet_wrap(.~item)+ labs(x=&quot;Raw Score&quot;, y=&quot;Number of Examinees&quot;)+ theme_classic() # computing standardized model summary statistics # objects for results J &lt;- 5 n.chain &lt;- 3 n.iters &lt;- length(unique(long_dat$iter)) ## Error in unique(long_dat$iter): object &#39;long_dat&#39; not found n.iters.PPMC &lt;- n.iters*n.chain ## Error in eval(expr, envir, enclos): object &#39;n.iters&#39; not found realized.SMBC.array &lt;- array(NA, c(n.iters.PPMC, J, J)) ## Error in array(NA, c(n.iters.PPMC, J, J)): object &#39;n.iters.PPMC&#39; not found postpred.SMBC.array &lt;- array(NA, c(n.iters.PPMC, J, J)) ## Error in array(NA, c(n.iters.PPMC, J, J)): object &#39;n.iters.PPMC&#39; not found realized.SGDDM.vector &lt;- array(NA, c(n.iters.PPMC)) ## Error in array(NA, c(n.iters.PPMC)): object &#39;n.iters.PPMC&#39; not found postpred.SGDDM.vector &lt;- array(NA, c(n.iters.PPMC)) ## Error in array(NA, c(n.iters.PPMC)): object &#39;n.iters.PPMC&#39; not found ii &lt;- i &lt;- c &lt;- 1 # iteration condiitons iter.cond &lt;- unique(dat_long$iter) Xobs &lt;- as.matrix(dat[,-6]) for(i in 1:length(iter.cond)){ for(c in 1:3){ cc &lt;- iter.cond[i] Xexp &lt;- dat_long[dat_long$chain==c &amp; dat_long$iter==cc , ] %&gt;% pivot_wider( id_cols = person, names_from = &quot;item&quot;, values_from = &quot;x.exp&quot;, names_prefix = &quot;item&quot; ) %&gt;% ungroup()%&gt;% select(item1:item5)%&gt;% as.matrix() Xppd &lt;- dat_long[dat_long$chain==c &amp; dat_long$iter==cc , ] %&gt;% pivot_wider( id_cols = person, names_from = &quot;item&quot;, values_from = &quot;x.ppd&quot;, names_prefix = &quot;item&quot; ) %&gt;% ungroup()%&gt;% select(item1:item5)%&gt;% as.matrix() # compute realized values realized.SMBC.array[ii, ,] &lt;- calculate.SMBC.matrix(Xobs, Xexp) realized.SGDDM.vector[ii] &lt;- calculate.SGDDM(Xobs, Xexp) # compute PPD values postpred.SMBC.array[ii, ,] &lt;- calculate.SMBC.matrix(Xppd, Xexp) postpred.SGDDM.vector[ii] &lt;- calculate.SGDDM(Xppd, Xexp) ii &lt;- ii + 1 } } ## Error in eval(expr, envir, enclos): object &#39;realized.SMBC.array&#39; not found Next, generate plots to help summarize and describe the posterior predictor distributions of these statistics. plot.dat.ppd &lt;- data.frame( real = realized.SGDDM.vector, ppd = postpred.SGDDM.vector ) ggplot(plot.dat.ppd, aes(x=real, y=ppd))+ geom_point()+ geom_abline(intercept = 0, slope=1)+ lims(x=c(0,0.5), y=c(0, 0.5)) # transform smbc into plotable format ddim &lt;- dim(postpred.SMBC.array) plot.dat.ppd &lt;- as.data.frame(matrix(0, nrow=ddim[1]*ddim[2]*ddim[3], ncol=4)) colnames(plot.dat.ppd) &lt;- c(&quot;itemj&quot;, &quot;itemjj&quot;, &quot;real&quot;, &quot;ppd&quot;) ii &lt;- i &lt;- j &lt;- jj &lt;- 1 for(i in 1:ddim[1]){ for(j in 1:ddim[2]){ for(jj in 1:ddim[3]){ plot.dat.ppd[ii, 1] &lt;- j plot.dat.ppd[ii, 2] &lt;- jj plot.dat.ppd[ii, 3] &lt;- realized.SMBC.array[i, j, jj] plot.dat.ppd[ii, 4] &lt;- postpred.SMBC.array[i, j, jj] ii &lt;- ii + 1 } } } plot.dat.ppd &lt;- plot.dat.ppd %&gt;% filter(itemj &lt; itemjj) %&gt;% mutate( cov = paste0(&quot;cov(&quot;, itemj, &quot;, &quot;, itemjj,&quot;)&quot;) ) ggplot(plot.dat.ppd, aes(x=real, y=ppd))+ geom_point(alpha=0.25)+ geom_density2d(adjust=2)+ geom_abline(intercept = 0, slope=1)+ facet_wrap(.~cov)+ lims(x=c(-1,1), y=c(-1,1))+ theme_classic() "],["lsat-example-stan.html", "11.4 LSAT Example - Stan", " 11.4 LSAT Example - Stan TO-DO "],["irt-models-for-polytomous-data.html", "11.5 IRT Models for Polytomous Data", " 11.5 IRT Models for Polytomous Data A commonly used IRT model for polytomous items is the graded response model (GRM). Below is one way of describing the model. Let \\(x_{ij}\\) be the observed response to item \\(j\\) from examinee \\(i\\) that may take on values 1, 2, , \\(K_j\\), where \\(K_j\\) is the number of possible responses/outcomes for item \\(j\\). In many applications, the number of response options is constant across items, though this need not be the case. The GRM using conditional probability statements about the probability of a response being at or above a specific category and obtaining the probability for each category as a difference of two such conditional probabilities. That is \\[P(x_{ij} = k \\mid \\theta_i, \\boldsymbol{d}_j,a_j) = P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j) - P(x_{ij \\geq k+1 \\mid \\theta_i, d_{j(k+1)},a_j),\\] where \\(\\boldsymbol{d}_j\\) is the collection of location/threshold parameters for item \\(j\\). The GRM takes on a structure similar to the 2-PL for any one category \\[P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j)=F(a_j\\theta_i + d_{jk}).\\] The conditional probability of observed responses may be modeled similarly as we have used for dichotomous responses but with a few important differences. The conditional distribution of the data is \\[p(\\boldsymbol{x}\\mid \\boldsymbol{\\theta},\\boldsymbol{\\omega}) = \\prod_{i=1}^np(\\boldsymbol{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{i=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\] where each \\(x_{ij}\\) is specified as a categorical random variable (or multinomial). A categorical random variable is a generalization of the Bernoulli distribution which is defined be the collection of category response probabilities \\[x_{ij} \\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)).\\] The above helps form the likelihood of the observed data. Next, the prior distribution is described because what the structure should be is not necessarily obvious. First, the prior for the latent ability follows the same logic from the dichotomous model. We employ an exchangeability assumption to specify independent priors for each respondent with a normally distribution prior. Next, the measurement model parameters priors are described. We again can assume exchangeability and arrive at a common but independent prior across items, and assume that the priors for the location and discrimination parameters are independent. These assumptions may not be tenable in theory, but they are practically useful. The priors for discrimination stay the same as the dichotomous model. The priors for the location parameters are a bit more involved. For the location parameters, the first location parameter \\(d_{j1}\\) specifies the probability of responding a 1 or greater which is a certainty if they gave a response. Therefore, the probability would be 1. We set \\(d_{j1} = -\\inf\\) and then set a normal prior for \\(d_{j2}\\sim \\mathrm{Normal}(\\mu_{d2},\\sigma^2_{d2})\\). The priors for the remaining location parameters (\\(d_{3}-d_{k}\\)) can be specified as truncated normal distributions. That is, the location of the next threshold is constrained to be larger than the previous threshold and is formally \\[d_{jk} \\sim \\mathrm{Normal}^{&gt;d_{j(k-1)}}(\\mu_{d_k},\\sigma^2_{d_k},\\ \\mathrm{for}\\ k=3, ...,K_j.\\] The posterior distribution for the GRM can be parameterized as follows. The model as described below is very general and can accommodate varying number of thresholds per item but is constrained to only 1 latent factor. \\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{a}\\mid \\mathbf{x}) \\propto \\prod_{i=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j)p(\\theta_i)p(a_j)\\prod_{k=2}^{K_j}p(d_{jk}),\\] where \\[\\begin{align*} x_{ij}\\mid\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;\\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\mathbf{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= \\left(P(x_{ij}=1\\mid\\theta_i, \\boldsymbol{d}_j, a_j), \\cdots, P(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j)\\right),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ P(x_{ij}=k\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq k\\mid\\theta_i,d_{jk}, a_j) - P(x_{ij}\\geq k+1\\mid\\theta_i, d_{j(k+1)}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k = 1,\\cdots,K_j-1;\\\\ P(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq K_j\\mid\\theta_i,d_{jK_j}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ P(x_{ij}\\geq k\\mid\\theta_i, d_{jk}, a_j) &amp;= F(a_j\\theta_j + d_{jk}),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k=2,\\cdots,K_j;\\\\ P(x_{ij}\\geq 1\\mid\\theta_i, d_{j1}, a_j) &amp;= 1,\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\theta_i \\mid \\mu_{\\theta}, \\sigma^2_{\\theta} &amp;\\sim \\mathrm{Normal}(\\mu_{\\theta}, \\sigma^2_{\\theta}),\\ \\mathrm{for}\\ i = 1, \\cdots, n;\\\\ a_j \\mid \\mu_{a}, \\sigma^2_{a} &amp;\\sim \\mathrm{Normal}^{+}(\\mu_{a}, \\sigma^2_{a}),\\ \\mathrm{for}\\ j=1, \\cdots, J;\\\\ d_{j2}\\mid\\mu_{j2}, \\sigma^2_{j2} &amp;\\sim \\mathrm{Normal}(\\mu_{j2}, \\sigma^2_{j2} ),\\ \\mathrm{for}\\ j=1, \\cdots, J;\\ \\mathrm{and}\\\\ d_{jk}\\mid\\mu_{d_{jk}},\\sigma^2_{d_{jk}} &amp;\\sim \\mathrm{Normal}^{&gt;d_{j(k-1)}}(\\mu_{d_{jk}},\\sigma^2_{d_{jk}}),\\ \\mathrm{for}\\ j=1, \\cdots, J,\\ k=3, ...,K_j. \\end{align*}\\] "],["grm-peer-interactions-example.html", "11.6 GRM Peer Interactions Example", " 11.6 GRM Peer Interactions Example The book uses an example of Peer Interactions from 500 responses to seven items. All the responses are coded from 1 to 5 on an agreement Likert-type scale. A DAG for the GRM corresponding to these data is shown below. Figure 11.4: DAG for the for Peer Interactions GRM analysis The path diagram version is substantially simpler and identical to the path diagram for the 3-PL and factor analysis diagrams. Highlighting the similarity in substantive modeling of polytomous items to dichotomous items. Figure 11.5: Path diagram for the Peer Interactions GRM analysis For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 11.6: Model specification diagram for the Peer Interactions GRM analysis 11.6.1 Example Specific Model Specification In fitting the GRM to the Peer Interactions data, we can be more precise about the prior and likelihood structure. Below is a breakdown of the model specific to this example. Everything is structurally identical to the previous page but specific values are chosen for the hyperparameters. \\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{a}\\mid \\mathbf{x}) \\propto \\prod_{i=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j)p(\\theta_i)p(a_j)\\prod_{k=2}^{K_j}p(d_{jk}),\\] where \\[\\begin{align*} x_{ij}\\mid\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;\\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)),\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\ \\mathbf{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= \\left(P(x_{ij}=1\\mid\\theta_i, \\boldsymbol{d}_j, a_j), \\cdots, P(x_{ij}=5\\mid\\theta_i, \\boldsymbol{d}_j, a_j)\\right),\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\ P(x_{ij}=k\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq k\\mid\\theta_i,d_{jk}, a_j) - P(x_{ij}\\geq k+1\\mid\\theta_i, d_{j(k+1)}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7,\\ k = 1,\\cdots,4;\\\\ P(x_{ij}=5\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &amp;= P(x_{ij}\\geq 5\\mid\\theta_i,d_{j5}, a_j),\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\ P(x_{ij}\\geq k\\mid\\theta_i, d_{jk}, a_j) &amp;= F(a_j\\theta_j + d_{jk}) = \\frac{\\exp\\left(a_j\\theta_i +d_{jk}\\right)}{1+\\exp\\left(a_j\\theta_i +d_{jk}\\right)},\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7,\\ k=2,\\cdots,5;\\\\ P(x_{ij}\\geq 1\\mid\\theta_i, d_{j1}, a_j) &amp;= 1,\\ \\mathrm{for}\\ i=1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\ \\theta_i \\mid \\mu_{\\theta}, \\sigma^2_{\\theta} &amp;\\sim \\mathrm{Normal}(0, 1),\\ \\mathrm{for}\\ i = 1, \\cdots, 500;\\\\ a_j \\mid \\mu_{a}, \\sigma^2_{a} &amp;\\sim \\mathrm{Normal}^{+}(0,2),\\ \\mathrm{for}\\ j=1, \\cdots, 7;\\\\ d_{j2}\\mid\\mu_{d_{j2}}, \\sigma^2_{d_{j2}} &amp;\\sim \\mathrm{Normal}(2,2),\\ \\mathrm{for}\\ j=1, \\cdots, 7;\\\\ d_{j3}\\mid\\mu_{d_{j3}},\\sigma^2_{d_{j3}} &amp;\\sim \\mathrm{Normal}^{&gt;d_{j2}}(1, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 7;\\\\ d_{j4}\\mid\\mu_{d_{j4}},\\sigma^2_{d_{j4}} &amp;\\sim \\mathrm{Normal}^{&gt;d_{j3}}(-1, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 7; \\mathrm{and}\\\\ d_{j5}\\mid\\mu_{d_{j5}},\\sigma^2_{d_{j5}} &amp;\\sim \\mathrm{Normal}^{&gt;d_{j4}}(-2, 2),\\ \\mathrm{for}\\ j=1, \\cdots, 7. \\end{align*}\\] "],["pi-example-jags.html", "11.7 PI Example - JAGS", " 11.7 PI Example - JAGS In the below implementation, I had to change d[j,3] ~ dnorm(1, .5)I(d[j,4],d[j,2]) to d[j,3] ~ dnorm(1, .5);I(,d[j,2]) because (1) R is dumb and doesnt realize that I is a JAGS function; and (2) JAGS does not allow for a directed cycle. The directed cycle in the DAG is when the range of values for d[j,3] is fixed to be within I(d[j,4],d[j,2]) and is not permissible. We need to simply constrain the thresholds to be decreasing or smaller than the previous threshold. Im not sure of the underlying technical reason for this error, but I found that adding the semi-colon fixes the issue when defining the model as an R function. jags.model.peer.int &lt;- function(){ ####################################### # Specify the item response measurement model for the observables ####################################### for (i in 1:n){ for(j in 1:J){ ################################### # Specify the probabilities of a value being greater than or equal to each category ################################### for(k in 2:(K[j])){ # P(greater than or equal to category k &gt; 1) logit(P.gte[i,j,k]) &lt;- a[j]*theta[i]+d[j,k] } # P(greater than or equal to category 1) P.gte[i,j,1] &lt;- 1 ################################### # Specify the probabilities of a value being equal to each category ################################### for(k in 1:(K[j]-1)){ # P(greater equal to category k &lt; K) P[i,j,k] &lt;- P.gte[i,j,k]-P.gte[i,j,k+1] } # P(greater equal to category K) P[i,j,K[j]] &lt;- P.gte[i,j,K[j]] ################################### # Specify the distribution for each observable ################################### x[i,j] ~ dcat(P[i,j,1:K[j]]) } } ####################################### # Specify the (prior) distribution for the latent variables ####################################### for (i in 1:n){ theta[i] ~ dnorm(0, 1) # distribution for the latent variables } ####################################### # Specify the prior distribution for the measurement model parameters ####################################### for(j in 1:J){ d[j,2] ~ dnorm(2, .5) # Locations for k = 2 d[j,3] ~ dnorm(1, .5);I(,d[j,2]) # Locations for k = 3 d[j,4] ~ dnorm(-1, .5);I(,d[j,3]) # Locations for k = 4 d[j,5] ~ dnorm(-2, .5);I(,d[j,4]) # Locations for k = 5 a[j] ~ dnorm(1, .5); I(0,) # Discriminations for observables } } # closes the model # initial values start_values &lt;- list( list( d= matrix(c(NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00, NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00), ncol=5, nrow=7, byrow=T), a=c(1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01)), list( d= matrix(c(NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00, NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00), ncol=5, nrow=7, byrow=T), a=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00)), list( d= matrix(c(NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00, NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00), ncol=5, nrow=7, byrow=T), a=c(1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00)) ) # vector of all parameters to save param_save &lt;- c(&quot;a&quot;, &quot;d&quot;, &quot;theta&quot;) # dataset dat &lt;- read.table(&quot;data/PI.dat&quot;, header=T) mydata &lt;- list( n = nrow(dat), J = ncol(dat), K = rep(5, ncol(dat)), x = as.matrix(dat) ) # fit model fit &lt;- jags( model.file=jags.model.peer.int, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=2000, n.burnin = 1000, n.chains = 3) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 3500 ## Unobserved stochastic nodes: 535 ## Total graph size: 53050 ## ## Initializing model ## ## | | | 0% | |++ | 4% | |++++ | 8% | |++++++ | 12% | |++++++++ | 16% | |++++++++++ | 20% | |++++++++++++ | 24% | |++++++++++++++ | 28% | |++++++++++++++++ | 32% | |++++++++++++++++++ | 36% | |++++++++++++++++++++ | 40% | |++++++++++++++++++++++ | 44% | |++++++++++++++++++++++++ | 48% | |++++++++++++++++++++++++++ | 52% | |++++++++++++++++++++++++++++ | 56% | |++++++++++++++++++++++++++++++ | 60% | |++++++++++++++++++++++++++++++++ | 64% | |++++++++++++++++++++++++++++++++++ | 68% | |++++++++++++++++++++++++++++++++++++ | 72% | |++++++++++++++++++++++++++++++++++++++ | 76% | |++++++++++++++++++++++++++++++++++++++++ | 80% | |++++++++++++++++++++++++++++++++++++++++++ | 84% | |++++++++++++++++++++++++++++++++++++++++++++ | 88% | |++++++++++++++++++++++++++++++++++++++++++++++ | 92% | |++++++++++++++++++++++++++++++++++++++++++++++++ | 96% | |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% ## | | | 0% | |** | 4% | |**** | 8% | |****** | 12% | |******** | 16% | |********** | 20% | |************ | 24% | |************** | 28% | |**************** | 32% | |****************** | 36% | |******************** | 40% | |********************** | 44% | |************************ | 48% | |************************** | 52% | |**************************** | 56% | |****************************** | 60% | |******************************** | 64% | |********************************** | 68% | |************************************ | 72% | |************************************** | 76% | |**************************************** | 80% | |****************************************** | 84% | |******************************************** | 88% | |********************************************** | 92% | |************************************************ | 96% | |**************************************************| 100% print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpW4ITDP/modelcd06e8c1a5d.txt&quot;, fit using jags, ## 3 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 2.145 0.160 1.827 2.033 2.149 2.257 2.445 1.007 340 ## a[2] 3.520 0.265 3.029 3.339 3.515 3.699 4.078 1.018 120 ## a[3] 4.042 0.292 3.483 3.845 4.044 4.230 4.664 1.013 160 ## a[4] 3.595 0.259 3.118 3.416 3.589 3.761 4.130 1.021 100 ## a[5] 2.464 0.174 2.122 2.347 2.468 2.577 2.807 1.022 95 ## a[6] 1.877 0.144 1.612 1.780 1.873 1.972 2.161 1.001 3000 ## a[7] 1.506 0.128 1.261 1.420 1.509 1.590 1.752 1.001 2100 ## d[1,2] 4.954 0.332 4.324 4.735 4.943 5.171 5.639 1.010 870 ## d[2,2] 5.201 0.379 4.512 4.935 5.179 5.445 5.993 1.002 3000 ## d[3,2] 6.638 0.457 5.809 6.318 6.631 6.948 7.551 1.002 1400 ## d[4,2] 6.422 0.445 5.575 6.122 6.413 6.712 7.306 1.002 1000 ## d[5,2] 4.810 0.307 4.219 4.598 4.808 5.003 5.423 1.006 410 ## d[6,2] 3.856 0.253 3.379 3.680 3.853 4.021 4.367 1.026 82 ## d[7,2] 4.306 0.301 3.757 4.096 4.297 4.503 4.941 1.001 3000 ## d[1,3] 2.962 0.212 2.556 2.820 2.957 3.104 3.382 1.007 520 ## d[2,3] 1.207 0.210 0.793 1.065 1.205 1.347 1.627 1.015 180 ## d[3,3] 3.419 0.286 2.865 3.226 3.417 3.615 3.993 1.010 280 ## d[4,3] 2.742 0.250 2.241 2.572 2.742 2.907 3.227 1.003 760 ## d[5,3] 2.707 0.199 2.330 2.570 2.703 2.838 3.107 1.009 250 ## d[6,3] 1.974 0.164 1.666 1.861 1.970 2.080 2.307 1.024 87 ## d[7,3] 2.465 0.174 2.127 2.349 2.463 2.582 2.810 1.003 910 ## d[1,4] 0.662 0.142 0.391 0.566 0.658 0.756 0.941 1.009 240 ## d[2,4] -1.741 0.219 -2.199 -1.879 -1.730 -1.591 -1.341 1.034 71 ## d[3,4] -1.297 0.234 -1.760 -1.455 -1.287 -1.133 -0.865 1.025 89 ## d[4,4] -0.769 0.198 -1.157 -0.904 -0.766 -0.633 -0.396 1.028 78 ## d[5,4] -0.329 0.155 -0.630 -0.436 -0.329 -0.220 -0.039 1.017 130 ## d[6,4] -0.479 0.131 -0.733 -0.568 -0.479 -0.387 -0.230 1.024 89 ## d[7,4] 0.280 0.121 0.039 0.199 0.282 0.363 0.514 1.006 350 ## d[1,5] -2.496 0.189 -2.896 -2.620 -2.487 -2.365 -2.154 1.007 330 ## d[2,5] -5.093 0.371 -5.861 -5.343 -5.084 -4.836 -4.383 1.026 82 ## d[3,5] -5.614 0.408 -6.459 -5.875 -5.596 -5.328 -4.865 1.016 140 ## d[4,5] -4.831 0.342 -5.546 -5.060 -4.806 -4.602 -4.191 1.023 100 ## d[5,5] -3.591 0.243 -4.091 -3.754 -3.590 -3.431 -3.127 1.018 120 ## d[6,5] -3.216 0.211 -3.644 -3.358 -3.214 -3.076 -2.820 1.004 510 ## d[7,5] -2.738 0.187 -3.113 -2.863 -2.729 -2.608 -2.383 1.003 810 ## theta[1] 0.841 0.280 0.274 0.648 0.839 1.038 1.368 1.003 980 ## theta[2] -0.985 0.254 -1.484 -1.157 -0.982 -0.811 -0.496 1.004 650 ## theta[3] -1.040 0.248 -1.520 -1.206 -1.036 -0.872 -0.547 1.003 740 ## theta[4] -0.259 0.259 -0.778 -0.426 -0.259 -0.088 0.251 1.002 1400 ## theta[5] 0.118 0.262 -0.416 -0.046 0.123 0.285 0.618 1.005 430 ## theta[6] 0.092 0.255 -0.409 -0.077 0.103 0.259 0.595 1.003 740 ## theta[7] 1.452 0.318 0.864 1.234 1.448 1.650 2.124 1.006 600 ## theta[8] 0.771 0.265 0.250 0.586 0.777 0.952 1.274 1.001 2200 ## theta[9] -0.017 0.250 -0.505 -0.183 -0.017 0.150 0.485 1.001 2000 ## theta[10] 0.360 0.270 -0.163 0.183 0.358 0.534 0.908 1.005 500 ## theta[11] -0.770 0.256 -1.259 -0.949 -0.772 -0.602 -0.253 1.003 880 ## theta[12] -0.211 0.257 -0.745 -0.378 -0.210 -0.037 0.285 1.003 980 ## theta[13] 1.040 0.260 0.519 0.868 1.039 1.215 1.549 1.007 290 ## theta[14] -0.014 0.253 -0.529 -0.174 -0.006 0.156 0.461 1.001 3000 ## theta[15] -0.686 0.245 -1.164 -0.851 -0.684 -0.523 -0.208 1.002 2000 ## theta[16] -0.453 0.286 -1.016 -0.646 -0.453 -0.259 0.116 1.001 3000 ## theta[17] 2.361 0.479 1.540 2.024 2.302 2.648 3.410 1.004 570 ## theta[18] 0.239 0.258 -0.239 0.061 0.233 0.412 0.775 1.005 430 ## theta[19] 0.035 0.261 -0.457 -0.142 0.031 0.209 0.546 1.002 3000 ## theta[20] 0.236 0.262 -0.277 0.061 0.231 0.412 0.756 1.001 2100 ## theta[21] 0.359 0.249 -0.128 0.197 0.361 0.522 0.835 1.003 890 ## theta[22] -1.023 0.263 -1.556 -1.192 -1.013 -0.848 -0.508 1.001 3000 ## theta[23] 0.681 0.264 0.178 0.504 0.680 0.859 1.192 1.007 330 ## theta[24] 0.066 0.266 -0.465 -0.111 0.071 0.251 0.571 1.002 1800 ## theta[25] 2.355 0.485 1.536 2.007 2.310 2.651 3.431 1.001 2200 ## theta[26] 0.546 0.296 -0.020 0.340 0.538 0.737 1.145 1.004 540 ## theta[27] -0.762 0.258 -1.258 -0.944 -0.761 -0.591 -0.253 1.003 780 ## theta[28] 0.195 0.252 -0.311 0.034 0.197 0.369 0.682 1.003 830 ## theta[29] 1.062 0.290 0.493 0.868 1.059 1.255 1.641 1.001 2000 ## theta[30] 0.372 0.243 -0.091 0.207 0.369 0.530 0.852 1.003 1400 ## theta[31] 0.475 0.255 -0.006 0.295 0.469 0.652 0.975 1.004 610 ## theta[32] 0.415 0.266 -0.102 0.240 0.414 0.591 0.938 1.003 860 ## theta[33] 0.427 0.263 -0.106 0.252 0.429 0.601 0.947 1.001 3000 ## theta[34] 0.133 0.247 -0.358 -0.035 0.136 0.294 0.613 1.006 430 ## theta[35] -1.314 0.260 -1.820 -1.491 -1.317 -1.138 -0.811 1.010 210 ## theta[36] 0.774 0.243 0.292 0.607 0.769 0.938 1.265 1.005 460 ## theta[37] -0.535 0.264 -1.053 -0.713 -0.535 -0.357 -0.029 1.002 2000 ## theta[38] -0.732 0.251 -1.222 -0.904 -0.737 -0.562 -0.231 1.003 680 ## theta[39] -0.466 0.311 -1.076 -0.677 -0.457 -0.258 0.159 1.002 1500 ## theta[40] -0.148 0.257 -0.654 -0.320 -0.148 0.025 0.364 1.003 1300 ## theta[41] 0.215 0.288 -0.335 0.024 0.209 0.398 0.799 1.002 1300 ## theta[42] -2.234 0.353 -2.983 -2.465 -2.219 -1.985 -1.600 1.003 1000 ## theta[43] 2.068 0.404 1.384 1.792 2.019 2.292 3.018 1.002 1700 ## theta[44] -0.409 0.290 -0.974 -0.610 -0.409 -0.214 0.142 1.003 820 ## theta[45] 2.349 0.470 1.567 2.001 2.304 2.638 3.406 1.003 990 ## theta[46] 0.782 0.257 0.282 0.606 0.786 0.958 1.276 1.001 3000 ## theta[47] -0.859 0.265 -1.377 -1.038 -0.859 -0.676 -0.333 1.012 170 ## theta[48] -0.181 0.262 -0.714 -0.350 -0.181 -0.010 0.326 1.004 680 ## theta[49] 0.166 0.264 -0.366 -0.004 0.162 0.334 0.704 1.002 3000 ## theta[50] -0.582 0.285 -1.132 -0.775 -0.583 -0.394 -0.021 1.005 740 ## theta[51] -0.377 0.250 -0.867 -0.545 -0.376 -0.211 0.129 1.003 920 ## theta[52] -1.019 0.250 -1.516 -1.186 -1.019 -0.858 -0.529 1.003 880 ## theta[53] 0.428 0.264 -0.096 0.260 0.428 0.603 0.961 1.002 1500 ## theta[54] 0.365 0.275 -0.193 0.188 0.361 0.549 0.905 1.001 3000 ## theta[55] -0.831 0.267 -1.362 -1.004 -0.831 -0.647 -0.321 1.002 1900 ## theta[56] 0.784 0.253 0.297 0.616 0.780 0.958 1.278 1.001 3000 ## theta[57] 1.396 0.262 0.882 1.219 1.396 1.564 1.944 1.001 3000 ## theta[58] -0.840 0.255 -1.337 -1.011 -0.836 -0.670 -0.342 1.001 3000 ## theta[59] 0.094 0.256 -0.415 -0.079 0.098 0.270 0.584 1.002 3000 ## theta[60] 0.343 0.267 -0.167 0.161 0.346 0.517 0.861 1.006 1100 ## theta[61] -1.794 0.313 -2.423 -1.995 -1.781 -1.582 -1.206 1.002 1200 ## theta[62] -0.422 0.254 -0.936 -0.592 -0.419 -0.262 0.083 1.002 1900 ## theta[63] 2.342 0.476 1.537 2.000 2.304 2.644 3.424 1.002 1300 ## theta[64] -0.920 0.252 -1.421 -1.083 -0.924 -0.757 -0.423 1.002 1100 ## theta[65] 0.369 0.274 -0.159 0.184 0.364 0.547 0.914 1.003 1200 ## theta[66] 0.779 0.264 0.268 0.602 0.783 0.958 1.309 1.001 2800 ## theta[67] 0.339 0.251 -0.157 0.171 0.342 0.508 0.827 1.001 3000 ## theta[68] 1.601 0.293 1.060 1.404 1.582 1.793 2.221 1.002 1100 ## theta[69] 1.093 0.261 0.575 0.919 1.096 1.273 1.590 1.002 1400 ## theta[70] 1.596 0.291 1.040 1.403 1.587 1.781 2.178 1.002 1400 ## theta[71] -0.016 0.246 -0.489 -0.185 -0.015 0.148 0.469 1.004 530 ## theta[72] -1.027 0.257 -1.526 -1.199 -1.023 -0.853 -0.527 1.002 1500 ## theta[73] 0.145 0.268 -0.374 -0.035 0.149 0.320 0.681 1.004 660 ## theta[74] -2.301 0.392 -3.175 -2.544 -2.276 -2.020 -1.612 1.002 1800 ## theta[75] -0.131 0.279 -0.683 -0.313 -0.128 0.049 0.415 1.004 960 ## theta[76] -1.106 0.238 -1.556 -1.269 -1.113 -0.942 -0.639 1.007 310 ## [ reached getOption(&quot;max.print&quot;) -- omitted 425 rows ] ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 699.0 and DIC = 7012.0 ## DIC is an estimate of expected predictive error (lower deviance is better). round(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% &quot;theta&quot;, ], 3) ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 2.145 0.160 1.827 2.033 2.149 2.257 2.445 1.007 340 ## a[2] 3.520 0.265 3.029 3.339 3.515 3.699 4.078 1.018 120 ## a[3] 4.042 0.292 3.483 3.845 4.044 4.230 4.664 1.013 160 ## a[4] 3.595 0.259 3.118 3.416 3.589 3.761 4.130 1.021 100 ## a[5] 2.464 0.174 2.122 2.347 2.468 2.577 2.807 1.022 95 ## a[6] 1.877 0.144 1.612 1.780 1.873 1.972 2.161 1.001 3000 ## a[7] 1.506 0.128 1.261 1.420 1.509 1.590 1.752 1.001 2100 ## d[1,2] 4.954 0.332 4.324 4.735 4.943 5.171 5.639 1.010 870 ## d[2,2] 5.201 0.379 4.512 4.935 5.179 5.445 5.993 1.002 3000 ## d[3,2] 6.638 0.457 5.809 6.318 6.631 6.948 7.551 1.002 1400 ## d[4,2] 6.422 0.445 5.575 6.122 6.413 6.712 7.306 1.002 1000 ## d[5,2] 4.810 0.307 4.219 4.598 4.808 5.003 5.423 1.006 410 ## d[6,2] 3.856 0.253 3.379 3.680 3.853 4.021 4.367 1.026 82 ## d[7,2] 4.306 0.301 3.757 4.096 4.297 4.503 4.941 1.001 3000 ## d[1,3] 2.962 0.212 2.556 2.820 2.957 3.104 3.382 1.007 520 ## d[2,3] 1.207 0.210 0.793 1.065 1.205 1.347 1.627 1.015 180 ## d[3,3] 3.419 0.286 2.865 3.226 3.417 3.615 3.993 1.010 280 ## d[4,3] 2.742 0.250 2.241 2.572 2.742 2.907 3.227 1.003 760 ## d[5,3] 2.707 0.199 2.330 2.570 2.703 2.838 3.107 1.009 250 ## d[6,3] 1.974 0.164 1.666 1.861 1.970 2.080 2.307 1.024 87 ## d[7,3] 2.465 0.174 2.127 2.349 2.463 2.582 2.810 1.003 910 ## d[1,4] 0.662 0.142 0.391 0.566 0.658 0.756 0.941 1.009 240 ## d[2,4] -1.741 0.219 -2.199 -1.879 -1.730 -1.591 -1.341 1.034 71 ## d[3,4] -1.297 0.234 -1.760 -1.455 -1.287 -1.133 -0.865 1.025 89 ## d[4,4] -0.769 0.198 -1.157 -0.904 -0.766 -0.633 -0.396 1.028 78 ## d[5,4] -0.329 0.155 -0.630 -0.436 -0.329 -0.220 -0.039 1.017 130 ## d[6,4] -0.479 0.131 -0.733 -0.568 -0.479 -0.387 -0.230 1.024 89 ## d[7,4] 0.280 0.121 0.039 0.199 0.282 0.363 0.514 1.006 350 ## d[1,5] -2.496 0.189 -2.896 -2.620 -2.487 -2.365 -2.154 1.007 330 ## d[2,5] -5.093 0.371 -5.861 -5.343 -5.084 -4.836 -4.383 1.026 82 ## d[3,5] -5.614 0.408 -6.459 -5.875 -5.596 -5.328 -4.865 1.016 140 ## d[4,5] -4.831 0.342 -5.546 -5.060 -4.806 -4.602 -4.191 1.023 100 ## d[5,5] -3.591 0.243 -4.091 -3.754 -3.590 -3.431 -3.127 1.018 120 ## d[6,5] -3.216 0.211 -3.644 -3.358 -3.214 -3.076 -2.820 1.004 510 ## d[7,5] -2.738 0.187 -3.113 -2.863 -2.729 -2.608 -2.383 1.003 810 ## deviance 6312.984 37.557 6240.558 6286.102 6312.879 6338.275 6387.906 1.010 210 # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;,&quot;with medians and 80% intervals&quot;) bayesplot::mcmc_areas(plot.data, regex_pars = &quot;d&quot;, prob = 0.8) + plot_title + lims(x=c(-10, 10)) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing ## scale. ## Warning: Removed 1 rows containing missing values (geom_segment). bayesplot::mcmc_areas( plot.data, pars = c(paste0(&quot;a[&quot;, 1:7, &quot;]&quot;)), prob = 0.8) + plot_title bayesplot::mcmc_acf(plot.data,pars = c(paste0(&quot;a[&quot;, 1:7, &quot;]&quot;))) bayesplot::mcmc_trace(plot.data,pars = c(paste0(&quot;a[&quot;, 1:7, &quot;]&quot;))) ggmcmc::ggs_grb(ggs(jags.mcmc), family=&quot;d&quot;) ggmcmc::ggs_grb(ggs(jags.mcmc), family=&quot;a&quot;) ggmcmc::ggs_autocorrelation(ggs(jags.mcmc), family=&quot;d&quot;) "],["pi-example-stan.html", "11.8 PI Example - Stan", " 11.8 PI Example - Stan TO-DO "],["latent-response-formulation.html", "11.9 Latent Response Formulation", " 11.9 Latent Response Formulation Connecting IRT models to a factor analytic perspective can be helpful from a modeling standpoint. Especially when ones model is multidimensional leading into structural equation models. A useful connection can be made by introducing extra variable(s) into the model to represent the latent response variable underlying the observed categorical response variable. We can think of this latent response variables as a latent continuous variable hypothesized to underlie the observed categorical variable that discretized due to data collection or difficulty in measurement; or when this natural interpretation is not appropriate, we can think of the latent response variable as a propensity measure for the given response. Although this is not a perfect interpretation, the use of a latent response formulation eases some of the computational machinery and allows for a nice connection between IRT and CFA models. Next, the latent response formulation is shown for a set of dichotomous outcomes. This model is conceptually a 2-PL/2-PNO (2 parameter normal ogive) model and is essentially a probit model. The model can be defined as \\[x^{\\ast}_{ij} = a_j\\theta_i+d_j+\\varepsilon_{ij},\\] where, for each \\(j\\), \\(\\varepsilon_{ij} \\sim \\mathrm{Normal}(0, \\sigma^2_{\\varepsilon_j})\\). And can jointly be expressed as \\[x^{\\ast}_{ij} \\sim \\mathrm{Normal}(a_j\\theta_i+d_j, \\sigma^2_{\\varepsilon_j}).\\] The probability of an observed response is then modeled as probability of the latent response variable for person \\(i\\) on item \\(j\\) being greater than or equal to some threshold \\(\\gamma_j\\). 11.9.1 LSAT Example Revisted jags.model.lsat &lt;- function(){ for (i in 1:n){ for(j in 1:J){ # latent response variable xstar[i,j] ~ dnorm(a[j]*theta[i]+d[j], 1) P[i,j] &lt;- c[j]+(1-c[j])*phi(xstar[i,j]) # 3P-NO expression x[i,j] ~ dbern(P[i,j]) # distribution for each observable } } for (i in 1:n){ theta[i] ~ dnorm(0, 1) # distribution for the latent variables } for(j in 1:J){ d[j] ~ dnorm(0, .5) # Locations for observables a[j] ~ dnorm(1, .5); I(0,) # Discriminations for observables c[j] ~ dbeta(5,17) # Lower asymptotes for observables } } # closes the model # initial values start_values &lt;- list( list(&quot;d&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;a&quot;=c(1.00, 1.00, 1.00, 1.00, 1.00), &quot;c&quot;=c(0.20, 0.20, 0.20, 0.20, 0.20)), list(&quot;d&quot;=c(-3.00, -3.00, -3.00, -3.00, -3.00), &quot;a&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;c&quot;=c(0.50, 0.50, 0.50, 0.50, 0.50)), list(&quot;d&quot;=c(3.00, 3.00, 3.00, 3.00, 3.00), &quot;a&quot;=c(0.1, 0.1, 0.1, 0.1, 0.1), &quot;c&quot;=c(0.05, 0.05, 0.05, 0.05, 0.05)) ) # vector of all parameters to save param_save &lt;- c(&quot;a&quot;, &quot;d&quot;, &quot;c&quot;, &quot;theta&quot;) # dataset dat &lt;- read.table(&quot;data/LSAT.dat&quot;, header=T) mydata &lt;- list( n = nrow(dat), J = ncol(dat), x = as.matrix(dat) ) # fit model fit &lt;- jags( model.file=jags.model.lsat, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=26000, n.burnin = 6000, n.chains = 3, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 5000 ## Unobserved stochastic nodes: 6015 ## Total graph size: 36027 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpW4ITDP/modelcd01affc14.txt&quot;, fit using jags, ## 3 chains, each with 26000 iterations (first 6000 discarded), n.thin = 20 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.629 0.244 0.238 0.460 0.599 0.765 1.187 1.010 270 ## a[2] 0.809 0.264 0.400 0.619 0.768 0.970 1.395 1.005 480 ## a[3] 1.440 0.581 0.605 0.993 1.321 1.808 2.773 1.007 420 ## a[4] 0.714 0.223 0.356 0.559 0.683 0.842 1.240 1.010 440 ## a[5] 0.622 0.261 0.245 0.460 0.591 0.741 1.197 1.027 90 ## c[1] 0.249 0.096 0.091 0.178 0.241 0.311 0.454 1.002 1300 ## c[2] 0.260 0.097 0.095 0.186 0.252 0.324 0.459 1.003 1100 ## c[3] 0.249 0.072 0.105 0.197 0.253 0.304 0.376 1.011 220 ## c[4] 0.251 0.097 0.088 0.182 0.244 0.310 0.466 1.005 1000 ## c[5] 0.247 0.097 0.088 0.178 0.237 0.305 0.479 1.002 2900 ## d[1] 1.969 0.200 1.595 1.844 1.959 2.079 2.420 1.004 3000 ## d[2] 0.402 0.254 -0.198 0.270 0.455 0.582 0.781 1.004 690 ## d[3] -0.559 0.382 -1.419 -0.806 -0.505 -0.266 0.027 1.014 190 ## d[4] 0.730 0.219 0.205 0.620 0.762 0.875 1.064 1.021 520 ## d[5] 1.447 0.174 1.067 1.348 1.458 1.559 1.773 1.006 420 ## theta[1] -1.624 0.767 -3.169 -2.132 -1.592 -1.088 -0.206 1.001 2300 ## theta[2] -1.618 0.784 -3.230 -2.121 -1.592 -1.053 -0.172 1.001 3000 ## theta[3] -1.631 0.786 -3.286 -2.143 -1.594 -1.102 -0.117 1.001 3000 ## theta[4] -1.285 0.778 -2.876 -1.798 -1.254 -0.737 0.187 1.001 2200 ## theta[5] -1.281 0.778 -2.821 -1.806 -1.248 -0.734 0.212 1.001 2800 ## theta[6] -1.255 0.783 -2.755 -1.781 -1.231 -0.709 0.224 1.001 3000 ## theta[7] -1.273 0.766 -2.811 -1.783 -1.243 -0.751 0.140 1.001 3000 ## theta[8] -1.269 0.781 -2.833 -1.782 -1.264 -0.738 0.211 1.003 950 ## theta[9] -1.292 0.766 -2.844 -1.799 -1.256 -0.779 0.134 1.001 3000 ## theta[10] -1.328 0.790 -2.933 -1.848 -1.315 -0.777 0.135 1.001 3000 ## theta[11] -1.292 0.786 -2.877 -1.792 -1.272 -0.764 0.226 1.002 1000 ## theta[12] -0.914 0.806 -2.577 -1.451 -0.888 -0.360 0.556 1.001 2100 ## theta[13] -0.909 0.788 -2.491 -1.438 -0.869 -0.357 0.538 1.005 480 ## theta[14] -0.921 0.784 -2.515 -1.441 -0.915 -0.393 0.579 1.001 3000 ## theta[15] -0.926 0.772 -2.497 -1.439 -0.909 -0.387 0.508 1.001 3000 ## theta[16] -0.902 0.794 -2.547 -1.425 -0.891 -0.357 0.629 1.001 3000 ## theta[17] -0.928 0.757 -2.484 -1.414 -0.908 -0.431 0.473 1.001 2900 ## theta[18] -0.917 0.788 -2.455 -1.463 -0.913 -0.371 0.567 1.001 3000 ## theta[19] -0.884 0.767 -2.425 -1.383 -0.871 -0.375 0.565 1.002 2000 ## theta[20] -0.928 0.779 -2.517 -1.458 -0.912 -0.392 0.571 1.001 2800 ## theta[21] -0.926 0.783 -2.454 -1.426 -0.913 -0.385 0.534 1.003 800 ## theta[22] -0.919 0.804 -2.560 -1.454 -0.875 -0.358 0.587 1.002 1300 ## theta[23] -1.383 0.837 -3.056 -1.953 -1.373 -0.798 0.194 1.004 620 ## theta[24] -0.954 0.860 -2.693 -1.526 -0.930 -0.348 0.677 1.001 2300 ## theta[25] -0.987 0.879 -2.721 -1.574 -0.999 -0.376 0.694 1.002 1800 ## theta[26] -1.021 0.864 -2.707 -1.608 -1.026 -0.430 0.688 1.007 320 ## theta[27] -1.002 0.871 -2.800 -1.580 -0.983 -0.408 0.631 1.003 740 ## theta[28] -0.510 0.875 -2.206 -1.085 -0.501 0.074 1.200 1.002 1900 ## theta[29] -0.486 0.878 -2.239 -1.071 -0.455 0.124 1.215 1.001 2700 ## theta[30] -0.485 0.875 -2.244 -1.072 -0.461 0.111 1.142 1.002 1500 ## theta[31] -0.534 0.887 -2.314 -1.111 -0.528 0.058 1.196 1.003 740 ## theta[32] -1.318 0.769 -2.855 -1.830 -1.305 -0.782 0.148 1.001 2700 ## theta[33] -0.940 0.780 -2.518 -1.426 -0.916 -0.409 0.521 1.002 1100 ## theta[34] -0.968 0.786 -2.557 -1.494 -0.945 -0.429 0.509 1.001 3000 ## theta[35] -0.939 0.788 -2.552 -1.456 -0.899 -0.391 0.534 1.005 500 ## theta[36] -0.938 0.797 -2.520 -1.477 -0.921 -0.384 0.587 1.001 2300 ## theta[37] -0.920 0.802 -2.553 -1.471 -0.884 -0.360 0.597 1.002 1700 ## theta[38] -0.932 0.791 -2.487 -1.448 -0.890 -0.398 0.551 1.001 3000 ## theta[39] -0.920 0.779 -2.487 -1.445 -0.932 -0.373 0.596 1.002 1700 ## theta[40] -0.931 0.784 -2.557 -1.446 -0.917 -0.378 0.523 1.001 3000 ## theta[41] -0.545 0.783 -2.057 -1.076 -0.519 -0.027 0.967 1.003 790 ## theta[42] -0.534 0.792 -2.097 -1.056 -0.510 -0.008 1.044 1.001 2700 ## theta[43] -0.573 0.779 -2.180 -1.052 -0.560 -0.042 0.923 1.001 3000 ## theta[44] -0.543 0.809 -2.180 -1.075 -0.529 0.008 1.034 1.001 3000 ## theta[45] -0.548 0.803 -2.131 -1.088 -0.524 -0.022 1.022 1.001 3000 ## theta[46] -0.559 0.788 -2.122 -1.094 -0.536 -0.033 0.992 1.001 3000 ## theta[47] -0.517 0.808 -2.203 -1.017 -0.496 0.019 1.013 1.005 480 ## theta[48] -0.547 0.793 -2.081 -1.069 -0.524 -0.027 0.973 1.006 380 ## theta[49] -0.541 0.797 -2.139 -1.078 -0.521 0.007 0.919 1.001 2400 ## theta[50] -0.555 0.787 -2.101 -1.064 -0.550 -0.010 0.937 1.003 710 ## theta[51] -0.542 0.790 -2.141 -1.057 -0.534 -0.005 0.955 1.006 350 ## theta[52] -0.531 0.795 -2.189 -1.022 -0.511 -0.023 0.959 1.002 2700 ## theta[53] -0.554 0.797 -2.206 -1.061 -0.529 -0.027 0.928 1.001 3000 ## theta[54] -0.549 0.797 -2.140 -1.076 -0.530 -0.009 0.976 1.004 610 ## theta[55] -0.562 0.792 -2.124 -1.081 -0.544 -0.029 0.993 1.000 3000 ## theta[56] -0.532 0.787 -2.099 -1.051 -0.525 -0.004 1.011 1.003 900 ## theta[57] -0.461 0.892 -2.229 -1.046 -0.435 0.148 1.238 1.001 2400 ## theta[58] -0.512 0.912 -2.341 -1.144 -0.495 0.124 1.234 1.001 2700 ## theta[59] -0.529 0.897 -2.326 -1.121 -0.517 0.103 1.155 1.001 3000 ## theta[60] -0.523 0.886 -2.319 -1.091 -0.510 0.084 1.140 1.001 3000 ## theta[61] -0.529 0.913 -2.412 -1.138 -0.525 0.101 1.195 1.002 2800 ## theta[62] 0.017 0.905 -1.724 -0.621 0.036 0.652 1.784 1.001 3000 ## theta[63] 0.024 0.903 -1.764 -0.591 0.038 0.651 1.707 1.005 430 ## theta[64] 0.002 0.885 -1.840 -0.583 0.029 0.615 1.669 1.001 3000 ## theta[65] 0.031 0.889 -1.777 -0.544 0.039 0.622 1.747 1.001 2900 ## theta[66] 0.045 0.902 -1.747 -0.567 0.050 0.679 1.720 1.001 3000 ## theta[67] 0.048 0.899 -1.846 -0.539 0.092 0.649 1.762 1.001 3000 ## theta[68] 0.029 0.902 -1.812 -0.561 0.034 0.653 1.754 1.002 1400 ## theta[69] -0.005 0.917 -1.879 -0.614 0.034 0.629 1.706 1.003 790 ## theta[70] -0.012 0.909 -1.851 -0.594 0.006 0.605 1.700 1.001 3000 ## theta[71] 0.045 0.896 -1.771 -0.553 0.068 0.658 1.739 1.001 3000 ## theta[72] 0.030 0.891 -1.783 -0.566 0.060 0.644 1.671 1.004 610 ## theta[73] 0.013 0.895 -1.821 -0.569 0.030 0.629 1.734 1.001 3000 ## theta[74] 0.018 0.875 -1.752 -0.545 0.024 0.592 1.714 1.002 1600 ## theta[75] 0.023 0.879 -1.790 -0.538 0.015 0.627 1.688 1.001 3000 ## theta[76] 0.029 0.895 -1.784 -0.551 0.061 0.621 1.701 1.001 3000 ## theta[77] -1.201 0.786 -2.807 -1.709 -1.185 -0.671 0.270 1.001 2800 ## theta[78] -1.227 0.770 -2.787 -1.724 -1.217 -0.702 0.223 1.001 2700 ## theta[79] -1.238 0.759 -2.783 -1.752 -1.223 -0.708 0.169 1.001 3000 ## theta[80] -1.201 0.760 -2.742 -1.706 -1.184 -0.697 0.206 1.001 3000 ## theta[81] -1.229 0.749 -2.749 -1.729 -1.216 -0.711 0.173 1.002 2100 ## theta[82] -1.208 0.776 -2.793 -1.718 -1.189 -0.685 0.318 1.001 3000 ## theta[83] -1.219 0.765 -2.781 -1.717 -1.198 -0.694 0.271 1.001 3000 ## theta[84] -1.216 0.776 -2.808 -1.730 -1.194 -0.659 0.193 1.001 3000 ## theta[85] -1.239 0.753 -2.780 -1.738 -1.233 -0.727 0.195 1.004 630 ## theta[86] -1.237 0.761 -2.794 -1.759 -1.195 -0.719 0.217 1.001 2500 ## theta[87] -0.834 0.749 -2.354 -1.342 -0.810 -0.318 0.582 1.001 2600 ## theta[88] -0.876 0.760 -2.401 -1.385 -0.851 -0.347 0.575 1.001 2400 ## theta[89] -0.841 0.760 -2.418 -1.320 -0.833 -0.330 0.612 1.004 600 ## theta[90] -0.854 0.760 -2.348 -1.368 -0.812 -0.347 0.575 1.005 420 ## theta[91] -0.830 0.769 -2.352 -1.339 -0.834 -0.309 0.636 1.001 3000 ## theta[92] -0.849 0.757 -2.392 -1.348 -0.843 -0.324 0.591 1.001 3000 ## theta[93] -0.874 0.777 -2.461 -1.372 -0.867 -0.342 0.618 1.001 3000 ## theta[94] -0.845 0.760 -2.476 -1.347 -0.805 -0.336 0.561 1.002 1400 ## theta[95] -0.861 0.757 -2.380 -1.377 -0.845 -0.328 0.537 1.001 2400 ## theta[96] -0.873 0.749 -2.369 -1.377 -0.866 -0.342 0.516 1.002 1000 ## [ reached getOption(&quot;max.print&quot;) -- omitted 905 rows ] ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 5378.3 and DIC = 9070.2 ## DIC is an estimate of expected predictive error (lower deviance is better). round(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% &quot;theta&quot;, ], 3) ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] 0.629 0.244 0.238 0.460 0.599 0.765 1.187 1.010 270 ## a[2] 0.809 0.264 0.400 0.619 0.768 0.970 1.395 1.005 480 ## a[3] 1.440 0.581 0.605 0.993 1.321 1.808 2.773 1.007 420 ## a[4] 0.714 0.223 0.356 0.559 0.683 0.842 1.240 1.010 440 ## a[5] 0.622 0.261 0.245 0.460 0.591 0.741 1.197 1.027 90 ## c[1] 0.249 0.096 0.091 0.178 0.241 0.311 0.454 1.002 1300 ## c[2] 0.260 0.097 0.095 0.186 0.252 0.324 0.459 1.003 1100 ## c[3] 0.249 0.072 0.105 0.197 0.253 0.304 0.376 1.011 220 ## c[4] 0.251 0.097 0.088 0.182 0.244 0.310 0.466 1.005 1000 ## c[5] 0.247 0.097 0.088 0.178 0.237 0.305 0.479 1.002 2900 ## d[1] 1.969 0.200 1.595 1.844 1.959 2.079 2.420 1.004 3000 ## d[2] 0.402 0.254 -0.198 0.270 0.455 0.582 0.781 1.004 690 ## d[3] -0.559 0.382 -1.419 -0.806 -0.505 -0.266 0.027 1.014 190 ## d[4] 0.730 0.219 0.205 0.620 0.762 0.875 1.064 1.021 520 ## d[5] 1.447 0.174 1.067 1.348 1.458 1.559 1.773 1.006 420 ## deviance 3691.893 103.981 3489.316 3621.852 3691.282 3760.804 3901.424 1.008 340 # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) # the below two plots are too big to be useful given the 1000 observations. #R2jags::traceplot(jags.mcmc) # gelman-rubin-brook #gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;d[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;a[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;c[&quot;, 1:5, &quot;]&quot;)), prob = 0.8) + plot_title "],["final-notes.html", "11.10 Final Notes", " 11.10 Final Notes A fully Bayesian approach to psychometric modeling helps highlight the major similarities between factor analytic frameworks and the item response theory perspective. "],["missing-data-modeling.html", "Chapter 12 Missing Data Modeling", " Chapter 12 Missing Data Modeling TO-DO "],["latent-class-analysis.html", "Chapter 13 Latent Class Analysis", " Chapter 13 Latent Class Analysis TO-DO "],["bayesian-networks.html", "Chapter 14 Bayesian Networks", " Chapter 14 Bayesian Networks TO-DO "],["references.html", "References", " References Kelley, T. L. 1923. Statistical Method. New York, NY: Macmillan. "]]
>>>>>>> Stashed changes
