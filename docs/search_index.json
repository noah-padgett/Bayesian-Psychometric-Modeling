[
["confirmatory-factor-analysis.html", "Chapter 9 Confirmatory Factor Analysis", " Chapter 9 Confirmatory Factor Analysis The full Bayesian specification of a general CFA model for all associated unknowns is as follows. This includes probability statements, notation, parameters, likelihood, priors, and hyperparameters. The observed data is defined as the \\(n\\times J\\) matrix \\(\\mathbf{X}\\) for the \\(J\\) observed measures. The CFA model parameters are defined as \\[\\begin{align*} \\mathbf{x}_i &amp;= \\tau + \\Lambda\\xi_i + \\varepsilon_i\\\\ \\Sigma (\\mathbf{x}) &amp;= \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi \\end{align*}\\] \\(\\Xi\\) is the \\(n\\times M\\) matrix of latent variable scores on the \\(M\\) latent variables for the \\(n\\) respondents/subjects. For an single subject, \\(\\xi_i\\) represents the vector of scores on the latent variable(s). Values (location, scale, orientation, etc.) or \\(\\xi_i\\) are conditional on (1) \\(\\kappa\\), the \\(M\\times 1\\) vector of latent variable means, and (2) \\(\\Phi\\), the \\(M\\times M\\) covariance matrix of variable variables; \\(\\tau\\) is the \\(J\\times 1\\) vector of observed variable intercepts which is the expected value for the observed measures when the latent variable(s) are all \\(0\\); \\(\\Lambda\\) is the \\(J\\times M\\) matrix of factor loadings where the \\(j\\)th row and \\(m\\)th column represents the factor loading of the \\(j\\)th observed variable on the \\(m\\)th latent variable; \\(\\delta_i\\) is the \\(J\\times 1\\) vector of errors, where \\(E(\\delta_i)=\\mathbf{0}\\) with \\(\\mathrm{var}(\\delta_i)=\\Psi\\) which is the \\(J\\times J\\) error covariance matrix. \\[\\begin{align*} p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi\\mid \\mathbf{X}) &amp;\\propto p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)\\\\ &amp;= p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi) p(\\Xi\\mid\\kappa, \\Phi) p(\\kappa) p(\\Phi) p(\\tau) p(\\Lambda) p(\\Psi)\\\\ &amp;= \\prod_{i=1}^{n}\\prod_{j=1}^J\\prod_{m=1}^M p(x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj}) p(\\xi_i\\mid\\kappa, \\Phi) p(\\kappa_m) p(\\Phi) p(\\tau_j) p(\\lambda_j) p(\\psi_{jj}) \\end{align*}\\] where \\[\\begin{align*} x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj} &amp;\\sim \\mathrm{Normal}(\\tau_j+\\xi_i\\lambda^{\\prime}_j, \\psi_{jj}),\\ \\mathrm{for}\\ i=1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\ \\xi_i\\mid\\kappa, \\Phi &amp;\\sim \\mathrm{Normal}(\\kappa, \\Phi),\\ \\mathrm{for}\\ i=1, \\cdots, n;\\\\ \\kappa_m &amp;\\sim \\mathrm{Normal}(\\mu_{\\kappa},\\sigma^2_{\\kappa}),\\ \\mathrm{for}\\ m = 1, \\cdots, M;\\\\ \\Phi &amp;\\sim \\mathrm{Inverse-Wishart}(\\Phi_0, d);\\\\ \\tau_j &amp;\\sim \\mathrm{Normal}(\\mu_{\\tau},\\sigma^2_{\\tau}),\\ \\mathrm{for}\\ j = 1, \\cdots, J;\\\\ \\lambda_{j,m} &amp;\\sim \\mathrm{Normal}(\\mu_{\\lambda}, \\sigma^2_{\\lambda}),\\ \\mathrm{for}\\ j = 1, \\cdots, J,\\ m = 1, \\cdots, M;\\\\ \\psi_{jj} &amp;\\sim \\mathrm{Inverse-Gamma}(\\nu_{\\psi}/2, \\nu_{\\psi}\\psi_0/2),\\ \\mathrm{for}\\ j=1, \\cdots, J. \\end{align*}\\] With the hyperparameters that are supplied by the analyst being defined as \\(\\mu_{\\kappa}\\) is the prior mean for the latent variable, \\(\\sigma^2_{\\kappa}\\) is the prior variance for the latent variable, \\(\\Phi_0\\) is the prior expectation for the covariance matrix among latent variables, \\(d\\) represents a dispersion parameter reflecting the magnitude of our beliefs about \\(\\Phi_0\\), \\(\\mu_{\\tau}\\) is the prior mean for the intercepts which reflects our knowledge about the location of the observed variables, \\(\\sigma^2_{\\tau}\\) is a measure of how much weight we want to give to the prior mean, \\(\\mu_{\\lambda}\\) is the prior mean for the factor loadings which can vary over items and latent variables, \\(\\sigma^2_{\\lambda}\\) is the measure of dispersion for the the factor loadings, where lower variances indicate a stronger belief about the values for the loadings, \\(\\nu_{\\psi}\\) is the measure of location for the gamma prior indicating our expectation for the magnitude of the error variance, \\(\\psi_0\\) is our uncertainty with respect to the location we selected for the variance, and Alternatively, we could place a prior on \\(\\Psi\\) instead of the individual residual variances. This would mean we would be placing a prior on the error-covariance matrix similar to how we specified a prior for latent variance covariance matrix. "],
["single-latent-variable-model.html", "9.1 Single Latent Variable Model", " 9.1 Single Latent Variable Model Here we consider the model in section 9.3 which is a CFA model with 1 latent variable and 5 observed indicators. The graphical representation of these factor models get pretty complex pretty quickly, but for this example I have reproduced a version of Figure 9.3b, shown below. Figure 9.1: DAG for CFA model with 1 latent variable However, as the authors noted, the path diagram tradition of conveying models is also very useful in discussing and describing the model, which I give next. Figure 9.2: DAG for CFA model with 1 latent variable For completeness, I have included the model specification diagram that more concretely connects the DAG and path diagram to the assumed distributions and priors. Figure 9.3: Model specification diagram for the CFA model with 1 latent factor "],
["stan-single-latent-variable.html", "9.2 Stan - Single Latent Variable", " 9.2 Stan - Single Latent Variable model_cfa1 &lt;- &#39; data { int N; int J; matrix[N, J] X; } parameters { real ksi[N]; //latent variable values real tau[J]; //intercepts real load[J-1]; //factor loadings real&lt;lower=0&gt; psi[J]; //residual variance //real kappa; // factor means real&lt;lower=0&gt; phi; // factor variances } transformed parameters { real lambda[J]; lambda[1] = 1; lambda[2:J] = load; } model { real kappa; kappa = 0; // likelihood for data for(i in 1:N){ for(j in 1:J){ X[i, j] ~ normal(tau[j] + ksi[i]*lambda[j], psi[j]); } } // prior for latent variable parameters ksi ~ normal(kappa, phi); phi ~ inv_gamma(5, 10); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); for(j in 1:(J-1)){ load[j] ~ normal(1, 10); } } &#39; # data must be in a list dat &lt;- read.table(&quot;code/CFA-One-Latent-Variable/Data/IIS.dat&quot;, header=T) mydata &lt;- list( N = 500, J = 5, X = as.matrix(dat) ) # initial values start_values &lt;- list( list(tau = c(.1,.1,.1,.1,.1), lambda=c(0, 0, 0, 0, 0), phi = 1, psi=c(1, 1, 1, 1, 1)), list(tau = c(3,3,3,3,3), lambda=c(3, 3, 3, 3, 3), phi = 2, psi=c(.5, .5, .5, .5, .5)), list(tau = c(5, 5, 5, 5, 5), lambda=c(6, 6, 6, 6, 6), phi = 2, psi=c(2, 2, 2, 2, 2)) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa1, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 1, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;- ## E&#39; not found # first get a basic breakdown of the posteriors print(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## Inference for Stan model: 2d84cef0cbb0e7ac36bc8c7a29bca5b9. ## 3 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## lambda[2] 0.81 0 0.05 0.72 0.78 0.81 0.85 0.92 1331 1 ## lambda[3] 0.47 0 0.04 0.40 0.45 0.47 0.50 0.56 2208 1 ## lambda[4] 1.11 0 0.07 0.97 1.06 1.10 1.15 1.26 1659 1 ## lambda[5] 1.06 0 0.07 0.94 1.01 1.06 1.10 1.20 1364 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 2531 1 ## tau[2] 3.90 0 0.03 3.84 3.88 3.90 3.92 3.95 2022 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 3185 1 ## tau[4] 3.03 0 0.04 2.96 3.01 3.03 3.06 3.11 2349 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 2033 1 ## psi[1] 0.60 0 0.02 0.55 0.58 0.60 0.61 0.64 8311 1 ## psi[2] 0.36 0 0.02 0.33 0.35 0.36 0.37 0.40 4523 1 ## psi[3] 0.37 0 0.01 0.35 0.37 0.37 0.38 0.40 9997 1 ## psi[4] 0.60 0 0.02 0.56 0.59 0.60 0.62 0.65 6662 1 ## psi[5] 0.48 0 0.02 0.44 0.47 0.48 0.49 0.52 5167 1 ## phi 0.60 0 0.03 0.53 0.58 0.60 0.62 0.67 1380 1 ## ksi[1] -0.23 0 0.22 -0.67 -0.38 -0.23 -0.08 0.21 17011 1 ## ksi[8] 0.85 0 0.23 0.40 0.70 0.85 1.00 1.30 12305 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Oct 16 13:08:53 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1]&quot;, &quot;ksi[8]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ## Warning: Removed 50 row(s) containing missing values (geom_path). ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(X, use = &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning: Removed 150 rows containing missing values (geom_bar). ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE # Expanded Posterior Plot colnames(dat) &lt;- paste0(&quot;x&quot;,1:5) lav.mod &lt;- &#39; xi =~ 1*x1 + x2 + x3 + x4 + x5 xi ~~ xi x1 ~ 1 x2 ~ 1 x3 ~ 1 x4 ~ 1 x5 ~ 1 &#39; lav.fit &lt;- lavaan::cfa(lav.mod, data=dat) MLE &lt;- lavaan::parameterEstimates(lav.fit) prior_tau &lt;- function(x){dnorm(x, 3, 10)} x.tau&lt;- seq(1, 5, 0.01) prior.tau &lt;- data.frame(tau=x.tau, dens.mtau = prior_tau(x.tau)) prior_lambda &lt;- function(x){dnorm(x, 1, 10)} x.lambda&lt;- seq(0, 2, 0.01) prior.lambda &lt;- data.frame(lambda=x.lambda, dens.lambda = prior_lambda(x.lambda)) prior_sig &lt;- function(x){dinvgamma(x, 5, 10)} x.sig&lt;- seq(.01, 1, 0.01) prior.sig &lt;- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig)) prior_sige &lt;- function(x){dinvgamma(x, 1, 4)} x.sige&lt;- seq(.1, 10, 0.1) prior.sige &lt;- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige)) prior_ksi &lt;- function(x){ mu &lt;- 0 sig &lt;- rinvgamma(1, 5, 10) rnorm(x, mu, sig) } x.ksi&lt;- seq(-5, 5, 0.01) prior.ksi &lt;- data.frame(ksi=prior_ksi(10000)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; # get stan samples plot.data &lt;- as.data.frame(plot.data) # make plotting pieces p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[3, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[4, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`lambda[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.lambda, aes(x=lambda, y=dens.lambda, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[5, 4], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ lims(x=c(0.25, 1.5))+theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + plot_layout(guides=&quot;collect&quot;) ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). ## Warning: Removed 75 row(s) containing missing values (geom_path). # phi p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`phi`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[6,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) # psi p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[1]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[12,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p3 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[2]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[13,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p4 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[3]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[14,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p5 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[4]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[15,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p6 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=`psi[5]`, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sig, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=sqrt(MLE[16,4]), color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(guides = &quot;collect&quot;) "],
["stan-two-latent-variable.html", "9.3 Stan - Two Latent Variable", " 9.3 Stan - Two Latent Variable 9.3.1 Inverse-Wishart Prior Using Stan based on a nearly identical model structure presented in the text. model_cfa_2factor &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; matrix[M, M] phi0; } parameters { matrix[M, M] phi; // latent variable covaraince matrix matrix[N, M] ksi; //latent variable values real lambda[J]; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); // prior for ksi ksi[i] ~ multi_normal(rep_vector(0, M), phi); } // latent variable variance matrix phi ~ inv_wishart(2, phi0); // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat, phi0 = matrix(c(1, .3, .3, 1), ncol=2) ) # # initial values start_values &lt;- list( list( phi= structure( .Data= c(1, 0.30, 0.30, 1), .Dim=c(2, 2)), tau = c(3, 3, 3, 3, 3), lambda= c(1, 1, 1, 1, 1), psi=c(.5, .5, .5, .5, .5) ), list( phi= structure( .Data= c(1, 0, 0, 1), .Dim=c(2, 2)), tau = c(5, 5, 5, 5, 5), lambda= c(1, .7, .7, 1, .7), psi=c(2, 2, 2, 2, 2) ), list( phi= structure( .Data= c(1, 0.10, 0.10, 1), .Dim=c(2, 2)), tau = c(1, 1, 1, 1, 1), lambda= c(1, 1.3, 1.3, 1, 1.3), psi=c(1, 1, 1, 1, 1) ) ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa_2factor, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) control = list(adapt_delta = 0.9, max_treedepth = 12), refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found ## Warning: There were 14950 divergent transitions after warmup. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## to find out why this is a problem and how to eliminate them. ## Warning: There were 50 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 12. See ## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded ## Warning: Examine the pairs() plot to diagnose sampling problems ## Warning: The largest R-hat is 3.79, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 9cba7ccbea0f9cb6bfe744cf6be59b64. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 7.23 ## lambda[2] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 335751.68 ## lambda[3] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 277279.09 ## lambda[4] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 5.87 ## lambda[5] 1.00 0.20 0.24 0.70 0.70 1.00 1.30 1.30 2 397640.53 ## tau[1] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 1763632.95 ## tau[2] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 2151915.04 ## tau[3] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 2081316.86 ## tau[4] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 1660555.71 ## tau[5] 3.00 1.33 1.63 1.00 1.00 3.00 5.00 5.00 2 1654968.94 ## psi[1] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 486301.20 ## psi[2] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 551224.66 ## psi[3] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 596832.25 ## psi[4] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 486021.86 ## psi[5] 1.17 0.51 0.62 0.50 0.50 1.00 2.00 2.00 2 279141.94 ## phi[1,1] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 3.58 ## phi[1,2] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 320432.06 ## phi[2,1] 0.13 0.10 0.12 0.00 0.00 0.10 0.30 0.30 2 320528.37 ## phi[2,2] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 2 5.80 ## ksi[1,1] 0.36 0.80 0.98 -0.74 -0.74 0.17 1.64 1.64 2 1253804.87 ## ksi[1,2] 0.02 0.96 1.18 -1.65 -1.65 0.83 0.87 0.87 2 1456604.89 ## ksi[8,1] -0.60 0.93 1.13 -1.65 -1.65 -1.13 0.97 0.97 2 1536897.56 ## ksi[8,2] 1.18 0.28 0.35 0.71 0.71 1.29 1.54 1.54 2 526350.60 ## ## Samples were drawn using NUTS(diag_e) at Fri Oct 23 10:48:19 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot( fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) 9.3.2 LKJ Cholesky Parameterization Because I had such massive problems with the above, I search for how people estimate CFA models in Stan. I found that most people use the LKJ Cholesky parameterization. Some helpful pages that I used to help get this to work. Stan Userâ€™s Guide on Factor Covaraince Parameterization Michael DeWitt - Confirmatory Factor Analysis in Stan Rick Farouni - Fitting a Bayesian Factor Analysis Model in Stan model_cfa2 &lt;- &quot; data { int N; int J; int M; matrix[N, J] X; } parameters { cholesky_factor_corr[M] L; // Cholesky decomp of // corr mat of random slopes vector[M] A; // Vector of factor variances matrix[N, M] ksi; //latent variable values vector[J] lambda; //factor loadings matrix real tau[J]; //intercepts real&lt;lower=0&gt; psi[J]; //residual variance } transformed parameters { matrix[M, M] A0; vector[M] S; A0 = diag_pre_multiply(A, L); S = sqrt(A); } model { // likelihood for data for(i in 1:N){ X[i, 1] ~ normal(tau[1] + ksi[i,1]*lambda[1], psi[1]); X[i, 2] ~ normal(tau[2] + ksi[i,1]*lambda[2], psi[2]); X[i, 3] ~ normal(tau[3] + ksi[i,1]*lambda[3], psi[3]); X[i, 4] ~ normal(tau[4] + ksi[i,2]*lambda[4], psi[4]); X[i, 5] ~ normal(tau[5] + ksi[i,2]*lambda[5], psi[5]); } // latent variable parameters A ~ inv_gamma(5, 10); L ~ lkj_corr_cholesky(M); for(i in 1:N){ ksi[i] ~ multi_normal_cholesky(rep_vector(0, M), A0); } // prior for measurement model parameters tau ~ normal(3, 10); psi ~ inv_gamma(5, 10); // factor loading patterns lambda[1] ~ normal(1, .001); lambda[2] ~ normal(1, 10); lambda[3] ~ normal(1, 10); lambda[4] ~ normal(1, .001); lambda[5] ~ normal(1, 10); } generated quantities { matrix[M, M] R; matrix[M, M] phi; R = tcrossprod(L); phi = quad_form_diag(R, S); } &quot; # data must be in a list dat &lt;- as.matrix(read.table(&quot;code/CFA-Two-Latent-Variables/Data/IIS.dat&quot;, header=T)) mydata &lt;- list( N = 500, J = 5, M = 2, X = dat ) # Next, need to fit the model # I have explicitly outlined some common parameters fit &lt;- stan( model_code = model_cfa2, # model code to be compiled data = mydata, # my data #init = init_fun, #start_values, # starting values chains = 3, # number of Markov chains #warmup = 1000, # number of warm up iterations per chain iter = 10000, # total number of iterations per chain cores = 3, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;R&quot;, &quot;A&quot;, &quot;A0&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## Inference for Stan model: 09d22e8b8116ca5395ea85ab1df26f50. ## 3 chains, each with iter=10000; warmup=5000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=15000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## lambda[1] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 14546 1 ## lambda[2] 0.87 0 0.06 0.77 0.83 0.87 0.91 0.99 2330 1 ## lambda[3] 0.52 0 0.04 0.44 0.49 0.52 0.55 0.61 3462 1 ## lambda[4] 1.00 0 0.00 1.00 1.00 1.00 1.00 1.00 15733 1 ## lambda[5] 0.96 0 0.05 0.86 0.93 0.96 1.00 1.07 3454 1 ## tau[1] 3.33 0 0.04 3.26 3.31 3.33 3.36 3.41 4440 1 ## tau[2] 3.90 0 0.03 3.85 3.88 3.90 3.92 3.95 3406 1 ## tau[3] 4.60 0 0.02 4.55 4.58 4.60 4.61 4.64 5069 1 ## tau[4] 3.03 0 0.04 2.96 3.01 3.03 3.06 3.11 3858 1 ## tau[5] 3.71 0 0.04 3.64 3.69 3.71 3.74 3.78 3459 1 ## psi[1] 0.61 0 0.02 0.56 0.59 0.61 0.62 0.65 9826 1 ## psi[2] 0.32 0 0.02 0.28 0.31 0.32 0.33 0.36 4452 1 ## psi[3] 0.36 0 0.01 0.33 0.35 0.36 0.36 0.38 13161 1 ## psi[4] 0.57 0 0.03 0.52 0.55 0.57 0.58 0.62 8054 1 ## psi[5] 0.42 0 0.03 0.37 0.41 0.42 0.44 0.47 4909 1 ## R[1,1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## R[1,2] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2177 1 ## R[2,1] 0.86 0 0.03 0.80 0.84 0.86 0.88 0.91 2177 1 ## R[2,2] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## A[1] 0.60 0 0.04 0.53 0.57 0.60 0.62 0.67 2883 1 ## A[2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 4785 1 ## A0[1,1] 0.60 0 0.04 0.53 0.57 0.60 0.62 0.67 2883 1 ## A0[1,2] 0.00 NaN 0.00 0.00 0.00 0.00 0.00 0.00 NaN NaN ## A0[2,1] 0.60 0 0.04 0.53 0.58 0.60 0.63 0.68 6089 1 ## A0[2,2] 0.36 0 0.04 0.29 0.34 0.36 0.39 0.43 1750 1 ## phi[1,1] 0.60 0 0.04 0.53 0.57 0.60 0.62 0.67 2883 1 ## phi[1,2] 0.56 0 0.03 0.49 0.53 0.56 0.58 0.62 4219 1 ## phi[2,1] 0.56 0 0.03 0.49 0.53 0.56 0.58 0.62 4219 1 ## phi[2,2] 0.71 0 0.04 0.64 0.68 0.71 0.73 0.78 4785 1 ## ksi[1,1] -0.22 0 0.23 -0.68 -0.38 -0.22 -0.06 0.23 26119 1 ## ksi[1,2] -0.37 0 0.28 -0.91 -0.55 -0.37 -0.18 0.18 25490 1 ## ksi[8,1] 0.91 0 0.23 0.46 0.75 0.91 1.07 1.37 18842 1 ## ksi[8,2] 0.88 0 0.28 0.34 0.69 0.87 1.06 1.41 25520 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Oct 23 14:11:15 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit,pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars =c(&quot;lambda&quot;, &quot;tau&quot;, &quot;psi&quot;, &quot;phi&quot;, &quot;ksi[1, 1]&quot;, &quot;ksi[1, 2]&quot;, &quot;ksi[8, 1]&quot;, &quot;ksi[8, 2]&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit, family = c(&quot;lambda&quot;))) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_grb(ggs(fit, family = &quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit, family=&quot;lambda&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;tau&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;psi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) ggs_autocorrelation(ggs(fit, family=&quot;phi&quot;)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density plot.data &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(paste0(&quot;lambda[&quot;,1:5,&quot;]&quot;)), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = paste0(&quot;tau[&quot;,1:5,&quot;]&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(paste0(&quot;psi[&quot;,1:5,&quot;]&quot;), &quot;phi[1,1]&quot;, &quot;phi[1,2]&quot;, &quot;phi[2,2]&quot;), prob = 0.8) + plot_title "]
]
