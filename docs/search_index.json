[
["normal-distribution-models.html", "Chapter 4 Normal Distribution Models", " Chapter 4 Normal Distribution Models This chapter was mainly analytic derivations, but there was one section that did code so I show that in JAGS and Stan. "],
["stan-model-for-mean-and-variance-unknown.html", "4.1 Stan Model for mean and variance unknown", " 4.1 Stan Model for mean and variance unknown The model for mean and variance unknown for normal sampling. Figure 4.1: DAG with for mean and variance unknown: Variance parameterization Or, alternatively, Figure 4.2: Model specification diagram for normal model model_normal &lt;- &#39; data { int N; real x[N]; real mu0; real sigma0; real alpha0; real beta0; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { x ~ normal(mu, sigma); mu ~ normal(mu0, sigma0); sigma ~ inv_gamma(alpha0, beta0); } &#39; # data must be in a list mydata &lt;- list( N = 10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92), mu0 = 75, sigma0 = 50, alpha0 = 5, beta0 = 150 ) # start values start_values &lt;- function(){ list(mu=50, sigma=5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_normal, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): ## &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 682d92f82066fa7e19436da3c3fccc69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu 84.00 0.05 4.83 74.12 81.00 84.10 87.05 93.42 8165 1 ## sigma 14.84 0.04 3.86 9.28 12.16 14.16 16.82 24.16 7951 1 ## lp__ -52.88 0.01 1.07 -55.76 -53.30 -52.56 -52.11 -51.83 5327 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Sep 18 10:13:44 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;mu&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;mu&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot posterior &lt;- as.data.frame(posterior) p &lt;- ggplot(posterior, aes(x=mu, y=sigma))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), sd(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(60.01, 120, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sig &lt;- function(x){extraDistr::dinvgamma(x, 5, 150)} x.sig &lt;- seq(0.01, 60, 0.01) prior.sig &lt;- data.frame(sigma=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=posterior, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=posterior, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sigma, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],
["jags-model-for-mean-and-variance-unknown-precision-parameterization.html", "4.2 JAGS Model for mean and variance unknown (precision parameterization)", " 4.2 JAGS Model for mean and variance unknown (precision parameterization) The model for mean and variance unknown for normal sampling. Figure 4.3: DAG with for mean and variance unknown: Precision parameterization Or, alternatively, Figure 4.4: Model specification diagram for normal model with precision parameterization Now for the computation using JAGS # model code jags.model &lt;- function(){ ############################################# # Conditional distribution for the data ############################################# for(i in 1:n){ x[i] ~ dnorm(mu, tau) # conditional distribution of the data } # closes loop over subjects ############################################# # Define the prior distributions for the unknown parameters # The mean of the data (mu) # The variance (sigma.squared) and precision (tau) of the data ############################################# mu ~ dnorm(mu.mu, tau.mu) # prior distribution for mu mu.mu &lt;- 75 # mean of the prior for mu sigma.squared.mu &lt;- 50 # variance of the prior for mu tau.mu &lt;- 1/sigma.squared.mu # precision of the prior for mu tau ~ dgamma(alpha, beta) # precision of the data sigma.squared &lt;- 1/tau # variance of the data sigma &lt;- pow(sigma.squared, 0.5) # taking square root nu.0 &lt;- 10 # hyperparameter for prior for tau sigma.squared.0 &lt;- 30 # hyperparameter for prior for tau alpha &lt;- nu.0/2 # hyperparameter for prior for tau beta &lt;- nu.0*sigma.squared.0/2 # hyperparameter for prior for tau } # data mydata &lt;- list( n=10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92)) # starting values start_values &lt;- function(){ list(&quot;mu&quot;=75, &quot;tau&quot;=0.1) } # vector of all parameters to save param_save &lt;- c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 10 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpmAkVWG/model3e08768a69a1.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## mu 83.253 2.225 78.720 81.811 83.295 84.735 87.512 1.001 10000 ## sigma 7.194 1.241 5.238 6.311 7.036 7.881 10.101 1.001 5700 ## tau 0.021 0.007 0.010 0.016 0.020 0.025 0.036 1.001 5700 ## deviance 71.222 1.872 69.381 69.892 70.666 71.963 76.152 1.001 4800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.8 and DIC = 73.0 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;mu&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;tau&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot p &lt;- ggplot(plot.data, aes(x=mu, y=tau))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), 1/var(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(70.01, 100, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_tau &lt;- function(x){dgamma(x, 5, 150)} x.tau &lt;- seq(0.0001, 0.06, 0.0001) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "]
]
