[
["index.html", "Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy Chapter 1 Getting Started/Overview Chapter", " Bayesian Psychometric Modeling (2016) by Roy Levy and Robert J. Mislevy R. Noah Padgett 2020-09-18 Chapter 1 Getting Started/Overview Chapter This online book is meant to serve a computation reference for the text Bayesian Psychometric Modeling by Roy Levy and Robert Mislevy. I hope that having a detailed computation guide using Stan is of interest to someone. To reference the book, I will frequently use BPM as a shorthand for the title of the text. Throughout this book, I have incorporated elaborations on the code to help set up the examples and provide more details when possible. The authors provided an accompanying website, where the examples are shown using WinBUGS and Netica. The later is used in Chapter 14 for estimation of Bayesian networks. I wanted to deeply dive into this text so rerunning all the examples in a different language provides an excellent excuse to do so. Also, since I can go into more detail in this format than they could, I have not restricted myself by cutting short of the analysis. Meaning that I have done my best to fully (or sufficient for the example) analyze the posteriors to see what issues may pop up and how I resolved them. "],
["software.html", "1.1 Software", " 1.1 Software We have used R to call Stan. So we have relied on rstan heavily. "],
["overview-of-assessment-and-psychometric-modeling.html", "1.2 Overview of Assessment and Psychometric Modeling", " 1.2 Overview of Assessment and Psychometric Modeling Chapter 1 of BPM provided an excellent overview of topics related to psychometric modeling. I wanted to highlight some particularly important pieces to keep in mind while modeling. “[We] view assessment as an instance of evidentiary reasoning.” (p. 3) This idea forms the basis for this text. Providing evidence in support of inferences, claims, decisions, etc. is a major stance of the probabilistic modeling used by Levy and MisLevy. Observed data provide some evidence, but not all data is evidence. This gets to their point that data are grounds to help provide evidence, but they also recognize that evidence does not depend solely on data. What these data represents is a major factor in helping to decide whether evidence has been gathered. Inferences depends on the data and the claim being made. They used excellent examples of the relationships between data, claims, and potential alternative explanations. These examples on pages 5-9 should be read. In summary, the idea is to use our claim to make predictions about what data we should observe. However, we have to use data to make inferences about our claims. The reversal is in line with the relationship between deductive reasoning and inductive reasoning. “A models is a simplified version of a real world scenario, wherein [relevant] features of the problem at hand are represented, while [less relevant features] are suppressed.” They took that stance that they acknowledge that any model they come up with is wrong, but they aim to develop a useful model to help solve a problem at hand. The models they are developing help describe the world according to the analyst. The according to analyst is important since this implies that the analyst is in control of how the model represents the world. The inclusion of the analyst as an active participant in the model building process instead of a passive observer is a realistic representation of the problem of assessment and modeling building. They highlighed 3 goals of modeling (p. 11) “represent the relationships among the relevant entities”; “provide the machinery for making inferences about what is unknown based on what is known”; and “offers machanisms for effectively communicating results to take actions in the real world.” Probability is interpreted as an approach to describing uncertainty about beliefs. This is called the epistemic interpretation of probability. Traditionally, probability in assessment has been interpreted in the frequency of an event. For example, if you flip a coin 100 times we could use a probability of 0.5 to represents the proportion of times the coin would land on heads. This would lead us to expect the number heads to occur to be approximately 50. In the epistemic interpretation, we could describe that we believe that the coin is fair meaning would would place equal weight to heads and tails when flipped. However, if we had a belief that the coin favors heads we could reflect this in the probability we assign to each event. This epistemic interpretation aligns with (1) above where we aim to provide evidence through assessment. Context of assessment should be incorporated into modeling whenever possible. Context (when, where, how long, how much, with whom, etc.) should be considered (at least) as part of the assessment and included in modeling/decision making. Without such details, the analyst may overlook an important consideration in making a decision. Evidence-Centered Design A framework for describing assessments and the context around the assessment. Three properties of ECD are helps us understand the argumentation behind the use of particular psychometric models; helps us through the assessment development process that might lead to such models; and does not require the use of such models. "],
["looking-forward.html", "1.3 Looking Forward", " 1.3 Looking Forward The remainder of this online accompanying text to BPM is organized as follows. Chapters 2-6 round of the Foundational information which includes a introduction of Bayesian inference (Chp 2), a discussion of the conceptual issues in Bayesian inferences (chp 3), a dive into normal distribution models (chp 4), a near 50,000 ft view of estimation with markov chain Monte Carlo (MCMC, chp 5), and introducing notation for regression modeling (chp 6). Next, we turn our attention to the meat of the book with section 2 which is the remainder of the text, chapters 7-14. These chapters move from basic psychometric modeling (chp 7) to classical test theory (chp 8), factor analysis (chp 9), item response theory (chp 11), latent class analysis (chp 13), and networks (chp 14). Other modeling issues and topics are discussed such as model comparison (chp 10) and missing data (chp 12). Throughout all these chapters I will go through all the example analyses using Stan instead of WinBUGS so that potential differences can be compared and discussed. "],
["chp2.html", "Chapter 2 Introduction to Bayesian Inference", " Chapter 2 Introduction to Bayesian Inference Chapter 2 is focused on introducing the fundamentals of Bayesian modeling. I will briefly reiterate some of these concepts, but Levy and Mislevy did an excellent job introducing the basic concepts so I defer to them. A few points I would like to highlight are The concept of likelihood is fundamental to Bayesian methods and frequentist methods as well. The likelihood function (denoted \\(p(\\mathbf{x} \\mid \\theta)\\) or equivalently \\(L(\\theta \\mid \\mathbf{x})\\)) is fundamental as this conditional probability describes our beliefs about the data generating process. Another way of thinking about the likelihood function is as the data model. The data model decribes how parameters of interest relates to the observed data. This key concept is used in frequentist methods (e.g., maximum likelihood estimation) to obtain point estimates of model parameters. A fundamental difference between maximum likelihood and Bayesian estimation is how we use the likelihood function to construct interval estimates of parameters (see next point). Interval estimates in Bayesian methods do not rely on the idea of repeated sampling. In frequentist analyses, the construction of interval estimates around maximum likelihood estimators is dependent on utilizing repeated sampling paradigm. The interval estimate around the MLE is referred to the sampling distribution of the parameter estimator. BPM discusses these features of maximum likelihood well on p. 26. In Bayesian methods, an interval estimate is constructed based on the distribution of the parameter and not the parameter estimator. This distinction makes Bayesian intervals based on the likely values of the parameter based on our prior beliefs and observed data. Bayes Theorem Bayes theorem is the underlying engine of all Bayesian methods. We use Bayes theorm to decompose conditional probabilities so that they can work for us. As an analyst, we interested in the plausible values of the parameters based on the observed data. This can be expressed as a conditional probability (\\(p(\\theta \\mid \\mathbf{x})\\)). Bayes theorm states that \\[\\begin{equation} \\begin{split} p(\\theta \\mid \\mathbf{x}) &amp;= \\frac{p(\\mathbf{x}, \\theta)}{p(\\mathbf{x})}\\\\ &amp;= \\frac{p(\\mathbf{x}\\mid \\theta)p(\\theta)}{p(\\mathbf{x})}.\\\\ \\end{split} \\tag{2.1} \\end{equation}\\] The distinction between frequentist and Bayesian approaches is more than treating model parameters as random. A different way to stating the difference between frequentist and Bayesian approaches is based on what is being conditioned on to make inferences. In a classic frequentist hypothesis testing scenario, the model parameters are conditioned on to calculate the probability of the observed data (i.e., \\(\\mathrm{Pr}(data \\mid \\theta)\\)). This implies that the data are treated as random variables, but this does not exclude the fact the \\(\\theta\\) can be a collection of parameters that have random components (e.g., random intercepts in HLM). However, in a Bayesian model, the model parameters are the object of interest and the data are conditioned on (i.e., \\(\\mathrm{Pr}(\\theta \\mid data)\\)). This implies that the data are treated as a fixed entity that is used to construct inferences. This is how BPM related Bayesian inference to inductive reasoning. The inductive reasoning comes from taking observations and trying to making claims about the general. "],
["beta-binomial-example.html", "2.1 Beta-binomial Example", " 2.1 Beta-binomial Example Here I go through the the first example from BPM. The example is a relatively simple beta-binomial model. Which is a way of modeling the number of occurrences of a bernoulli process. For example, suppose we were interested in the number of times a coin landed on heads. Here, we have a set number of coin flips (say \\(J\\)) and we are interested in the number of times the coin landed on heads (call this outcome \\(y\\)). We can model this structure letting \\(y\\) be a binomial random variable which we can express this as \\[y\\sim\\mathrm{Binomial}(\\theta, J)\\] where \\(\\theta\\) is the probability of heads on any given coin toss. As part of the Bayesian modeling I need to specify my prior belief as to the likely values of \\(\\theta\\). The probability \\(\\theta\\) lies in the interval \\([0, 1]\\). A nice probability distribution on this range is the beta distribution. That is, I can model my belief as to the likely values of the probability of heads by saying that \\(\\theta\\) is beta distributed which can be expressed as \\[\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\]. The two parameters for the beta distribution are representative of the shape the distribution will take. When \\(\\alpha = \\beta\\) the distribution is symmetrical, and when \\(\\alpha = \\beta=1\\) the beta distribution is flat or uniform over \\([0,1]\\). When a distribution is uniform I mean that all values are equally likely over the range of possible values which can be described as having the belief that all values are equally plausible. This model can be represented in a couple different ways. One way is as a directed acyclic graph (DAG). A DAG representation is very similar to path models in general structural equation modeling. The directed nature of the diagram highlights how observed variables (e.g., \\(y\\)) are modeled by unknown parameters \\(\\theta\\). All observed or explicitly defined variables/values are in rectangles while any latent variable or model parameter are in circles. DAG representation of model for the beta-binomal model is Figure 2.1: Directed Acyclic Graph (DAG) for the beta-binomial model I have given an alternative DAG representation that includes all relevant details. In terms of a DAG, I prefer this representation as all the assumed model components are made explicit. However, in more complex models this approach will likely lead to very dense and possible unuseful representations. Figure 2.2: DAG with explicit representation for all beta-binomial model components Yet another alternative representation is what I call a model specification chart. This takes on a similar feel as a DAG in that the flow of model parameters can be shown, but with the major difference that I use the distributional notation explicitly. Figure 2.3: Model specification diagram for beta-binomial model I will stick with these last two representations as much as possible. 2.1.1 Computation using Stan Now, I’m finally getting to the analysis part. I have done my best to be descriptive of what the Stan code represents and how it works (in a general how to use this sense). I highly recommend a look at the example analysis by the development team to help see their approach as well (see here Stan analysis). model_beta_binomial &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y; real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { y ~ binomial(J, theta); theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_binomial, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): ## &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: db495166b911389af4867f0120ac5e81. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.38 0.53 0.6 0.67 0.79 1420 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Sep 11 11:11:59 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. I have downloaded the .bug file from the text website and I will load it into R for viewing. First, let’s take a look at the model described by BPM on p. 39. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) } # data statement list(J = 10, y = 7, alpha = 6, beta = 6) Next, we want to use the above model. Using OpensBUGS through R can be a little clunky as I had to create objects with the filepaths of the data and model code then get R to read those in through the function openbugs. Otherwise, the code is similar to style to the code used for calling Stan. # model code model.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Binomial/Binomial data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Binomial/Binomial Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 3.6 1.1 2.6 2.8 3.2 4.0 6.7 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 4.0 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.1.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# y ~ dbin(theta, J) ################################# # Prior distribution ################################# theta ~ dbeta(alpha, beta) } # data mydata &lt;- list( J = 10, y = 7, alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpkVCUHW/model7fc662b72.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.592 0.100 0.393 0.526 0.594 0.662 0.782 1.003 870 ## deviance 3.547 1.096 2.643 2.766 3.157 3.919 6.495 1.002 1400 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.6 and DIC = 4.1 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],
["beta-bernoulli-example.html", "2.2 Beta-Bernoulli Example", " 2.2 Beta-Bernoulli Example For this next example, I use the same data as the previous model. But now, instead of treating the individual events as part of a whole and sum over the successes, I will treat the model in a more hierarchical manner. A hierarchical model here simply implies that I’ll be using the same probability function for all individual observations. We express this by saying that the observations depend on the index (\\(j=1, 2, ..., J\\)) but that the parameter of interest does not vary across \\(j\\). Two DAG representations similar to the previous examples are shown below. The major difference in these representations from the previous example is the inclusion of a plate that represents the observations depend on the index \\(j\\). Figure 2.4: DAG for the beta-bernoulli model Figure 2.5: DAG with explicit representation for all beta-bernoulli model components In my favor representation, this model can be expressed as Figure 2.6: Model specification diagram for beta-bernoulli model We will use the same \\(\\mathrm{Beta}(\\alpha, \\beta)\\) prior for \\(\\theta\\) as in the previous example. The model code changes to the following, 2.2.1 Computation using Stan model_beta_bernoulli &lt;- &#39; // data block needs to describe the variable // type (e.g., real, int, etc.) and the name // in the data object passed data { int J; int y[J]; //declare observations as an integer vector of length J real alpha; real beta; } // parameters block needs to specify the // unknown parameters parameters { real&lt;lower=0, upper=1&gt;theta; } // model block needs to describe the data-model // and the prior specification model { for(j in 1:J){ y[j] ~ bernoulli(theta); } theta ~ beta(alpha, beta); } // there must be a blank line after all blocks &#39; # data must be in a list mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,1,1,1), alpha = 6, beta = 6 ) # start values can be done automatically by stan or # done explicitly be the analyst (me). I prefer # to try to be explicit so that I can *try* to # guarantee that the initial chains start. # The values can be specified as a function # which lists the values to the respective # parameters start_values &lt;- function(){ list(theta = 0.5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_beta_bernoulli, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): ## &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit, pars=&quot;theta&quot;) ## Inference for Stan model: 7a99cbb09826cf6efe5d323426433fa9. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.59 0 0.1 0.38 0.52 0.59 0.67 0.79 1219 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Sep 18 10:04:10 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit, pars=&quot;theta&quot;) # traceplots rstan::traceplot(fit, pars = c(&quot;theta&quot;), inc_warmup = TRUE) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title # I prefer a posterior plot that includes prior and MLE MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.2 Computation using WinBUGS (OpenBUGS) Here, I am simply contrasting the computation from Stan to how BPM describes the computations using WinBUGS. First, let’s take a look at the model described by BPM on p. 41. # A model block model{ ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } } # data statement list(J=10, y=c(1,0,1,0,1,1,1,1,0,1), alpha=6, beta=6) The code is similar to style to the code used for calling Stan. However you’ll notice a difference in how a probability distribution is referenced. # model code model.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli Model.bug&quot;) # get data file data.file &lt;- paste0(w.d,&quot;/code/Bernoulli/Bernoulli data.txt&quot;) # starting values start_values &lt;- function(){ list(theta=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- openbugs( data= data.file, model.file = model.file, # R grabs the file and runs it in openBUGS parameters.to.save = param_save, inits=start_values, n.chains = 4, n.iter = 2000, n.burnin = 1000, n.thin = 1 ) ## Sampling has been started ... print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/Box/Research/Morgan-Padgett/Bayesian-Psychometric-Modeling/code/Bernoulli/Bernoulli Model.bug&quot;, fit using OpenBUGS, ## 4 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 4000 iterations saved ## mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.6 0.1 0.4 0.5 0.6 0.7 0.8 1 1500 ## deviance 13.2 1.1 12.2 12.3 12.7 13.5 16.3 1 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = Dbar-Dhat) ## pD = 0.4 and DIC = 13.6 ## DIC is an estimate of expected predictive error (lower deviance is better). posterior &lt;- fit$sims.matrix plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=as.data.frame(posterior), aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) 2.2.3 Computation using JAGS (R2jags) Here, I utilize JAGS, which is nearly identical to WinBUGS in how the underlying mechanics work to compute the posterior but is easily to use through R. # model code jags.model &lt;- function(){ ################################# # Conditional distribution of the data ################################# for(j in 1:J){ y[j] ~ dbern(theta) } ################################# # Prior distribution ################################# theta ~ dbeta(alpha,beta) } # data mydata &lt;- list( J = 10, y = c(1,0,1,1,0,0,1,NA,1,1), alpha = 6, beta = 6 ) # starting values start_values &lt;- function(){ list(&quot;theta&quot;=0.5) } # vector of all parameters to save param_save &lt;- c(&quot;theta&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=1000, n.burnin = 500, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 9 ## Unobserved stochastic nodes: 2 ## Total graph size: 14 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpkVCUHW/model7fc47272ed0.txt&quot;, fit using jags, ## 4 chains, each with 1000 iterations (first 500 discarded) ## n.sims = 2000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.570 0.106 0.356 0.496 0.572 0.646 0.770 1.001 1900 ## deviance 12.229 0.958 11.458 11.549 11.866 12.550 15.029 1.001 2000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.5 and DIC = 12.7 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # convert to singel data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;theta&quot;), prob = 0.8) + plot_title MLE &lt;- 0.7 prior &lt;- function(x){dbeta(x, 6, 6)} x &lt;- seq(0, 1, 0.01) prior.dat &lt;- data.frame(X=x, dens = prior(x)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; ggplot()+ geom_density(data=plot.data, aes(x=theta, color=&quot;Posterior&quot;))+ geom_line(data=prior.dat, aes(x=x, y=dens, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE, color=&quot;MLE&quot;))+ labs(title=&quot;Posterior density comparedto prior and MLE&quot;)+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) "],
["conceptual-issues-in-bayesian-inference.html", "Chapter 3 Conceptual Issues in Bayesian Inference", " Chapter 3 Conceptual Issues in Bayesian Inference This chapter was conceptual so there was no code. "],
["normal-distribution-models.html", "Chapter 4 Normal Distribution Models", " Chapter 4 Normal Distribution Models This chapter was mainly analytic derivations, but there was one section that did code so I show that in JAGS and Stan. "],
["stan-model-for-mean-and-variance-unknown.html", "4.1 Stan Model for mean and variance unknown", " 4.1 Stan Model for mean and variance unknown The model for mean and variance unknown for normal sampling. Figure 4.1: DAG with for mean and variance unknown: Variance parameterization Or, alternatively, Figure 4.2: Model specification diagram for normal model model_normal &lt;- &#39; data { int N; real x[N]; real mu0; real sigma0; real alpha0; real beta0; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { x ~ normal(mu, sigma); mu ~ normal(mu0, sigma0); sigma ~ inv_gamma(alpha0, beta0); } &#39; # data must be in a list mydata &lt;- list( N = 10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92), mu0 = 75, sigma0 = 50, alpha0 = 5, beta0 = 150 ) # start values start_values &lt;- function(){ list(mu=50, sigma=5) } # Next, need to fit the model # I have explicited outlined some common parameters fit &lt;- stan( model_code = model_normal, # model code to be compiled data = mydata, # my data init = start_values, # starting values chains = 4, # number of Markov chains warmup = 1000, # number of warm up iterations per chain iter = 5000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0 # no progress shown ) ## Warning in system(paste(CXX, ARGS), ignore.stdout = TRUE, ignore.stderr = TRUE): ## &#39;-E&#39; not found # first get a basic breakdown of the posteriors print(fit) ## Inference for Stan model: 682d92f82066fa7e19436da3c3fccc69. ## 4 chains, each with iter=5000; warmup=1000; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=16000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu 84.00 0.05 4.83 74.12 81.00 84.10 87.05 93.42 8165 1 ## sigma 14.84 0.04 3.86 9.28 12.16 14.16 16.82 24.16 7951 1 ## lp__ -52.88 0.01 1.07 -55.76 -53.30 -52.56 -52.11 -51.83 5327 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Sep 18 10:13:44 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # plot the posterior in a # 95% probability interval # and 80% to contrast the dispersion plot(fit) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) # traceplots rstan::traceplot(fit, pars = c(&quot;mu&quot;, &quot;sigma&quot;), inc_warmup = TRUE) # Gelman-Rubin-Brooks Convergence Criterion ggs_grb(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # autocorrelation ggs_autocorrelation(ggs(fit)) + theme_bw() + theme(panel.grid = element_blank()) # plot the posterior density posterior &lt;- as.matrix(fit) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( posterior, pars = c(&quot;mu&quot;, &quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot posterior &lt;- as.data.frame(posterior) p &lt;- ggplot(posterior, aes(x=mu, y=sigma))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), sd(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(60.01, 120, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_sig &lt;- function(x){extraDistr::dinvgamma(x, 5, 150)} x.sig &lt;- seq(0.01, 60, 0.01) prior.sig &lt;- data.frame(sigma=x.sig, dens.sig = prior_sig(x.sig)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=posterior, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=posterior, aes(x=sigma, color=&quot;Posterior&quot;))+ geom_line(data=prior.sig, aes(x=sigma, y=dens.sig, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],
["jags-model-for-mean-and-variance-unknown-precision-parameterization.html", "4.2 JAGS Model for mean and variance unknown (precision parameterization)", " 4.2 JAGS Model for mean and variance unknown (precision parameterization) The model for mean and variance unknown for normal sampling. Figure 4.3: DAG with for mean and variance unknown: Precision parameterization Or, alternatively, Figure 4.4: Model specification diagram for normal model with precision parameterization Now for the computation using JAGS # model code jags.model &lt;- function(){ ############################################# # Conditional distribution for the data ############################################# for(i in 1:n){ x[i] ~ dnorm(mu, tau) # conditional distribution of the data } # closes loop over subjects ############################################# # Define the prior distributions for the unknown parameters # The mean of the data (mu) # The variance (sigma.squared) and precision (tau) of the data ############################################# mu ~ dnorm(mu.mu, tau.mu) # prior distribution for mu mu.mu &lt;- 75 # mean of the prior for mu sigma.squared.mu &lt;- 50 # variance of the prior for mu tau.mu &lt;- 1/sigma.squared.mu # precision of the prior for mu tau ~ dgamma(alpha, beta) # precision of the data sigma.squared &lt;- 1/tau # variance of the data sigma &lt;- pow(sigma.squared, 0.5) # taking square root nu.0 &lt;- 10 # hyperparameter for prior for tau sigma.squared.0 &lt;- 30 # hyperparameter for prior for tau alpha &lt;- nu.0/2 # hyperparameter for prior for tau beta &lt;- nu.0*sigma.squared.0/2 # hyperparameter for prior for tau } # data mydata &lt;- list( n=10, x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92)) # starting values start_values &lt;- function(){ list(&quot;mu&quot;=75, &quot;tau&quot;=0.1) } # vector of all parameters to save param_save &lt;- c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;) # fit model fit &lt;- jags( model.file=jags.model, data=mydata, inits=start_values, parameters.to.save = param_save, n.iter=4000, n.burnin = 1000, n.chains = 4, n.thin=1, progress.bar = &quot;none&quot;) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 10 ## Unobserved stochastic nodes: 2 ## Total graph size: 26 ## ## Initializing model print(fit) ## Inference for Bugs model at &quot;C:/Users/noahp/AppData/Local/Temp/RtmpmAkVWG/model3e08768a69a1.txt&quot;, fit using jags, ## 4 chains, each with 4000 iterations (first 1000 discarded) ## n.sims = 12000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## mu 83.253 2.225 78.720 81.811 83.295 84.735 87.512 1.001 10000 ## sigma 7.194 1.241 5.238 6.311 7.036 7.881 10.101 1.001 5700 ## tau 0.021 0.007 0.010 0.016 0.020 0.025 0.036 1.001 5700 ## deviance 71.222 1.872 69.381 69.892 70.666 71.963 76.152 1.001 4800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.8 and DIC = 73.0 ## DIC is an estimate of expected predictive error (lower deviance is better). # extract posteriors for all chains jags.mcmc &lt;- as.mcmc(fit) R2jags::traceplot(jags.mcmc) # gelman-rubin-brook gelman.plot(jags.mcmc) # convert to single data.frame for density plot a &lt;- colnames(as.data.frame(jags.mcmc[[1]])) plot.data &lt;- data.frame(as.matrix(jags.mcmc, chains=T, iters = T)) colnames(plot.data) &lt;- c(&quot;chain&quot;, &quot;iter&quot;, a) plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;, &quot;with medians and 80% intervals&quot;) mcmc_areas( plot.data, pars = c(&quot;mu&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;tau&quot;), prob = 0.8) + plot_title mcmc_areas( plot.data, pars = c(&quot;sigma&quot;), prob = 0.8) + plot_title # bivariate plot p &lt;- ggplot(plot.data, aes(x=mu, y=tau))+ geom_point()+ theme_bw()+ theme(panel.grid = element_blank()) p # I prefer a posterior plot that includes prior and MLE MLE &lt;- c(mean(mydata$x), 1/var(mydata$x)) prior_mu &lt;- function(x){dnorm(x, 75, 50)} x.mu &lt;- seq(70.01, 100, 0.01) prior.mu &lt;- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu)) prior_tau &lt;- function(x){dgamma(x, 5, 150)} x.tau &lt;- seq(0.0001, 0.06, 0.0001) prior.tau &lt;- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau)) cols &lt;- c(&quot;Posterior&quot;=&quot;#0072B2&quot;, &quot;Prior&quot;=&quot;#E69F00&quot;, &quot;MLE&quot;= &quot;black&quot;)#&quot;#56B4E9&quot;, &quot;#E69F00&quot; &quot;#CC79A7&quot; p1 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=mu, color=&quot;Posterior&quot;))+ geom_line(data=prior.mu, aes(x=x.mu, y=dens.mu, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[1], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p2 &lt;- ggplot()+ geom_density(data=plot.data, aes(x=tau, color=&quot;Posterior&quot;))+ geom_line(data=prior.tau, aes(x=tau, y=dens.tau, color=&quot;Prior&quot;))+ geom_vline(aes(xintercept=MLE[2], color=&quot;MLE&quot;))+ scale_color_manual(values=cols, name=NULL)+ theme_bw()+ theme(panel.grid = element_blank()) p1 + p2 + plot_layout(guides=&quot;collect&quot;) "],
["markov-chain-monte-carlo-estimation.html", "Chapter 5 Markov Chain Monte Carlo Estimation", " Chapter 5 Markov Chain Monte Carlo Estimation Some significant applications are demonstrated in this chapter. "],
["gibbs-sampling.html", "5.1 Gibbs Sampling", " 5.1 Gibbs Sampling "],
["metropolis-sampling.html", "5.2 Metropolis Sampling", " 5.2 Metropolis Sampling "],
["regression.html", "Chapter 6 Regression", " Chapter 6 Regression "],
["canonical-bayesian-psychometric-modeling.html", "Chapter 7 Canonical Bayesian Psychometric Modeling", " Chapter 7 Canonical Bayesian Psychometric Modeling "],
["references.html", "References", " References "]
]
