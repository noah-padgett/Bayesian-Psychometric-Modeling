[{"path":"overview.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"online book meant serve computation reference text Bayesian Psychometric Modeling Roy Levy Robert Mislevy.\nhope detailed computation guide using Stan interest someone.\nreference book, frequently use BPM shorthand title text.Throughout book, incorporated elaborations code help set examples provide details possible.\nauthors provided accompanying website, examples shown using WinBUGS Netica.\nlater used Chapter 14 estimation Bayesian networks.\nwanted deeply dive text rerunning examples different language provides excellent excuse .Also, since can go detail format , restricted cutting short analysis.\nMeaning done best fully (sufficient example) analyze posteriors see issues may pop resolved .","code":""},{"path":"overview.html","id":"software","chapter":"1 Overview","heading":"1.1 Software","text":"used R call Stan. relied rstan heavily.NEED UPDATE - NEEDS LOT INFORMATION","code":""},{"path":"overview.html","id":"overview-of-assessment-and-psychometric-modeling","chapter":"1 Overview","heading":"1.2 Overview of Assessment and Psychometric Modeling","text":"Chapter 1 BPM provided excellent overview topics related psychometric modeling.\nwanted highlight particularly important pieces keep mind modeling.“[] view assessment instance evidentiary reasoning.” (p. 3)\nidea forms basis text. Providing evidence support inferences, claims, decisions, etc. major stance probabilistic modeling used Levy MisLevy.“[] view assessment instance evidentiary reasoning.” (p. 3)idea forms basis text. Providing evidence support inferences, claims, decisions, etc. major stance probabilistic modeling used Levy MisLevy.Observed data provide evidence, data evidence.\ngets point data grounds help provide evidence, also recognize evidence depend solely data.\ndata represents major factor helping decide whether evidence gathered.Observed data provide evidence, data evidence.gets point data grounds help provide evidence, also recognize evidence depend solely data.\ndata represents major factor helping decide whether evidence gathered.Inferences depends data claim made.\nused excellent examples relationships data, claims, potential alternative explanations.\nexamples pages 5-9 read.\nsummary, idea use claim make predictions data observe.\nHowever, use data make inferences claims.\nreversal line relationship deductive reasoning inductive reasoning.Inferences depends data claim made.used excellent examples relationships data, claims, potential alternative explanations.\nexamples pages 5-9 read.\nsummary, idea use claim make predictions data observe.\nHowever, use data make inferences claims.\nreversal line relationship deductive reasoning inductive reasoning.“models simplified version real world scenario, wherein [relevant] features problem hand represented, [less relevant features] suppressed.”\ntook stance acknowledge model come wrong, aim develop useful model help solve problem hand.\nmodels developing help describe world according analyst.\naccording analyst important since implies analyst control model represents world.\ninclusion analyst active participant model building process instead passive observer realistic representation problem assessment modeling building.\nhighlighed 3 goals modeling (p. 11)\n“represent relationships among relevant entities”;\n“provide machinery making inferences unknown based known”; \n“offers machanisms effectively communicating results take actions real world.”\n“models simplified version real world scenario, wherein [relevant] features problem hand represented, [less relevant features] suppressed.”took stance acknowledge model come wrong, aim develop useful model help solve problem hand.\nmodels developing help describe world according analyst.\naccording analyst important since implies analyst control model represents world.\ninclusion analyst active participant model building process instead passive observer realistic representation problem assessment modeling building.\nhighlighed 3 goals modeling (p. 11)“represent relationships among relevant entities”;“provide machinery making inferences unknown based known”; “offers machanisms effectively communicating results take actions real world.”Probability interpreted approach describing uncertainty beliefs.\ncalled epistemic interpretation probability.\nTraditionally, probability assessment interpreted frequency event.\nexample, flip coin 100 times use probability 0.5 represents proportion times coin land heads.\nlead us expect number heads occur approximately 50.\nepistemic interpretation, describe believe coin fair meaning place equal weight heads tails flipped.\nHowever, belief coin favors heads reflect probability assign event.\nepistemic interpretation aligns (1) aim provide evidence assessment.Probability interpreted approach describing uncertainty beliefs.called epistemic interpretation probability.\nTraditionally, probability assessment interpreted frequency event.\nexample, flip coin 100 times use probability 0.5 represents proportion times coin land heads.\nlead us expect number heads occur approximately 50.\nepistemic interpretation, describe believe coin fair meaning place equal weight heads tails flipped.\nHowever, belief coin favors heads reflect probability assign event.\nepistemic interpretation aligns (1) aim provide evidence assessment.Context assessment incorporated modeling whenever possible.\nContext (, , long, much, , etc.) considered (least) part assessment included modeling/decision making.\nWithout details, analyst may overlook important consideration making decision.Context assessment incorporated modeling whenever possible.Context (, , long, much, , etc.) considered (least) part assessment included modeling/decision making.\nWithout details, analyst may overlook important consideration making decision.Evidence-Centered Design\nframework describing assessments context around assessment.\nThree properties ECD \nhelps us understand argumentation behind use particular psychometric models;\nhelps us assessment development process might lead models; \nrequire use models.\nEvidence-Centered DesignA framework describing assessments context around assessment.\nThree properties ECD arehelps us understand argumentation behind use particular psychometric models;helps us assessment development process might lead models; anddoes require use models.","code":""},{"path":"overview.html","id":"looking-forward","chapter":"1 Overview","heading":"1.3 Looking Forward","text":"remainder online accompanying text BPM organized follows.\nChapters 2-6 round Foundational information includes introduction Bayesian inference (Chp 2), discussion conceptual issues Bayesian inferences (chp 3), dive normal distribution models (chp 4), near 50,000 ft view estimation markov chain Monte Carlo (MCMC, chp 5), introducing notation regression modeling (chp 6).\nNext, turn attention meat book section 2 remainder text, chapters 7-14.\nchapters move basic psychometric modeling (chp 7) classical test theory (chp 8), factor analysis (chp 9), item response theory (chp 11), latent class analysis (chp 13), networks (chp 14).\nmodeling issues topics discussed model comparison (chp 10) missing data (chp 12).\nThroughout chapters go example analyses using Stan instead WinBUGS potential differences can compared discussed.","code":""},{"path":"chp2.html","id":"chp2","chapter":"2 Introduction to Bayesian Inference","heading":"2 Introduction to Bayesian Inference","text":"Chapter 2 focused introducing fundamentals Bayesian modeling.\nbriefly reiterate concepts, Levy Mislevy excellent job introducing basic concepts defer .\npoints like highlight areThe concept likelihood fundamental Bayesian methods frequentist methods well.\nlikelihood function (denoted \\(p(\\mathbf{x} \\mid \\theta)\\) equivalently \\(L(\\theta \\mid \\mathbf{x})\\)) fundamental conditional probability describes beliefs data generating process.\nAnother way thinking likelihood function data model.\ndata model decribes parameters interest relates observed data.\nkey concept used frequentist methods (e.g., maximum likelihood estimation) obtain point estimates model parameters.\nfundamental difference maximum likelihood Bayesian estimation use likelihood function construct interval estimates parameters (see next point).concept likelihood fundamental Bayesian methods frequentist methods well.likelihood function (denoted \\(p(\\mathbf{x} \\mid \\theta)\\) equivalently \\(L(\\theta \\mid \\mathbf{x})\\)) fundamental conditional probability describes beliefs data generating process.\nAnother way thinking likelihood function data model.\ndata model decribes parameters interest relates observed data.\nkey concept used frequentist methods (e.g., maximum likelihood estimation) obtain point estimates model parameters.\nfundamental difference maximum likelihood Bayesian estimation use likelihood function construct interval estimates parameters (see next point).Interval estimates Bayesian methods rely idea repeated sampling.\nfrequentist analyses, construction interval estimates around maximum likelihood estimators dependent utilizing repeated sampling paradigm.\ninterval estimate around MLE referred sampling distribution parameter estimator.\nBPM discusses features maximum likelihood well p. 26.\nBayesian methods, interval estimate constructed based distribution parameter parameter estimator.\ndistinction makes Bayesian intervals based likely values parameter based prior beliefs observed data.Interval estimates Bayesian methods rely idea repeated sampling.frequentist analyses, construction interval estimates around maximum likelihood estimators dependent utilizing repeated sampling paradigm.\ninterval estimate around MLE referred sampling distribution parameter estimator.\nBPM discusses features maximum likelihood well p. 26.\nBayesian methods, interval estimate constructed based distribution parameter parameter estimator.\ndistinction makes Bayesian intervals based likely values parameter based prior beliefs observed data.Bayes Theorem\nBayes theorem underlying engine Bayesian methods.\nuse Bayes theorm decompose conditional probabilities can work us.\nanalyst, interested plausible values parameters based observed data.\ncan expressed conditional probability (\\(p(\\theta \\mid \\mathbf{x})\\)).\nBayes theorm states \n\\[\\begin{equation}\n  \\begin{split}\n  p(\\theta \\mid \\mathbf{x}) &= \\frac{p(\\mathbf{x}, \\theta)}{p(\\mathbf{x})}\\\\\n&= \\frac{p(\\mathbf{x}\\mid \\theta)p(\\theta)}{p(\\mathbf{x})}.\\\\\n  \\end{split}\n  \\tag{2.1}\n\\end{equation}\\]Bayes TheoremBayes theorem underlying engine Bayesian methods.\nuse Bayes theorm decompose conditional probabilities can work us.\nanalyst, interested plausible values parameters based observed data.\ncan expressed conditional probability (\\(p(\\theta \\mid \\mathbf{x})\\)).\nBayes theorm states \n\\[\\begin{equation}\n  \\begin{split}\n  p(\\theta \\mid \\mathbf{x}) &= \\frac{p(\\mathbf{x}, \\theta)}{p(\\mathbf{x})}\\\\\n&= \\frac{p(\\mathbf{x}\\mid \\theta)p(\\theta)}{p(\\mathbf{x})}.\\\\\n  \\end{split}\n  \\tag{2.1}\n\\end{equation}\\]distinction frequentist Bayesian approaches treating model parameters random. different way stating difference frequentist Bayesian approaches based conditioned make inferences.\nclassic frequentist hypothesis testing scenario, model parameters conditioned calculate probability observed data (.e., \\(\\mathrm{Pr}(data \\mid \\theta)\\)).\nimplies data treated random variables, exclude fact \\(\\theta\\) can collection parameters random components (e.g., random intercepts HLM).\nHowever, Bayesian model, model parameters object interest data conditioned (.e., \\(\\mathrm{Pr}(\\theta \\mid data)\\)).\nimplies data treated fixed entity used construct inferences.\nBPM related Bayesian inference inductive reasoning.\ninductive reasoning comes taking observations trying making claims general.distinction frequentist Bayesian approaches treating model parameters random. different way stating difference frequentist Bayesian approaches based conditioned make inferences.classic frequentist hypothesis testing scenario, model parameters conditioned calculate probability observed data (.e., \\(\\mathrm{Pr}(data \\mid \\theta)\\)).\nimplies data treated random variables, exclude fact \\(\\theta\\) can collection parameters random components (e.g., random intercepts HLM).\nHowever, Bayesian model, model parameters object interest data conditioned (.e., \\(\\mathrm{Pr}(\\theta \\mid data)\\)).\nimplies data treated fixed entity used construct inferences.\nBPM related Bayesian inference inductive reasoning.\ninductive reasoning comes taking observations trying making claims general.","code":""},{"path":"chp2.html","id":"beta-binomial-example","chapter":"2 Introduction to Bayesian Inference","heading":"2.1 Beta-binomial Example","text":"go first example BPM.\nexample relatively simple beta-binomial model.\nway modeling number occurrences bernoulli process.\nexample, suppose interested number times coin landed heads.\n, set number coin flips (say \\(J\\)) interested number times coin landed heads (call outcome \\(y\\)).\ncan model structure letting \\(y\\) binomial random variable can express \n\\[y\\sim\\mathrm{Binomial}(\\theta, J)\\]\n\\(\\theta\\) probability heads given coin toss.\npart Bayesian modeling need specify prior belief likely values \\(\\theta\\).\nprobability \\(\\theta\\) lies interval \\([0, 1]\\).\nnice probability distribution range beta distribution.\n, can model belief likely values probability heads saying \\(\\theta\\) beta distributed can expressed \n\\[\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\].\ntwo parameters beta distribution representative shape distribution take.\n\\(\\alpha = \\beta\\) distribution symmetrical, \\(\\alpha = \\beta=1\\) beta distribution flat uniform \\([0,1]\\).\ndistribution uniform mean values equally likely range possible values can described belief values equally plausible.model can represented couple different ways. One way directed acyclic graph (DAG).\nDAG representation similar path models general structural equation modeling.\ndirected nature diagram highlights observed variables (e.g., \\(y\\)) modeled unknown parameters \\(\\theta\\).\nobserved explicitly defined variables/values rectangles latent variable model parameter circles.\nDAG representation model beta-binomal model \nFigure 2.1: Directed Acyclic Graph (DAG) beta-binomial model\ngiven alternative DAG representation includes relevant details.\nterms DAG, prefer representation assumed model components made explicit.\nHowever, complex models approach likely lead dense possible unuseful representations.\nFigure 2.2: DAG explicit representation beta-binomial model components\nYet another alternative representation call model specification chart.\ntakes similar feel DAG flow model parameters can shown, major difference use distributional notation explicitly.\nFigure 2.3: Model specification diagram beta-binomial model\nstick last two representations much possible.","code":""},{"path":"chp2.html","id":"computation-using-stan","chapter":"2 Introduction to Bayesian Inference","heading":"2.1.1 Computation using Stan","text":"Now, ’m finally getting analysis part.\ndone best descriptive Stan code represents works (general use sense).\nhighly recommend look example analysis development team help see approach well (see Stan analysis).","code":"\nmodel_beta_binomial <- '\n// data block needs to describe the variable\n//  type (e.g., real, int, etc.) and the name\n//  in the data object passed\ndata {\n  int J;\n  int y;\n  real alpha;\n  real beta;\n}\n// parameters block needs to specify the \n//  unknown parameters\nparameters {\n  real<lower=0, upper=1>theta;\n}\n// model block needs to describe the data-model\n//  and the prior specification\nmodel {\n  y ~ binomial(J, theta);\n  theta ~ beta(alpha, beta);\n}\n// there must be a blank line after all blocks\n\n'\n# data must be in a list\nmydata <- list(\n  J = 10,\n  y = 7,\n  alpha = 6,\n  beta = 6\n)\n\n# start values can be done automatically by stan or\n#   done explicitly be the analyst (me). I prefer \n#   to try to be explicit so that I can *try* to \n#   guarantee that the initial chains start.\n#   The values can be specified as a function \n#   which lists the values to the respective \n#   parameters\nstart_values <- function(){\n  list(theta = 0.5)\n}\n\n# Next, need to fit the model\n#   I have explicited outlined some common parameters\nfit <- stan(\n  model_code = model_beta_binomial, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warmup iterations per chain\n  iter = 2000,            # total number of iterations per chain\n  cores = 2,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit, pars=\"theta\")## Inference for Stan model: anon_model.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##       mean se_mean  sd 2.5%  25% 50%  75% 97.5% n_eff Rhat\n## theta 0.59       0 0.1 0.39 0.53 0.6 0.66  0.78  1511    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:27:45 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit, pars=\"theta\")\n# traceplots\nrstan::traceplot(fit, pars = c(\"theta\"), inc_warmup = TRUE)\n# plot the posterior density\nposterior <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  posterior,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=as.data.frame(posterior),\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"chp2.html","id":"computation-using-winbugs-openbugs","chapter":"2 Introduction to Bayesian Inference","heading":"2.1.2 Computation using WinBUGS (OpenBUGS)","text":", simply contrasting computation Stan BPM describes computations using WinBUGS.\ndownloaded .bug file text website load R viewing.First, let’s take look model described BPM p. 39.Next, want use model.\nUsing OpensBUGS R can little clunky create objects filepaths data model code get R read function openbugs.\nOtherwise, code similar style code used calling Stan.","code":"# A model block\nmodel{\n  #################################\n  # Prior distribution\n  #################################\n  theta ~ dbeta(alpha,beta)\n  #################################\n  # Conditional distribution of the data\n  #################################\n  y ~ dbin(theta, J)\n}\n# data statement\nlist(J = 10, y = 7, alpha = 6, beta = 6)\n# model code\nmodel.file <- paste0(w.d,\"/code/Binomial/Binomial Model.bug\")\n\n# get data file\ndata.file <- paste0(w.d,\"/code/Binomial/Binomial data.txt\")\n\n# starting values\nstart_values <- function(){\n  list(theta=0.5)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"theta\")\n\n# fit model\nfit <- openbugs(\n  data= data.file, \n  model.file = model.file, # R grabs the file and runs it in openBUGS\n  parameters.to.save = param_save,\n  inits=start_values,\n  n.chains = 4,\n  n.iter = 2000,\n  n.burnin = 1000,\n  n.thin = 1\n)\n\nprint(fit)\n\nposterior <- fit$sims.matrix\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  posterior,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\n\n\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=as.data.frame(posterior),\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  labs(title=\"Posterior density comparedto prior and MLE\")+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"chp2.html","id":"computation-using-jags-r2jags","chapter":"2 Introduction to Bayesian Inference","heading":"2.1.3 Computation using JAGS (R2jags)","text":", utilize JAGS, nearly identical WinBUGS underlying mechanics work compute posterior easily use R.","code":"\n# model code\njags.model <- function(){\n  #################################\n  # Conditional distribution of the data\n  #################################\n  y ~ dbin(theta, J)\n  #################################\n  # Prior distribution\n  #################################\n  theta ~ dbeta(alpha, beta)\n}\n# data\nmydata <- list(\n  J = 10,\n  y = 7,\n  alpha = 6,\n  beta = 6\n)\n# starting values\nstart_values <- function(){\n  list(\"theta\"=0.5)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"theta\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=1000,\n  n.burnin = 500,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 1\n##    Unobserved stochastic nodes: 1\n##    Total graph size: 5\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model45447f3b5e11.txt\", fit using jags,\n##  4 chains, each with 1000 iterations (first 500 discarded)\n##  n.sims = 2000 iterations saved\n##          mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff\n## theta      0.596   0.103 0.396 0.525 0.596 0.670 0.791 1.002  2000\n## deviance   3.536   1.056 2.644 2.749 3.151 3.937 6.416 1.004   960\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 0.6 and DIC = 4.1\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# convert to singel data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=plot.data,\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  labs(title=\"Posterior density comparedto prior and MLE\")+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"chp2.html","id":"beta-bernoulli-example","chapter":"2 Introduction to Bayesian Inference","heading":"2.2 Beta-Bernoulli Example","text":"next example, use data previous model.\nnow, instead treating individual events part whole sum successes, treat model hierarchical manner.\nhierarchical model simply implies ’ll using probability function individual observations.\nexpress saying observations depend index (\\(j=1, 2, ..., J\\)) parameter interest vary across \\(j\\).\nTwo DAG representations similar previous examples shown .\nmajor difference representations previous example inclusion plate represents observations depend index \\(j\\).\nFigure 2.4: DAG beta-bernoulli model\n\nFigure 2.5: DAG explicit representation beta-bernoulli model components\nflavor representation, model can expressed \nFigure 2.6: Model specification diagram beta-bernoulli model\nuse \\(\\mathrm{Beta}(\\alpha, \\beta)\\) prior \\(\\theta\\) previous example.\nmodel code changes following,","code":""},{"path":"chp2.html","id":"computation-using-stan-1","chapter":"2 Introduction to Bayesian Inference","heading":"2.2.1 Computation using Stan","text":"","code":"\nmodel_beta_bernoulli <- '\n// data block needs to describe the variable\n//  type (e.g., real, int, etc.) and the name\n//  in the data object passed\ndata {\n  int J;\n  int y[J];  //declare observations as an integer vector of length J\n  real alpha;\n  real beta;\n}\n// parameters block needs to specify the \n//  unknown parameters\nparameters {\n  real<lower=0, upper=1>theta;\n}\n// model block needs to describe the data-model\n//  and the prior specification\nmodel {\n  for(j in 1:J){\n    y[j] ~ bernoulli(theta);\n  }\n  theta ~ beta(alpha, beta);\n}\n// there must be a blank line after all blocks\n\n'\n# data must be in a list\nmydata <- list(\n  J = 10,\n  y = c(1,0,1,1,0,0,1,1,1,1),\n  alpha = 6,\n  beta = 6\n)\n\n# start values can be done automatically by stan or\n#   done explicitly be the analyst (me). I prefer \n#   to try to be explicit so that I can *try* to \n#   guarantee that the initial chains start.\n#   The values can be specified as a function \n#   which lists the values to the respective \n#   parameters\nstart_values <- function(){\n  list(theta = 0.5)\n}\n\n# Next, need to fit the model\n#   I have explicited outlined some common parameters\nfit <- stan(\n  model_code = model_beta_bernoulli, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warmup iterations per chain\n  iter = 2000,            # total number of iterations per chain\n  cores = 2,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit, pars=\"theta\")## Inference for Stan model: anon_model.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##       mean se_mean  sd 2.5%  25% 50%  75% 97.5% n_eff Rhat\n## theta 0.59       0 0.1 0.38 0.53 0.6 0.67  0.78  1527    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:28:34 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit, pars=\"theta\")\n# traceplots\nrstan::traceplot(fit, pars = c(\"theta\"), inc_warmup = TRUE)\n# plot the posterior density\nposterior <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  posterior,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=as.data.frame(posterior),\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"chp2.html","id":"computation-using-winbugs-openbugs-1","chapter":"2 Introduction to Bayesian Inference","heading":"2.2.2 Computation using WinBUGS (OpenBUGS)","text":", simply contrasting computation Stan BPM describes computations using WinBUGS.\nFirst, let’s take look model described BPM p. 41.code similar style code used calling Stan.\nHowever ’ll notice difference probability distribution referenced.","code":"# A model block\nmodel{\n  #################################\n  # Prior distribution\n  #################################\n  theta ~ dbeta(alpha,beta)\n  \n  #################################\n  # Conditional distribution of the data\n  #################################\n  for(j in 1:J){\n    y[j] ~ dbern(theta)\n  }\n}\n# data statement\nlist(J=10, y=c(1,0,1,0,1,1,1,1,0,1), alpha=6, beta=6)\n# model code\nmodel.file <- paste0(w.d,\"/code/Bernoulli/Bernoulli Model.bug\")\n\n# get data file\ndata.file <- paste0(w.d,\"/code/Bernoulli/Bernoulli data.txt\")\n\n# starting values\nstart_values <- function(){\n  list(theta=0.5)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"theta\")\n\n# fit model\nfit <- openbugs(\n  data= data.file, \n  model.file = model.file, # R grabs the file and runs it in openBUGS\n  parameters.to.save = param_save,\n  inits=start_values,\n  n.chains = 4,\n  n.iter = 2000,\n  n.burnin = 1000,\n  n.thin = 1\n)\n\nprint(fit)\n\nposterior <- fit$sims.matrix\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  posterior,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\n\n\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=as.data.frame(posterior),\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  labs(title=\"Posterior density comparedto prior and MLE\")+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"chp2.html","id":"computation-using-jags-r2jags-1","chapter":"2 Introduction to Bayesian Inference","heading":"2.2.3 Computation using JAGS (R2jags)","text":", utilize JAGS, nearly identical WinBUGS underlying mechanics work compute posterior easily use R.","code":"\n# model code\njags.model <- function(){\n  #################################\n  # Conditional distribution of the data\n  #################################\n  for(j in 1:J){\n    y[j] ~ dbern(theta)\n  }\n\n  #################################\n  # Prior distribution\n  #################################\n  theta ~ dbeta(alpha,beta)\n  \n}\n# data\nmydata <- list(\n  J = 10,\n  y = c(1,0,1,1,0,0,1,NA,1,1),\n  alpha = 6,\n  beta = 6\n)\n\n# starting values\nstart_values <- function(){\n  list(\"theta\"=0.5)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"theta\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=1000,\n  n.burnin = 500,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 9\n##    Unobserved stochastic nodes: 2\n##    Total graph size: 14\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model454429615b74.txt\", fit using jags,\n##  4 chains, each with 1000 iterations (first 500 discarded)\n##  n.sims = 2000 iterations saved\n##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\n## theta      0.570   0.105  0.359  0.498  0.572  0.644  0.764 1.003  1200\n## deviance  12.223   0.991 11.458 11.548 11.849 12.511 14.968 1.005   700\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 0.5 and DIC = 12.7\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# convert to singel data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"theta\"),\n  prob = 0.8) + \n  plot_title\nMLE <- 0.7\nprior <- function(x){dbeta(x, 6, 6)}\nx <- seq(0, 1, 0.01)\nprior.dat <- data.frame(X=x, dens = prior(x))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nggplot()+\n  geom_density(data=plot.data,\n               aes(x=theta, color=\"Posterior\"))+\n  geom_line(data=prior.dat,\n            aes(x=x, y=dens, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE, color=\"MLE\"))+\n  labs(title=\"Posterior density comparedto prior and MLE\")+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())"},{"path":"conceptual-issues-in-bayesian-inference.html","id":"conceptual-issues-in-bayesian-inference","chapter":"3 Conceptual Issues in Bayesian Inference","heading":"3 Conceptual Issues in Bayesian Inference","text":"chapter conceptual code.","code":""},{"path":"normal-distribution-models.html","id":"normal-distribution-models","chapter":"4 Normal Distribution Models","heading":"4 Normal Distribution Models","text":"chapter mainly analytic derivations, one section code show JAGS Stan.","code":""},{"path":"normal-distribution-models.html","id":"stan-model-for-mean-and-variance-unknown","chapter":"4 Normal Distribution Models","heading":"4.1 Stan Model for mean and variance unknown","text":"model mean variance unknown normal sampling.\nFigure 4.1: DAG mean variance unknown: Variance parameterization\n, alternatively,\nFigure 4.2: Model specification diagram normal model\n","code":"\nmodel_normal <- '\ndata {\n  int  N;\n  real x[N];\n  real mu0;\n  real sigma0;\n  real alpha0;\n  real beta0;\n}\n\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n\nmodel {\n  x ~ normal(mu, sigma);\n  mu ~ normal(mu0, sigma0);\n  sigma ~ inv_gamma(alpha0, beta0);\n}\n\n\n'\n# data must be in a list\nmydata <- list(\n  N = 10,\n  x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92),\n  mu0 = 75,\n  sigma0 = 50,\n  alpha0 = 5,\n  beta0 = 150\n)\n\n# start values \nstart_values <- function(){\n  list(mu=50, sigma=5)\n}\n\n# Next, need to fit the model\n#   I have explicited outlined some common parameters\nfit <- stan(\n  model_code = model_normal, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 2,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##         mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n## mu     84.04    0.05 4.69  74.66  81.10  84.02  86.97  93.53  8115    1\n## sigma  14.79    0.05 3.90   9.19  12.03  14.14  16.80  24.39  7202    1\n## lp__  -52.86    0.01 1.05 -55.75 -53.27 -52.54 -52.11 -51.83  5704    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:29:35 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit)## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit, pars = c(\"mu\", \"sigma\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\nggs_grb(ggs(fit)) + \n   theme_bw() + theme(panel.grid = element_blank())\n# autocorrelation\nggs_autocorrelation(ggs(fit)) + \n   theme_bw() + theme(panel.grid = element_blank())\n# plot the posterior density\nposterior <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  posterior,\n  pars = c(\"mu\", \"sigma\"),\n  prob = 0.8) + \n  plot_title\n# bivariate plot\nposterior <- as.data.frame(posterior)\np <- ggplot(posterior, aes(x=mu, y=sigma))+\n  geom_point()+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np\n# I prefer a posterior plot that includes prior and MLE\nMLE <- c(mean(mydata$x), sd(mydata$x))\nprior_mu <- function(x){dnorm(x, 75, 50)}\nx.mu <- seq(60.01, 120, 0.01)\nprior.mu <- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu))\nprior_sig <- function(x){extraDistr::dinvgamma(x, 5, 150)}\nx.sig <- seq(0.01, 60, 0.01)\nprior.sig <- data.frame(sigma=x.sig, dens.sig = prior_sig(x.sig))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=posterior,\n               aes(x=mu, color=\"Posterior\"))+\n  geom_line(data=prior.mu,\n            aes(x=x.mu, y=dens.mu, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=posterior,\n               aes(x=sigma, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sigma, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + plot_layout(guides=\"collect\")"},{"path":"normal-distribution-models.html","id":"jags-model-for-mean-and-variance-unknown-precision-parameterization","chapter":"4 Normal Distribution Models","heading":"4.2 JAGS Model for mean and variance unknown (precision parameterization)","text":"model mean variance unknown normal sampling.\nFigure 4.3: DAG mean variance unknown: Precision parameterization\n, alternatively,\nFigure 4.4: Model specification diagram normal model precision parameterization\nNow computation using JAGS","code":"\n# model code\njags.model <- function(){\n  #############################################\n  # Conditional distribution for the data\n  #############################################\n  \n  for(i in 1:n){\n    x[i] ~ dnorm(mu, tau)        # conditional distribution of the data\n  } # closes loop over subjects\n  \n  \n  #############################################\n  # Define the prior distributions for the unknown parameters\n  # The mean of the data (mu)\n  # The variance (sigma.squared) and precision (tau) of the data\n  #############################################\n  \n  mu ~ dnorm(mu.mu, tau.mu)      # prior distribution for mu\n  \n  mu.mu <- 75                          # mean of the prior for mu \n  sigma.squared.mu <- 50             # variance of the prior for mu\n  tau.mu <- 1/sigma.squared.mu   # precision of the prior for mu\n  \n  \n  tau ~ dgamma(alpha, beta)        # precision of the data\n  sigma.squared <- 1/tau             # variance of the data\n  sigma <- pow(sigma.squared, 0.5) # taking square root\n  \n  nu.0 <- 10                           # hyperparameter for prior for tau\n  sigma.squared.0 <- 30            # hyperparameter for prior for tau\n  \n  alpha <- nu.0/2                    # hyperparameter for prior for tau\n  beta <- nu.0*sigma.squared.0/2 # hyperparameter for prior for tau\n    \n}\n# data\nmydata <- list(\n  n=10, \n  x=c(91, 85, 72, 87, 71, 77, 88, 94, 84, 92))\n\n\n# starting values\nstart_values <- function(){\n  list(\"mu\"=75, \"tau\"=0.1)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"mu\", \"tau\", \"sigma\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=4000,\n  n.burnin = 1000,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 10\n##    Unobserved stochastic nodes: 2\n##    Total graph size: 26\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model4544158a4038.txt\", fit using jags,\n##  4 chains, each with 4000 iterations (first 1000 discarded)\n##  n.sims = 12000 iterations saved\n##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\n## mu        83.268   2.204 78.845 81.869 83.282 84.725 87.471 1.001 12000\n## sigma      7.176   1.238  5.251  6.288  7.007  7.903 10.050 1.001  4000\n## tau        0.021   0.007  0.010  0.016  0.020  0.025  0.036 1.001  4000\n## deviance  71.212   1.835 69.393 69.897 70.659 71.952 76.128 1.001 12000\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 1.7 and DIC = 72.9\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"mu\"),\n  prob = 0.8) + \n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"tau\"),\n  prob = 0.8) + \n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"sigma\"),\n  prob = 0.8) + \n  plot_title\n# bivariate plot\np <- ggplot(plot.data, aes(x=mu, y=tau))+\n  geom_point()+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np\n# I prefer a posterior plot that includes prior and MLE\nMLE <- c(mean(mydata$x), 1/var(mydata$x))\nprior_mu <- function(x){dnorm(x, 75, 50)}\nx.mu <- seq(70.01, 100, 0.01)\nprior.mu <- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu))\nprior_tau <- function(x){dgamma(x, 5, 150)}\nx.tau <- seq(0.0001, 0.06, 0.0001)\nprior.tau <- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=mu, color=\"Posterior\"))+\n  geom_line(data=prior.mu,\n            aes(x=x.mu, y=dens.mu, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=tau, color=\"Posterior\"))+\n  geom_line(data=prior.tau,\n            aes(x=tau, y=dens.tau, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + plot_layout(guides=\"collect\")"},{"path":"markov-chain-monte-carlo-estimation.html","id":"markov-chain-monte-carlo-estimation","chapter":"5 Markov Chain Monte Carlo Estimation","heading":"5 Markov Chain Monte Carlo Estimation","text":"chapter MCMC methods gives introduction common basic sampling approaches Bayesian methods.\nmethods inGibbs SamplingGibbs SamplingMetropolis SamplingMetropolis SamplingMetropolis-HastingsMetropolis-Hastingsand notes approaches related.\nimportant take away section practical issues MCMC methods.\npractical aspects estimation noted :Assessing convergence - making sure enough iterations used including potential scale reduction factor (\\(\\hat{R}\\)),Assessing convergence - making sure enough iterations used including potential scale reduction factor (\\(\\hat{R}\\)),Serial dependence - samples drawn posterior autocorrelated. means within chain draws dependent enough draws thinning samples sufficiently independent,Serial dependence - samples drawn posterior autocorrelated. means within chain draws dependent enough draws thinning samples sufficiently independent,Mixing - different chains search/sample parameter space different chains can sometimes get “stuck” sampling one part parameter space chains.Mixing - different chains search/sample parameter space different chains can sometimes get “stuck” sampling one part parameter space chains.Lastly, major take away chapter MCMC methods help approximate posterior distribution.\ndistribution solution full Bayesian analysis point estimate.","code":""},{"path":"regression.html","id":"regression","chapter":"6 Regression","heading":"6 Regression","text":"regression models, built DAG look like.\nrepresentations shown .\nFigure 6.1: DAG simple regression model 1 predictor\n\nFigure 6.2: DAG regression \\(J\\) predictors\n\nFigure 6.3: Expanded DAG representation regression hyperparameters included\nNext, gave general representation model specification diagram constructed.\nFigure 6.4: Model specification diagram linear regression model\n","code":""},{"path":"regression.html","id":"stan-model-for-regression-model","chapter":"6 Regression","heading":"6.1 Stan Model for Regression Model","text":"","code":"\nmodel_reg <- '\ndata {\n  int  N;\n  real x1[N];\n  real x2[N];\n  real y[N];\n}\n\nparameters {\n  real beta[3];\n  real<lower=0> tau;\n}\n\ntransformed parameters {\n  real<lower=0> sigma;\n  sigma = 1/sqrt(tau);\n}\n\nmodel {\n  for(i in 1:N){\n    y[i] ~ normal(beta[1] + beta[2]*x1[i] + beta[3]*x2[i], sigma);\n  }\n  beta  ~ normal(0, 100);\n  tau ~ gamma(1, 1);\n}\n\ngenerated quantities {\n  real varerror;\n  real vary;\n  real Rsquared;\n  real error[N];\n  \n  for(i in 1:N){\n    error[i] = y[i] - (beta[1] + beta[2]*x1[i] + beta[3]*x2[i]);\n  }\n  \n  varerror = variance(error);\n  vary = variance(y);\n  Rsquared = 1 - (varerror/vary);\n}\n\n'\n# data must be in a list\ndat <- read.table(\"data/Chp4_Reg_Chapter_Tests.dat\", header=T)\nmydata <- list(\n  N=nrow(dat), \n  x1=dat$Ch1Test,\n  x2=dat$Ch2Test,\n  y =dat$Ch3Test\n)\n\n\n# start values \nstart_values <- function(){\n  list(sigma=1, beta=c(0,0,0))\n}\n\n# Next, need to fit the model\n#   I have explicited outlined some common parameters\nfit <- stan(\n  model_code = model_reg, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 4,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##             mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n## beta[1]    -2.51    0.02 1.94  -6.29  -3.81  -2.52  -1.23   1.34  7240    1\n## beta[2]     0.66    0.00 0.17   0.33   0.55   0.66   0.77   0.98  6009    1\n## beta[3]     0.38    0.00 0.10   0.18   0.31   0.38   0.45   0.59  7692    1\n## tau         0.28    0.00 0.06   0.18   0.24   0.28   0.32   0.40  9514    1\n## sigma       1.91    0.00 0.20   1.57   1.77   1.90   2.03   2.34  9315    1\n## varerror    3.65    0.00 0.16   3.51   3.55   3.60   3.71   4.08  5093    1\n## vary        8.79    0.00 0.00   8.79   8.79   8.79   8.79   8.79     2    1\n## Rsquared    0.58    0.00 0.02   0.54   0.58   0.59   0.60   0.60  5093    1\n## error[1]   -0.26    0.02 1.41  -3.06  -1.19  -0.25   0.68   2.51  7462    1\n## error[2]    1.71    0.01 0.51   0.71   1.37   1.71   2.04   2.70  7777    1\n## error[3]   -1.06    0.01 0.57  -2.17  -1.44  -1.05  -0.67   0.05  7145    1\n## error[4]   -3.20    0.01 0.76  -4.69  -3.71  -3.19  -2.68  -1.72  6726    1\n## error[5]   -2.81    0.01 0.53  -3.86  -3.15  -2.80  -2.45  -1.77  9813    1\n## error[6]    0.43    0.00 0.42  -0.41   0.16   0.43   0.71   1.27  9775    1\n## error[7]   -1.95    0.00 0.40  -2.76  -2.21  -1.95  -1.69  -1.16  9278    1\n## error[8]   -6.23    0.00 0.39  -7.02  -6.49  -6.22  -5.96  -5.46 10946    1\n## error[9]    3.39    0.00 0.34   2.71   3.17   3.39   3.62   4.05 11804    1\n## error[10]   4.01    0.00 0.32   3.38   3.80   4.01   4.22   4.62 11853    1\n## error[11]  -0.75    0.00 0.36  -1.46  -0.99  -0.75  -0.51  -0.07  9716    1\n## error[12]   0.48    0.01 0.49  -0.48   0.15   0.49   0.81   1.43  8412    1\n## error[13]   2.48    0.01 0.49   1.52   2.15   2.49   2.81   3.43  8412    1\n## error[14]  -0.74    0.01 0.70  -2.10  -1.20  -0.73  -0.28   0.63  8370    1\n## error[15]   0.35    0.00 0.30  -0.24   0.15   0.35   0.56   0.94 13762    1\n## error[16]   0.35    0.00 0.30  -0.24   0.15   0.35   0.56   0.94 13762    1\n## error[17]   0.97    0.00 0.28   0.43   0.79   0.97   1.16   1.51 16267    1\n## error[18]   1.97    0.00 0.28   1.43   1.79   1.97   2.16   2.51 16267    1\n## error[19]  -0.41    0.00 0.28  -0.97  -0.60  -0.41  -0.22   0.15 15208    1\n## error[20]   1.59    0.00 0.28   1.03   1.40   1.59   1.78   2.15 15208    1\n## error[21]   1.21    0.00 0.33   0.57   0.99   1.21   1.43   1.84 12622    1\n## error[22]   2.21    0.00 0.33   1.57   1.99   2.21   2.43   2.84 12622    1\n## error[23]   0.83    0.00 0.39   0.05   0.57   0.83   1.09   1.58 10727    1\n## error[24]   0.83    0.00 0.39   0.05   0.57   0.83   1.09   1.58 10727    1\n## error[25]   2.99    0.01 0.89   1.25   2.40   2.99   3.58   4.72  7647    1\n## error[26]  -1.40    0.01 0.79  -2.94  -1.92  -1.40  -0.87   0.15  7707    1\n## error[27]  -0.92    0.00 0.44  -1.79  -1.22  -0.92  -0.62  -0.07  8831    1\n## error[28]   0.08    0.00 0.44  -0.79  -0.22   0.08   0.38   0.93  8831    1\n## error[29]   2.08    0.00 0.44   1.21   1.78   2.08   2.38   2.93  8831    1\n## error[30]  -1.69    0.00 0.32  -2.32  -1.90  -1.68  -1.47  -1.05 12268    1\n## error[31]  -0.69    0.00 0.32  -1.32  -0.90  -0.68  -0.47  -0.05 12268    1\n## error[32]  -0.07    0.00 0.30  -0.65  -0.27  -0.06   0.13   0.52 14903    1\n## error[33]   0.93    0.00 0.30   0.35   0.73   0.94   1.13   1.52 14903    1\n## error[34]   1.93    0.00 0.30   1.35   1.73   1.94   2.13   2.52 14903    1\n## error[35]  -1.45    0.00 0.31  -2.06  -1.66  -1.45  -1.24  -0.85 15317    1\n## error[36]  -0.45    0.00 0.31  -1.06  -0.66  -0.45  -0.24   0.15 15317    1\n## error[37]   1.55    0.00 0.31   0.94   1.34   1.55   1.76   2.15 15317    1\n## error[38]   0.17    0.00 0.35  -0.52  -0.06   0.17   0.41   0.86 13697    1\n## error[39]   1.17    0.00 0.35   0.48   0.94   1.17   1.41   1.86 13697    1\n## error[40]  -0.21    0.00 0.42  -1.03  -0.48  -0.21   0.07   0.60 12041    1\n## error[41]  -1.34    0.00 0.43  -2.19  -1.63  -1.35  -1.05  -0.49  8855    1\n## error[42]   0.90    0.00 0.37   0.16   0.64   0.90   1.15   1.63 11279    1\n## error[43]  -3.49    0.00 0.39  -4.24  -3.75  -3.49  -3.23  -2.74 12187    1\n## error[44]  -2.49    0.00 0.39  -3.24  -2.75  -2.49  -2.23  -1.74 12187    1\n## error[45]  -1.87    0.00 0.42  -2.69  -2.15  -1.87  -1.59  -1.05 12241    1\n## error[46]  -0.87    0.00 0.42  -1.69  -1.15  -0.87  -0.59  -0.05 12241    1\n## error[47]  -0.87    0.00 0.42  -1.69  -1.15  -0.87  -0.59  -0.05 12241    1\n## error[48]   0.13    0.00 0.42  -0.69  -0.15   0.13   0.41   0.95 12241    1\n## error[49]   0.13    0.00 0.42  -0.69  -0.15   0.13   0.41   0.95 12241    1\n## error[50]   0.13    0.00 0.42  -0.69  -0.15   0.13   0.41   0.95 12241    1\n## lp__      -59.44    0.02 1.47 -63.16 -60.15 -59.09 -58.37 -57.62  5819    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:31:00 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit)## 'pars' not specified. Showing first 10 parameters by default.## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\np1 <- ggs_grb(ggs(fit, family = \"beta\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np2 <- ggs_grb(ggs(fit, family = \"sigma\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1 + p2\n# autocorrelation\np1 <- ggs_autocorrelation(ggs(fit, family=\"beta\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np2 <- ggs_autocorrelation(ggs(fit, family=\"sigma\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1 + p2\n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"sigma\"),\n  prob = 0.8) + \n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"Rsquared\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\n# Expanded Posterior Plot\nfit.lm <- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat))\nMLE <- c(fit.lm$coefficients[,1], fit.lm$sigma**2, fit.lm$r.squared)\nprior_beta <- function(x){dnorm(x, 0, 1000)}\nx.beta <- seq(-10, 4.99, 0.01)\nprior.beta <- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta))\nprior_sig <- function(x){dgamma(x, 1, 1)}\nx.sig <- seq(0.01, 2.5, 0.01)\nprior.sig <- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nplot.data <- as.data.frame(plot.data)\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`beta[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`beta[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0, 1))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`beta[3]`, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[3], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0, 1))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np4 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=sigma, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np5 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=Rsquared, color=\"Posterior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0, 1))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + p4 + p5 + plot_layout(guides=\"collect\")## Warning: Removed 307 rows containing non-finite values (stat_density).## Warning: Removed 1399 row(s) containing missing values (geom_path).## Warning: Removed 1 rows containing non-finite values (stat_density).## Warning: Removed 1399 row(s) containing missing values (geom_path)."},{"path":"regression.html","id":"jags-model-for-regression-model","chapter":"6 Regression","heading":"6.2 JAGS Model for Regression Model","text":"","code":"\n# model code\njags.model <- function(){\n  ############################################\n  # Prior distributions\n  ############################################\n  beta.0 ~ dnorm(0, .001)   # prior for the intercept\n  beta.1 ~ dnorm(0, .001)   # prior for coefficient 1\n  beta.2 ~ dnorm(0, .001)   # prior for coefficient 2\n  tau.e  ~ dgamma(1, 1)     # prior for the error precision\n  sigma.e <- 1/sqrt(tau.e)  # standard deviation of the errors\n  \n  \n  ############################################\n  # Conditional distribution of the data\n  # Via a regression model\n  ############################################\n  for(i in 1:n){\n    y.prime[i] <- beta.0 + beta.1*x1[i] + beta.2*x2[i]      \n    y[i] ~ dnorm(y.prime[i], tau.e)         \n  }\n    \n  \n  ############################################\n  # Calculate R-squared\n  ############################################\n  for(i in 1:n){\n    error[i] <- y[i] - y.prime[i] \n  }\n  \n  var.error <- sd(error[])*sd(error[])\n  var.y <- sd(y[])*sd(y[])\n  \n  R.squared <- 1 - (var.error/var.y)\n    \n}\n# data\ndat <- read.table(\"data/Chp4_Reg_Chapter_Tests.dat\", header=T)\nmydata <- list(\n  n=nrow(dat), \n  x1=dat$Ch1Test,\n  x2=dat$Ch2Test,\n  y =dat$Ch3Test\n)\n\n\n# starting values\nstart_values <- function(){\n  list(\"tau.e\"=0.01, 'beta.0'=0, \"beta.1\"=0, \"beta.2\"=0)\n}\n\n# vector of all parameters to save\nparam_save <- c(\"tau.e\", \"beta.0\", \"beta.1\", \"beta.2\", \"R.squared\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=4000,\n  n.burnin = 1000,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 50\n##    Unobserved stochastic nodes: 4\n##    Total graph size: 262\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model45445b7f5ecf.txt\", fit using jags,\n##  4 chains, each with 4000 iterations (first 1000 discarded)\n##  n.sims = 12000 iterations saved\n##           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\n## R.squared   0.584   0.018   0.536   0.578   0.590   0.597   0.601 1.003 12000\n## beta.0     -2.540   1.926  -6.328  -3.825  -2.548  -1.239   1.257 1.001 12000\n## beta.1      0.657   0.166   0.331   0.546   0.658   0.767   0.985 1.001  8600\n## beta.2      0.383   0.103   0.182   0.315   0.384   0.452   0.583 1.001  8800\n## tau.e       0.282   0.057   0.182   0.242   0.278   0.318   0.403 1.001 12000\n## deviance  207.594   2.940 204.053 205.535 206.932 208.959 214.755 1.001 12000\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 4.3 and DIC = 211.9\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"beta.0\", \"beta.1\", \"beta.2\", \"tau.e\"),\n  prob = 0.8) + \n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"R.squared\"),\n  prob = 0.8) + \n  plot_title\n# Expanded Posterior Plot\nfit.lm <- summary(lm(Ch3Test ~ 1 + Ch1Test + Ch2Test, data=dat))\nMLE <- c(fit.lm$coefficients[,1], 1/fit.lm$sigma**2, fit.lm$r.squared)\nprior_beta <- function(x){dnorm(x, 0, 1000)}\nx.beta <- seq(-5, 4.99, 0.01)\nprior.beta <- data.frame(beta=x.beta, dens.beta = prior_beta(x.beta))\nprior_tau <- function(x){dgamma(x, 1, 1)}\nx.tau <- seq(0.01, 0.50, 0.01)\nprior.tau <- data.frame(tau=x.tau, dens.tau = prior_tau(x.tau))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=beta.0, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=beta.1, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0, 1))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=beta.2, color=\"Posterior\"))+\n  geom_line(data=prior.beta,\n            aes(x=beta, y=dens.beta, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[3], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0, 1))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np4 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=tau.e, color=\"Posterior\"))+\n  geom_line(data=prior.tau,\n            aes(x=tau, y=dens.tau, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np5 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=R.squared, color=\"Posterior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.5, 0.65))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + p4 + p5 + plot_layout(guides=\"collect\")## Warning: Removed 243 rows containing non-finite values (stat_density).## Warning: Removed 899 row(s) containing missing values (geom_path).## Warning: Removed 4 rows containing non-finite values (stat_density).## Warning: Removed 899 row(s) containing missing values (geom_path).## Warning: Removed 53 rows containing non-finite values (stat_density)."},{"path":"canonical-bayesian-psychometric-modeling.html","id":"canonical-bayesian-psychometric-modeling","chapter":"7 Canonical Bayesian Psychometric Modeling","heading":"7 Canonical Bayesian Psychometric Modeling","text":"chapter provides overview purposes psychometric modeling serve Bayesian approach can fit purpose.\ninformation introduced directed acyclic graph (DAG) representations basic models notation future chapters.canonical (conventional) psychometric analysis focuses scoring calibration.Scoring refers arriving representation examinee based performance assessment, andCalibration refers arriving representation measurement model parameters (possibly hyper-parameters).research operational setting, two core components can focus separately together.\nfocus simultaneous assessment measurement model parameters person parameters, may need concession made conduct .\none sense, like estimate parameters simultaneous uncertainty measurement model parameters reflected person parameter (ability) estimates.\nHowever, traditional psychometric analysis tends proceed 1) estimating measurement model parameters first integrating person parameter distribution likelihood function, 2) using now estimated measurement model parameters fixed quantities, estimate person parameters (factor scores).\nprocess results uncertainty person parameters decreased.\nOne potential benefit Bayesian approach calibration scoring can done simultaneously. may always interest current application traditional approach may still done.canonical psychometric model can expressed DAG using model similar path models traditionally used SEM.\nexpressed two forms model shown book .\nFigure 7.1: DAG canonical psychometric modeling\n\nFigure 7.2: Expanded DAG include measurement model parameters\n","code":""},{"path":"classical-test-theory.html","id":"classical-test-theory","chapter":"8 Classical Test Theory","heading":"8 Classical Test Theory","text":"traditional model specification CTT \n\\[X = T + E,\\]\n\\(X\\) observed test/measure score, \\(T\\) truce score wish make inferences , \\(E\\) error.\ntrue scores population mean \\(\\mu_T\\) variance \\(\\sigma^2_T\\).\nerrors individual expected 0 average, \\(\\mathbb{E}(E_i)=0\\) variance \\(\\sigma^2_E\\).\nerrors uncorrelated true score population, \n\\[\\mathbb{COV}(T, E) = \\sigma_{TE} = \\rho_{TE}\\sigma_{T}\\sigma_E = 0.\\]implications associated CTT model :population mean observed scores true scores\n\\[\\mu_x = \\mu_T.\\]population mean observed scores true scores\n\\[\\mu_x = \\mu_T.\\]observed score variance can decomposed \n\\[\\begin{align*}\n\\sigma^2_X &= \\sigma^2_T + \\sigma^2_E + 2\\sigma_{TE}\\\\\n&= \\sigma^2_T + \\sigma^2_E.\n\\end{align*}\\]observed score variance can decomposed \n\\[\\begin{align*}\n\\sigma^2_X &= \\sigma^2_T + \\sigma^2_E + 2\\sigma_{TE}\\\\\n&= \\sigma^2_T + \\sigma^2_E.\n\\end{align*}\\]can define reliability terms ratio true score variance observed score variance, \n\\[\\rho = \\frac{\\sigma^2_T}{\\sigma^2_X}  = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E}.\\]can define reliability terms ratio true score variance observed score variance, \n\\[\\rho = \\frac{\\sigma^2_T}{\\sigma^2_X}  = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E}.\\]interesting approach deriving estimates true scores flip traditional CTT model around define true score function observed score.\nuses Kelley’s formula (Kelley 1923),\n\\[\\begin{align*}\n\\hat{T}_i &= \\rho x_i + (1-\\rho)\\mu_x\\\\\n&= \\mu_x + \\rho (x_i - \\mu_x),\n\\end{align*}\\]\n\\(\\mu_x\\) mean observed scores \\(\\hat{T}_i\\) estimated true score individual \\(\\).\ninteresting formula since ’s notion incorporate uncertainty estimation true score.\nhigher uncertainty (lower reliability) less weight observed score rely population mean estimate.Bayesian feel , ’s nearly identical derive posterior mean conjugate normal model (see p.158).","code":""},{"path":"classical-test-theory.html","id":"example-1---known-measurement-model-parameters-with-1-measure","chapter":"8 Classical Test Theory","heading":"8.1 Example 1 - Known measurement model parameters with 1 measure","text":", discuss simple CTT example assume measurement model parameters known.\nmeans assume value \\(\\mu_t\\), \\(\\sigma^2_T\\), \\(\\sigma^2_E\\).\nnearly always need estimate quantities provide informed decision parameters .example using 3 observations (individuals) 1 measure per individual.\nDAG model shown .\nFigure 8.1: Simple CTT model 1 measure known measurement parameters\nsimply model specification using normal distributions underly probability functions.\nFigure 8.2: Model specification diagram known parameters CTT model\n","code":""},{"path":"classical-test-theory.html","id":"example-1---stan","chapter":"8 Classical Test Theory","heading":"8.2 Example 1 - Stan","text":"","code":"\nmodel_ctt1 <- '\ndata {\n  int  N;\n  real x[N];\n  real muT;\n  real sigmaT;\n  real sigmaE;\n}\n\nparameters {\n  real T[N];\n}\n\nmodel {\n  for(i in 1:N){\n    x[i] ~ normal(T[i], sigmaE);\n    T[i] ~ normal(muT, sigmaT);\n  }\n}\n\n'\n# data must be in a list\nmydata <- list(\n  N=3, \n  x=c(70, 80, 96),\n  muT = 80,\n  sigmaT = 6, #sqrt(36)\n  sigmaE = 4 # sqrt(16)\n)\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_ctt1, # model code to be compiled\n  data = mydata,          # my data\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 4,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##       mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## T[1] 73.13    0.02 3.31 66.66 70.88 73.12 75.36 79.66 18495    1\n## T[2] 79.94    0.02 3.33 73.45 77.70 79.94 82.17 86.48 18747    1\n## T[3] 91.10    0.03 3.33 84.61 88.81 91.10 93.36 97.74 15136    1\n## lp__ -4.92    0.01 1.23 -8.14 -5.46 -4.61 -4.03 -3.53  8272    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:32:47 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit)## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit, pars = c(\"T\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\np1 <- ggs_grb(ggs(fit, family = \"T\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1\n# autocorrelation\np1 <- ggs_autocorrelation(ggs(fit, family=\"T\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1 \n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"T[1]\",\"T[2]\",\"T[3]\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\n# Expanded Posterior Plot\nMLE <- mydata$x\nprior_t <- function(x){dnorm(x, 80, 6)}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=x.t, dens.t = prior_t(x.t))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nplot.data <- as.data.frame(plot.data)\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[3]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[3], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + plot_layout(guides=\"collect\")"},{"path":"classical-test-theory.html","id":"example-1---jags","chapter":"8 Classical Test Theory","heading":"8.3 Example 1 - JAGS","text":"","code":"\n# model code\njags.model.ctt1 <- function(){\n   ############################################\n     # CLASSICAL TEST THEORY MODEL\n     # WITH KNOWN HYPERPARAMETERS\n     #    TRUE SCORE MEAN, TRUE SCORE VARIANCE\n     #    ERROR VARIANCE\n     ############################################\n\n     ############################################\n     # KNOWN HYPERPARAMETERS\n     ############################################\n     mu.T <- 80               # Mean of the true scores\n     sigma.squared.T <- 36    # Variance of the true scores\n     sigma.squared.E <- 16    # Variance of the errors\n\n     tau.T <- 1/sigma.squared.T   # Precision of the true scores\n     tau.E <- 1/sigma.squared.E   # Precision of the errors\n\n \n     ############################################\n     # MODEL FOR TRUE SCORES AND OBSERVABLES\n     ############################################\n \n     for (i in 1:n) {\n          T[i] ~ dnorm(mu.T, tau.T)     # Distribution of true scores\n          x[i] ~ dnorm(T[i], tau.E)     # Distribution of observables\n     }\n    \n}\n# data\nmydata <- list(\n  n=3, \n  x=c(70, 80, 96)\n)\n\n# starting values\nstart_values <- function(){\n  list(\"T\"=c(80,80,80))\n}\n\n# vector of all parameters to save\nparam_save <- c(\"T\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.ctt1,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=4000,\n  n.burnin = 1000,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 3\n##    Unobserved stochastic nodes: 3\n##    Total graph size: 13\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model4544520665bc.txt\", fit using jags,\n##  4 chains, each with 4000 iterations (first 1000 discarded)\n##  n.sims = 12000 iterations saved\n##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\n## T[1]      73.086   3.292 66.649 70.841 73.088 75.308 79.526 1.001  7300\n## T[2]      80.064   3.272 73.653 77.859 80.065 82.292 86.524 1.001  5500\n## T[3]      91.045   3.341 84.470 88.763 91.061 93.306 97.453 1.001 12000\n## deviance  18.005   2.932 14.208 15.784 17.399 19.551 25.289 1.001 12000\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 4.3 and DIC = 22.3\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"T[1]\", \"T[2]\", \"T[3]\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\nMLE <- mydata$x\nprior_t <- function(x){dnorm(x, 80, 6)}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=x.t, dens.t = prior_t(x.t))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[3]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[3], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + plot_layout(guides=\"collect\")"},{"path":"classical-test-theory.html","id":"example-2---known-measurement-model-with-multiple-measures","chapter":"8 Classical Test Theory","heading":"8.4 Example 2 - Known Measurement Model with Multiple Measures","text":", thing changes first example now multiple observations per individual.\ncan think administer test (parallel tests) repeatedly without learning occuring.\nDAG model-specification change :\nFigure 8.3: Simple CTT model known measurement parameters multiple measures\n\nFigure 8.4: Model specification diagram known parameters CTT model multiple measures\n","code":""},{"path":"classical-test-theory.html","id":"example-2---stan","chapter":"8 Classical Test Theory","heading":"8.5 Example 2 - Stan","text":"","code":"\nmodel_ctt2 <- '\ndata {\n  int  N;\n  int  J;\n  matrix[N, J] X;\n  real muT;\n  real sigmaT;\n  real sigmaE;\n}\n\nparameters {\n  real T[N];\n}\n\nmodel {\n  for(i in 1:N){\n    T[i] ~ normal(muT, sigmaT);\n    for(j in 1:J){\n      X[i, j] ~ normal(T[i], sigmaE);\n    }\n  }\n}\n\n'\n# data must be in a list\nmydata <- list(\n  N = 10, J = 5, \n  X = matrix(\n    c(80, 77, 80, 73, 73,\n      83, 79, 78, 78, 77,\n      85, 77, 88, 81, 80,\n      76, 76, 76, 78, 67,\n      70, 69, 73, 71, 77,\n      87, 89, 92, 91, 87,\n      76, 75, 79, 80, 75,\n      86, 75, 80, 80, 82,\n      84, 79, 79, 77, 82,\n      96, 85, 91, 87, 90),\n    ncol=5, nrow=10, byrow=T),\n  muT = 80,\n  sigmaT = 6, #sqrt(36)\n  sigmaE = 4 # sqrt(16)\n)\n\n# initial values\nstart_values <- function(){\n  list(T=c(80,80,80,80,80,80,80,80,80,80))\n}\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_ctt2, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 4,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##         mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n## T[1]   76.88    0.01 1.72  73.47  75.72  76.90  78.03  80.21 31735    1\n## T[2]   79.08    0.01 1.73  75.69  77.93  79.06  80.22  82.47 29246    1\n## T[3]   82.01    0.01 1.72  78.65  80.84  82.01  83.20  85.40 29176    1\n## T[4]   75.05    0.01 1.71  71.72  73.89  75.06  76.22  78.41 28913    1\n## T[5]   72.66    0.01 1.71  69.29  71.52  72.65  73.82  76.04 29610    1\n## T[6]   88.44    0.01 1.70  85.12  87.32  88.44  89.57  91.76 28373    1\n## T[7]   77.24    0.01 1.70  73.91  76.08  77.24  78.38  80.57 30229    1\n## T[8]   80.56    0.01 1.69  77.25  79.43  80.56  81.72  83.84 28684    1\n## T[9]   80.19    0.01 1.70  76.86  79.04  80.19  81.33  83.53 26627    1\n## T[10]  89.00    0.01 1.74  85.60  87.83  88.99  90.18  92.39 29996    1\n## lp__  -23.47    0.03 2.22 -28.78 -24.72 -23.16 -21.87 -20.10  7486    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:33:57 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit)## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit, pars = c(\"T\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\np1 <- ggs_grb(ggs(fit, family = \"T\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1\n# autocorrelation\np1 <- ggs_autocorrelation(ggs(fit, family=\"T\")) + \n   theme_bw() + theme(panel.grid = element_blank())\np1 \n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = paste0(\"T[\",1:10,\"]\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\n# Expanded Posterior Plot\nMLE <- rowMeans(mydata$X)\nprior_t <- function(x){dnorm(x, 80, 6)}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=x.t, dens.t = prior_t(x.t))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nplot.data <- as.data.frame(plot.data)\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[5]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np4 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[10]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[10], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\n\np1 + p2 + p3 + p4 + plot_layout(guides=\"collect\")"},{"path":"classical-test-theory.html","id":"example-2---jags","chapter":"8 Classical Test Theory","heading":"8.6 Example 2 - JAGS","text":"","code":"\n# model code\njags.model.ctt2 <- function(){\n   ############################################\n     # CLASSICAL TEST THEORY\n     # WITH KNOWN \n     #    TRUE SCORE MEAN, TRUE SCORE VARIANCE\n     #    ERROR VARIANCE\n     ############################################\n\n     ############################################\n     # KNOWN HYPERPARAMETERS\n     ############################################\n     mu.T <- 80               # Mean of the true scores\n     sigma.squared.T <- 36    # Variance of the true scores\n     sigma.squared.E <- 16    # Variance of the errors\n\n     tau.T <- 1/sigma.squared.T   # Precision of the true scores\n     tau.E <- 1/sigma.squared.E   # Precision of the errors\n\n \n     ############################################\n     # MODEL FOR TRUE SCORES AND OBSERVABLES\n     ############################################\n \n     for (i in 1:N) {\n        T[i] ~ dnorm(mu.T, tau.T)     # Distribution of true scores\n        for(j in 1:J){\n          x[i, j] ~ dnorm(T[i], tau.E)     # Distribution of observables \n        }\n     }\n    \n}\n# data\nmydata <- list(\n  N = 10, J = 5, \n  x = matrix(\n    c(80, 77, 80, 73, 73,\n      83, 79, 78, 78, 77,\n      85, 77, 88, 81, 80,\n      76, 76, 76, 78, 67,\n      70, 69, 73, 71, 77,\n      87, 89, 92, 91, 87,\n      76, 75, 79, 80, 75,\n      86, 75, 80, 80, 82,\n      84, 79, 79, 77, 82,\n      96, 85, 91, 87, 90),\n    ncol=5, nrow=10, byrow=T)\n)\n\n# starting values\nstart_values <- function(){\n  list(\"T\"=rep(80,10))\n}\n\n# vector of all parameters to save\nparam_save <- c(\"T\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.ctt2,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=4000,\n  n.burnin = 1000,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 50\n##    Unobserved stochastic nodes: 10\n##    Total graph size: 68\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model4544135343d3.txt\", fit using jags,\n##  4 chains, each with 4000 iterations (first 1000 discarded)\n##  n.sims = 12000 iterations saved\n##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\n## T[1]      76.858   1.693  73.521  75.712  76.845  77.982  80.164 1.001  4200\n## T[2]      79.090   1.729  75.747  77.933  79.089  80.257  82.522 1.001 12000\n## T[3]      82.028   1.700  78.704  80.860  82.031  83.173  85.338 1.001 12000\n## T[4]      75.056   1.725  71.642  73.891  75.053  76.205  78.492 1.001  9800\n## T[5]      72.654   1.695  69.363  71.489  72.633  73.808  75.984 1.001 12000\n## T[6]      88.455   1.710  85.055  87.304  88.445  89.603  91.821 1.002  3100\n## T[7]      77.261   1.702  73.905  76.145  77.252  78.399  80.587 1.001 12000\n## T[8]      80.560   1.720  77.240  79.401  80.547  81.720  83.941 1.001 12000\n## T[9]      80.211   1.707  76.886  79.050  80.219  81.373  83.565 1.001  3900\n## T[10]     88.983   1.707  85.609  87.848  88.993  90.136  92.290 1.001 12000\n## deviance 269.583   4.346 263.043 266.415 268.950 271.992 279.779 1.001  6800\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 9.4 and DIC = 279.0\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"T[1]\", \"T[2]\", \"T[3]\"),\n  prob = 0.8) + \n  plot_title\n# I prefer a posterior plot that includes prior and MLE\nMLE <- rowMeans(mydata$X)## Error in rowMeans(mydata$X): 'x' must be an array of at least two dimensions\nprior_t <- function(x){dnorm(x, 80, 6)}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=x.t, dens.t = prior_t(x.t))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[5]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`T[10]`, color=\"Posterior\"))+\n  geom_line(data=prior.t,\n            aes(x=tr, y=dens.t, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[10], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + plot_layout(guides=\"collect\")"},{"path":"classical-test-theory.html","id":"example-3---unknown-measurement-model-with-multiple-measures","chapter":"8 Classical Test Theory","heading":"8.7 Example 3 - Unknown Measurement Model with Multiple Measures","text":", finally get () realistic case don’t much prior knowledge measurement model parameters (namely, variances).\nstructure relies hierarchically specifying priors induce conditional independence.\nDAG model-specification change :\nFigure 8.5: Simple CTT model unknown measurement parameters\n\nFigure 8.6: Model specification diagram unknown measurement model parameters\n","code":""},{"path":"classical-test-theory.html","id":"example-3---stan","chapter":"8 Classical Test Theory","heading":"8.8 Example 3 - Stan","text":"","code":"\nmodel_ctt3 <- '\ndata {\n  int  N;\n  int  J;\n  matrix[N, J] X;\n}\n\nparameters {\n  real T[N];\n  real muT;\n  real<lower=0> sigmaT;\n  real<lower=0> sigmaE;\n}\n\nmodel {\n  for(i in 1:N){\n    T[i] ~ normal(muT, sigmaT);\n    for(j in 1:J){\n      X[i, j] ~ normal(T[i], sigmaE);\n    }\n  }\n  muT ~ normal(80, 10);\n  sigmaT ~ inv_gamma(1, 6);\n  sigmaE ~ inv_gamma(1, 4);\n}\n\ngenerated quantities {\n  real rho;\n  real rhocomp;\n\n  rho = square(sigmaT)/(square(sigmaT) + square(sigmaE));\n  rhocomp = J*rho/((J-1)*rho + 1);\n}\n\n'\n# data must be in a list\nmydata <- list(\n  N = 10, J = 5,\n  X = matrix(\n    c(80, 77, 80, 73, 73,\n      83, 79, 78, 78, 77,\n      85, 77, 88, 81, 80,\n      76, 76, 76, 78, 67,\n      70, 69, 73, 71, 77,\n      87, 89, 92, 91, 87,\n      76, 75, 79, 80, 75,\n      86, 75, 80, 80, 82,\n      84, 79, 79, 77, 82,\n      96, 85, 91, 87, 90),\n    ncol=5, nrow=10, byrow=T)\n)\n\n# initial values\nstart_values <- function(){\n  list(T=c(80,80,80,80,80,80,80,80,80,80),\n       muT=80, sigmaT=10, sigmaE=5)\n}\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_ctt3, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 4,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 4,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\n## T[1]      76.86    0.01 1.52   73.90   75.86   76.86   77.86   79.92 24760    1\n## T[2]      79.08    0.01 1.52   76.10   78.07   79.08   80.08   82.08 25643    1\n## T[3]      82.03    0.01 1.52   79.02   81.01   82.03   83.05   84.97 24831    1\n## T[4]      75.02    0.01 1.56   71.98   73.97   75.01   76.08   78.14 23847    1\n## T[5]      72.62    0.01 1.56   69.57   71.59   72.60   73.66   75.74 21993    1\n## T[6]      88.50    0.01 1.58   85.34   87.48   88.51   89.54   91.57 21711    1\n## T[7]      77.23    0.01 1.53   74.22   76.22   77.23   78.26   80.24 22774    1\n## T[8]      80.56    0.01 1.52   77.56   79.54   80.55   81.57   83.55 28472    1\n## T[9]      80.19    0.01 1.52   77.19   79.19   80.20   81.20   83.17 24704    1\n## T[10]     89.05    0.01 1.55   85.88   88.04   89.08   90.09   92.05 21249    1\n## muT       80.12    0.01 1.94   76.32   78.88   80.11   81.37   83.91 17196    1\n## sigmaT     6.00    0.01 1.63    3.64    4.87    5.72    6.81   10.00 14787    1\n## sigmaE     3.50    0.00 0.40    2.82    3.22    3.47    3.75    4.39 15100    1\n## rho        0.72    0.00 0.11    0.49    0.65    0.73    0.80    0.90 16035    1\n## rhocomp    0.92    0.00 0.04    0.83    0.90    0.93    0.95    0.98 15119    1\n## lp__    -115.09    0.04 2.82 -121.51 -116.80 -114.72 -113.03 -110.63  5833    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:35:30 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit)## 'pars' not specified. Showing first 10 parameters by default.## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit, pars = c(\"T\", \"muT\", \"sigmaT\", \"sigmaE\", \"rho\", \"rhocomp\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\np1 <- ggs_grb(ggs(fit)) +\n   theme_bw() + theme(panel.grid = element_blank())\np1\n# autocorrelation\np1 <- ggs_autocorrelation(ggs(fit)) +\n   theme_bw() + theme(panel.grid = element_blank())\np1\n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"T[\",1:10,\"]\"), \"muT\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"sigmaT\", \"sigmaE\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"rho\", \"rhocomp\"),\n  prob = 0.8) +\n  plot_title\n# I prefer a posterior plot that includes prior and MLE\n# Expanded Posterior Plot\nMLE <- c(rowMeans(mydata$X), mean(mydata$X))\n\nprior_mu <- function(x){dnorm(x, 80, 10)}\nx.mu<- seq(50.1, 100, 0.1)\nprior.mu <- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu))\n\nprior_sigt <- function(x){dinvgamma(x, 1, 6)}\nx.sigt<- seq(.1, 15, 0.1)\nprior.sigt <- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt))\n\nprior_sige <- function(x){dinvgamma(x, 1, 4)}\nx.sige<- seq(.1, 10, 0.1)\nprior.sige <- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige))\n\nprior_t <- function(x){\n  mu <- rnorm(1, 80, 10)\n  sig <- rinvgamma(1, 1, 4)\n  rnorm(x, mu, sig)\n}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=prior_t(10000))\n\n\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\nplot.data <- as.data.frame(plot.data)\np1 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[5]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[10]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[10], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np4 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`muT`, color=\"Posterior\"))+\n  geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[11], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np5 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`sigmaT`, color=\"Posterior\"))+\n  geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=\"Prior\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np6 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`sigmaE`, color=\"Posterior\"))+\n  geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=\"Prior\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=\"collect\")"},{"path":"classical-test-theory.html","id":"example-3---jags","chapter":"8 Classical Test Theory","heading":"8.9 Example 3 - JAGS","text":"","code":"\n# model code\njags.model.ctt3 <- function(){\n     ############################################\n     # CLASSICAL TEST THEORY MODEL\n     # WITH UnkNOWN HYPERPARAMETERS\n     #    TRUE SCORE MEAN, TRUE SCORE VARIANCE\n     #    ERROR VARIANCE\n     ############################################\n\n     ############################################\n     # PRIOR DISTRIBUTIONS FOR HYPERPARAMETERS\n     ############################################\n     muT ~ dnorm(80,.01)     # Mean of the true scores\n\n     tau.T ~ dgamma(1, 36)   # Precision of the true scores\n     tau.E ~ dgamma(1, 16)   # Precision of the errors\n\n     sigma.squared.T <- 1/tau.T    # Variance of the true scores\n     sigma.squared.E <- 1/tau.E    # Variance of the errors\n     # get SD for summarizing\n     sigmaT <- pow(sigma.squared.T, 0.5)\n     sigmaE <- pow(sigma.squared.E, 0.5)\n     ############################################\n     # MODEL FOR TRUE SCORES AND OBSERVABLES\n     ############################################\n\n     for (i in 1:N) {\n          T[i] ~ dnorm(muT, tau.T)     # Distribution of true scores\n          for(j in 1:J){\n               X[i,j] ~ dnorm(T[i], tau.E)     # Distribution of observables\n          }\n     }\n\n     ############################################\n     # RELIABILITY\n     ############################################\n     rho <- sigma.squared.T/(sigma.squared.T+sigma.squared.E)\n     rhocomp <- J*rho/((J-1)*rho+1)\n\n}\n# data\nmydata <- list(\n  N = 10, J = 5,\n  X = matrix(\n    c(80, 77, 80, 73, 73,\n      83, 79, 78, 78, 77,\n      85, 77, 88, 81, 80,\n      76, 76, 76, 78, 67,\n      70, 69, 73, 71, 77,\n      87, 89, 92, 91, 87,\n      76, 75, 79, 80, 75,\n      86, 75, 80, 80, 82,\n      84, 79, 79, 77, 82,\n      96, 85, 91, 87, 90),\n    ncol=5, nrow=10, byrow=T)\n)\n\n# initial values\nstart_values <- list(\n  list(\"T\"=c(60,85,80,95,74,69,91,82,87,78),\n       \"muT\"=80, \"tau.E\"=0.06, \"tau.T\"=0.023),\n  list(\"T\"=c(63, 79, 74, 104, 80, 71, 95, 72, 80, 82),\n       \"muT\"=100, \"tau.E\"=0.09, \"tau.T\"=0.05),\n  list(\"T\"=c(59, 86, 88, 89, 76, 65, 94, 72, 95, 84),\n       \"muT\"=70, \"tau.E\"=0.03, \"tau.T\"=0.001),\n  list(\"T\"=c(60, 87, 90, 91, 77, 74, 95, 76, 83, 87),\n       \"muT\"=90, \"tau.E\"=0.01, \"tau.T\"=0.1)\n)\n\n# vector of all parameters to save\nparam_save <- c(\"T\",\"muT\",\"sigmaT\",\"sigmaE\", \"rho\", \"rhocomp\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.ctt2,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=4000,\n  n.burnin = 1000,\n  n.chains = 4,\n  n.thin=1,\n  progress.bar = \"none\")## Warning in jags.model(model.file, data = data, inits = init.values, n.chains = n.chains, : Unused\n## variable \"X\" in data## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 0\n##    Unobserved stochastic nodes: 60\n##    Total graph size: 68\n## \n## Deleting model## Error in setParameters(init.values[[i]], i): Error in node tau.E\n## Cannot set value of non-variable node\nprint(fit)## Inference for Stan model: anon_model.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\n## T[1]      76.86    0.01 1.52   73.90   75.86   76.86   77.86   79.92 24760    1\n## T[2]      79.08    0.01 1.52   76.10   78.07   79.08   80.08   82.08 25643    1\n## T[3]      82.03    0.01 1.52   79.02   81.01   82.03   83.05   84.97 24831    1\n## T[4]      75.02    0.01 1.56   71.98   73.97   75.01   76.08   78.14 23847    1\n## T[5]      72.62    0.01 1.56   69.57   71.59   72.60   73.66   75.74 21993    1\n## T[6]      88.50    0.01 1.58   85.34   87.48   88.51   89.54   91.57 21711    1\n## T[7]      77.23    0.01 1.53   74.22   76.22   77.23   78.26   80.24 22774    1\n## T[8]      80.56    0.01 1.52   77.56   79.54   80.55   81.57   83.55 28472    1\n## T[9]      80.19    0.01 1.52   77.19   79.19   80.20   81.20   83.17 24704    1\n## T[10]     89.05    0.01 1.55   85.88   88.04   89.08   90.09   92.05 21249    1\n## muT       80.12    0.01 1.94   76.32   78.88   80.11   81.37   83.91 17196    1\n## sigmaT     6.00    0.01 1.63    3.64    4.87    5.72    6.81   10.00 14787    1\n## sigmaE     3.50    0.00 0.40    2.82    3.22    3.47    3.75    4.39 15100    1\n## rho        0.72    0.00 0.11    0.49    0.65    0.73    0.80    0.90 16035    1\n## rhocomp    0.92    0.00 0.04    0.83    0.90    0.93    0.95    0.98 15119    1\n## lp__    -115.09    0.04 2.82 -121.51 -116.80 -114.72 -113.03 -110.63  5833    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:35:30 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)## Error in xy.coords(x, y, xlabel, ylabel, log = log, recycle = TRUE): 'list' object cannot be coerced to type 'double'\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)## Error in gelman.preplot(x, bin.width = bin.width, max.bins = max.bins, : Insufficient iterations to produce Gelman-Rubin plot\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))## Error in jags.mcmc[[1]]: this S4 class is not subsettable\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))## Error in y[, var.cols] <- x: number of items to replace is not a multiple of replacement length\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"T[\",1:10,\"]\"), \"muT\"),\n  prob = 0.8) +\n  plot_title## Error in x[, j, ] <- a[chain == j, , drop = FALSE]: replacement has length zero\nmcmc_areas(\n  plot.data,\n  pars = c(\"sigmaT\", \"sigmaE\"),\n  prob = 0.8) +\n  plot_title## Error in x[, j, ] <- a[chain == j, , drop = FALSE]: replacement has length zero\nmcmc_areas(\n  plot.data,\n  pars = c(\"rho\", \"rhocomp\"),\n  prob = 0.8) +\n  plot_title## Error in x[, j, ] <- a[chain == j, , drop = FALSE]: replacement has length zero\n# I prefer a posterior plot that includes prior and MLE\nMLE <- c(rowMeans(mydata$X), mean(mydata$X))\n\nprior_mu <- function(x){dnorm(x, 80, 10)}\nx.mu<- seq(50.1, 100, 0.1)\nprior.mu <- data.frame(mu=x.mu, dens.mu = prior_mu(x.mu))\n\nprior_sigt <- function(x){dinvgamma(x, 1, 6)}\nx.sigt<- seq(.1, 15, 0.1)\nprior.sigt <- data.frame(sigt=x.sigt, dens.sigt = prior_sigt(x.sigt))\n\nprior_sige <- function(x){dinvgamma(x, 1, 4)}\nx.sige<- seq(.1, 10, 0.1)\nprior.sige <- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige))\n\nprior_t <- function(x){\n  mu <- rnorm(1, 80, 10)\n  sig <- rinvgamma(1, 1, 4)\n  rnorm(x, mu, sig)\n}\nx.t<- seq(50.1, 100, 0.1)\nprior.t <- data.frame(tr=prior_t(10000))\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\np1 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[1]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np2 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[5]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[5], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np3 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`T[10]`, color=\"Posterior\"))+\n  geom_density(data=prior.t,aes(x=tr,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[10], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np4 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`muT`, color=\"Posterior\"))+\n  geom_line(data=prior.mu,aes(x=mu,y=dens.mu,color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[11], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np5 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`sigmaT`, color=\"Posterior\"))+\n  geom_line(data=prior.sigt,aes(x=sigt,y=dens.sigt,color=\"Prior\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np6 <- ggplot()+\n  geom_density(data=plot.data, aes(x=`sigmaE`, color=\"Posterior\"))+\n  geom_line(data=prior.sige,aes(x=sige,y=dens.sige,color=\"Prior\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol=3, guides=\"collect\")## Error in FUN(X[[i]], ...): object 'muT' not found"},{"path":"confirmatory-factor-analysis.html","id":"confirmatory-factor-analysis","chapter":"9 Confirmatory Factor Analysis","heading":"9 Confirmatory Factor Analysis","text":"full Bayesian specification general CFA model associated unknowns follows.\nincludes probability statements, notation, parameters, likelihood, priors, hyperparameters.\nobserved data defined \\(n\\times J\\) matrix \\(\\mathbf{X}\\) \\(J\\) observed measures.\nCFA model parameters defined \\[\\begin{align*}\n\\mathbf{x}_i &= \\tau + \\Lambda\\xi_i + \\varepsilon_i\\\\\n\\Sigma (\\mathbf{x}) &= \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi\n\\end{align*}\\]\\(\\Xi\\) \\(n\\times M\\) matrix latent variable scores \\(M\\) latent variables \\(n\\) respondents/subjects. single subject, \\(\\xi_i\\) represents vector scores latent variable(s). Values (location, scale, orientation, etc.) \\(\\xi_i\\) conditional (1) \\(\\kappa\\), \\(M\\times 1\\) vector latent variable means, (2) \\(\\Phi\\), \\(M\\times M\\) covariance matrix variable variables;\\(\\tau\\) \\(J\\times 1\\) vector observed variable intercepts expected value observed measures latent variable(s) \\(0\\);\\(\\Lambda\\) \\(J\\times M\\) matrix factor loadings \\(j\\)th row \\(m\\)th column represents factor loading \\(j\\)th observed variable \\(m\\)th latent variable;\\(\\delta_i\\) \\(J\\times 1\\) vector errors, \\(E(\\delta_i)=\\mathbf{0}\\) \\(\\mathrm{var}(\\delta_i)=\\Psi\\) \\(J\\times J\\) error covariance matrix.\\[\\begin{align*}\np(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi\\mid \\mathbf{X}) &\\propto p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)p(\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi)\\\\\n  &= p(\\mathbf{X}\\mid\\Xi, \\kappa, \\Phi, \\tau, \\Lambda, \\Psi) p(\\Xi\\mid\\kappa, \\Phi) p(\\kappa) p(\\Phi) p(\\tau) p(\\Lambda) p(\\Psi)\\\\\n  &= \\prod_{=1}^{n}\\prod_{j=1}^J\\prod_{m=1}^M p(x_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj}) p(\\xi_i\\mid\\kappa, \\Phi) p(\\kappa_m) p(\\Phi) p(\\tau_j) p(\\lambda_j) p(\\psi_{jj})\n\\end{align*}\\]\n\\[\\begin{align*}\nx_{ij}\\mid\\xi_i, \\tau_j,\\lambda_j, \\psi_{jj} &\\sim \\mathrm{Normal}(\\tau_j+\\xi_i\\lambda^{\\prime}_j, \\psi_{jj}),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\\n\\xi_i\\mid\\kappa, \\Phi &\\sim \\mathrm{Normal}(\\kappa, \\Phi),\\ \\mathrm{}\\ =1, \\cdots, n;\\\\\n\\kappa_m &\\sim \\mathrm{Normal}(\\mu_{\\kappa},\\sigma^2_{\\kappa}),\\ \\mathrm{}\\ m = 1, \\cdots, M;\\\\\n\\Phi &\\sim \\mathrm{Inverse-Wishart}(\\Phi_0, d);\\\\\n\\tau_j &\\sim \\mathrm{Normal}(\\mu_{\\tau},\\sigma^2_{\\tau}),\\ \\mathrm{}\\ j = 1, \\cdots, J;\\\\\n\\lambda_{j,m} &\\sim \\mathrm{Normal}(\\mu_{\\lambda}, \\sigma^2_{\\lambda}),\\ \\mathrm{}\\ j = 1, \\cdots, J,\\ m = 1, \\cdots, M;\\\\\n\\psi_{jj} &\\sim \\mathrm{Inverse-Gamma}(\\nu_{\\psi}/2, \\nu_{\\psi}\\psi_0/2),\\ \\mathrm{}\\ j=1, \\cdots, J.\n\\end{align*}\\]\nhyperparameters supplied analyst defined \\(\\mu_{\\kappa}\\) prior mean latent variable,\\(\\sigma^2_{\\kappa}\\) prior variance latent variable,\\(\\Phi_0\\) prior expectation covariance matrix among latent variables,\\(d\\) represents dispersion parameter reflecting magnitude beliefs \\(\\Phi_0\\),\\(\\mu_{\\tau}\\) prior mean intercepts reflects knowledge location observed variables,\\(\\sigma^2_{\\tau}\\) measure much weight want give prior mean,\\(\\mu_{\\lambda}\\) prior mean factor loadings can vary items latent variables,\\(\\sigma^2_{\\lambda}\\) measure dispersion factor loadings, lower variances indicate stronger belief values loadings,\\(\\nu_{\\psi}\\) measure location gamma prior indicating expectation magnitude error variance,\\(\\psi_0\\) uncertainty respect location selected variance, andAlternatively, place prior \\(\\Psi\\) instead individual residual variances. mean placing prior error-covariance matrix similar specified prior latent variance covariance matrix.","code":""},{"path":"confirmatory-factor-analysis.html","id":"single-latent-variable-model","chapter":"9 Confirmatory Factor Analysis","heading":"9.1 Single Latent Variable Model","text":"consider model section 9.3 CFA model 1 latent variable 5 observed indicators.\ngraphical representation factor models get pretty complex pretty quickly, example reproduced version Figure 9.3b, shown .\nFigure 9.1: DAG CFA model 1 latent variable\nHowever, authors noted, path diagram tradition conveying models also useful discussing describing model, give next.\nFigure 9.2: Path diagram CFA model 1 latent variable\ncompleteness, included model specification diagram concretely connects DAG path diagram assumed distributions priors.\nFigure 9.3: Model specification diagram CFA model 1 latent factor\n","code":""},{"path":"confirmatory-factor-analysis.html","id":"jags---single-latent-variable","chapter":"9 Confirmatory Factor Analysis","heading":"9.2 JAGS - Single Latent Variable","text":"","code":"\n# model code\njags.model.cfa1 <- function(){\n\n  ########################################\n  # Specify the factor analysis measurement model for the observables\n  ##############################################\n  for (i in 1:n){\n    for(j in 1:J){\n      mu[i,j] <- tau[j] + ksi[i]*lambda[j]      # model implied expectation for each observable\n      x[i,j] ~ dnorm(mu[i,j], inv.psi[j])    # distribution for each observable\n    }\n  }\n  \n  \n  ##################################\n  # Specify the (prior) distribution for the latent variables\n  ####################################\n  for (i in 1:n){\n    ksi[i] ~ dnorm(kappa, inv.phi)  # distribution for the latent variables\n  }\n  \n  \n  ######################################\n  # Specify the prior distribution for the parameters that govern the latent variables\n  ###################################\n  kappa <- 0              # Mean of factor 1\n  inv.phi ~ dgamma(5, 10) # Precision of factor 1\n  phi <- 1/inv.phi        # Variance of factor 1\n  \n  \n  ########################################\n  # Specify the prior distribution for the measurement model parameters\n  ########################################\n  for(j in 1:J){\n    tau[j] ~ dnorm(3, .1)        # Intercepts for observables\n    inv.psi[j] ~ dgamma(5, 10) # Precisions for observables\n    psi[j] <- 1/inv.psi[j]   # Variances for observables\n  }\n  \n  lambda[1] <- 1.0              # loading fixed to 1.0 \n  for (j in 2:J){\n    lambda[j] ~ dnorm(1, .1)    # prior distribution for the remaining loadings\n  }\n}\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  n = 500, J = 5,\n  x = as.matrix(dat)\n)\n\n\n# initial values\nstart_values <- list(\n  list(\"tau\"=c(.1, .1, .1, .1, .1),\n       \"lambda\"=c(NA, 0, 0, 0, 0),\n       \"inv.phi\"=1,\n       \"inv.psi\"=c(1, 1, 1, 1, 1)),\n  list(\"tau\"=c(3, 3, 3, 3, 3),\n       \"lambda\"=c(NA, 3, 3, 3, 3),\n       \"inv.phi\"=2,\n       \"inv.psi\"=c(2, 2, 2, 2, 2)),\n  list(\"tau\"=c(5, 5, 5, 5, 5),\n       \"lambda\"=c(NA, 6, 6, 6, 6),\n       \"inv.phi\"=.5,\n       \"inv.psi\"=c(.5, .5, .5, .5, .5))\n)\n\n# vector of all parameters to save\n# exclude fixed lambda since it throws an error in\n# in the GRB plot\nparam_save <- c(\"tau\", paste0(\"lambda[\",2:5,\"]\"), \"phi\", \"psi\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.cfa1,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=5000,\n  n.burnin = 2500,\n  n.chains = 3,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 2500\n##    Unobserved stochastic nodes: 515\n##    Total graph size: 8029\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model45442151126.txt\", fit using jags,\n##  3 chains, each with 5000 iterations (first 2500 discarded)\n##  n.sims = 7500 iterations saved\n##            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat n.eff\n## lambda[2]    0.730   0.045    0.646    0.699    0.729    0.760    0.820 1.001  7500\n## lambda[3]    0.421   0.037    0.351    0.396    0.420    0.445    0.495 1.001  7500\n## lambda[4]    1.053   0.066    0.930    1.008    1.051    1.095    1.187 1.001  6000\n## lambda[5]    0.987   0.059    0.875    0.947    0.985    1.026    1.107 1.002  2400\n## phi          0.434   0.043    0.354    0.404    0.432    0.463    0.524 1.002  2600\n## psi[1]       0.373   0.028    0.320    0.353    0.372    0.391    0.433 1.001  7500\n## psi[2]       0.183   0.014    0.157    0.173    0.182    0.192    0.212 1.003  1100\n## psi[3]       0.180   0.012    0.157    0.171    0.179    0.187    0.205 1.002  1400\n## psi[4]       0.378   0.030    0.323    0.357    0.376    0.398    0.440 1.001  3800\n## psi[5]       0.265   0.022    0.225    0.250    0.264    0.279    0.309 1.002  2500\n## tau[1]       3.333   0.040    3.255    3.305    3.333    3.359    3.412 1.001  7500\n## tau[2]       3.898   0.029    3.841    3.879    3.898    3.917    3.955 1.001  7500\n## tau[3]       4.596   0.023    4.553    4.581    4.596    4.611    4.641 1.002  2300\n## tau[4]       3.033   0.042    2.953    3.005    3.033    3.062    3.116 1.001  4400\n## tau[5]       3.712   0.037    3.639    3.688    3.712    3.737    3.784 1.001  7500\n## deviance  3379.989  42.451 3297.778 3351.818 3379.183 3408.556 3465.562 1.001  7500\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 901.2 and DIC = 4281.2\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"tau[\",1:5,\"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"lambda[\", 2:5, \"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"psi[\", 1:5, \"]\"), \"phi\"),\n  prob = 0.8) +\n  plot_title"},{"path":"confirmatory-factor-analysis.html","id":"stan---single-latent-variable","chapter":"9 Confirmatory Factor Analysis","heading":"9.3 Stan - Single Latent Variable","text":"","code":"\nmodel_cfa1 <- '\ndata {\n  int  N;\n  int  J;\n  matrix[N, J] X;\n}\n\nparameters {\n  real ksi[N]; //latent variable values\n  real tau[J]; //intercepts\n  real load[J-1]; //factor loadings\n  real<lower=0> psi[J]; //residual variance\n  //real kappa; // factor means\n  real<lower=0> phi; // factor variances\n}\n\ntransformed parameters {\n  real lambda[J];\n  lambda[1] = 1;\n  lambda[2:J] = load;\n}\n\nmodel {\n  real kappa;\n  kappa = 0;\n  // likelihood for data\n  for(i in 1:N){\n    for(j in 1:J){\n      X[i, j] ~ normal(tau[j] + ksi[i]*lambda[j], psi[j]);\n    }\n  }\n  // prior for latent variable parameters\n  ksi ~ normal(kappa, phi);\n  \n  phi ~ inv_gamma(5, 10);\n  // prior for measurement model parameters\n  tau ~ normal(3, 10);\n  psi ~ inv_gamma(5, 10);\n\n  for(j in 1:(J-1)){\n    load[j] ~ normal(1, 10);\n  }\n  \n}\n\n'\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  N = 500, J = 5,\n  X = as.matrix(dat)\n)\n\n# initial values\nstart_values <- list(\n  list(tau = c(.1,.1,.1,.1,.1), lambda=c(0, 0, 0, 0, 0), phi = 1, psi=c(1, 1, 1, 1, 1)),\n  list(tau = c(3,3,3,3,3), lambda=c(3, 3, 3, 3, 3), phi = 2, psi=c(.5, .5, .5, .5, .5)),\n  list(tau = c(5, 5, 5, 5, 5), lambda=c(6, 6, 6, 6, 6), phi = 2, psi=c(2, 2, 2, 2, 2))\n)\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_cfa1, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains = 3,             # number of Markov chains\n  warmup = 1000,          # number of warm up iterations per chain\n  iter = 5000,            # total number of iterations per chain\n  cores = 1,              # number of cores (could use one per chain)\n  refresh = 0             # no progress shown\n)\n\n# first get a basic breakdown of the posteriors\nprint(fit,pars =c(\"lambda\", \"tau\", \"psi\", \"phi\", \"ksi[1]\", \"ksi[8]\"))## Inference for Stan model: anon_model.\n## 3 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=12000.\n## \n##            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## lambda[1]  1.00     NaN 0.00  1.00  1.00  1.00  1.00  1.00   NaN  NaN\n## lambda[2]  0.81       0 0.05  0.72  0.78  0.81  0.85  0.92  1491    1\n## lambda[3]  0.47       0 0.04  0.40  0.45  0.47  0.50  0.55  2377    1\n## lambda[4]  1.10       0 0.07  0.97  1.05  1.10  1.15  1.26  1682    1\n## lambda[5]  1.06       0 0.07  0.93  1.01  1.06  1.10  1.20  1491    1\n## tau[1]     3.33       0 0.04  3.26  3.31  3.33  3.36  3.41  2409    1\n## tau[2]     3.90       0 0.03  3.84  3.88  3.90  3.92  3.95  2080    1\n## tau[3]     4.60       0 0.02  4.55  4.58  4.60  4.61  4.64  3140    1\n## tau[4]     3.03       0 0.04  2.96  3.01  3.03  3.06  3.11  2329    1\n## tau[5]     3.71       0 0.04  3.64  3.69  3.71  3.74  3.78  2098    1\n## psi[1]     0.60       0 0.02  0.55  0.58  0.60  0.61  0.64  8087    1\n## psi[2]     0.36       0 0.02  0.33  0.35  0.36  0.37  0.40  4521    1\n## psi[3]     0.37       0 0.01  0.35  0.37  0.37  0.38  0.40  9691    1\n## psi[4]     0.60       0 0.02  0.56  0.59  0.60  0.62  0.65  7454    1\n## psi[5]     0.48       0 0.02  0.44  0.47  0.48  0.50  0.52  5708    1\n## phi        0.60       0 0.03  0.54  0.58  0.60  0.62  0.67  1459    1\n## ksi[1]    -0.23       0 0.22 -0.67 -0.38 -0.23 -0.08  0.21 18018    1\n## ksi[8]     0.85       0 0.23  0.41  0.69  0.84  0.99  1.30 11586    1\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 17:38:46 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit,pars =c(\"lambda\", \"tau\", \"psi\", \"phi\", \"ksi[1]\", \"ksi[8]\"))## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit,pars =c(\"lambda\", \"tau\", \"psi\", \"phi\", \"ksi[1]\", \"ksi[8]\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\nggs_grb(ggs(fit, family = c(\"lambda\"))) +\n   theme_bw() + theme(panel.grid = element_blank())## Warning: Removed 50 row(s) containing missing values (geom_path).\nggs_grb(ggs(fit, family = \"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\n# autocorrelation\nggs_autocorrelation(ggs(fit, family=\"lambda\")) +\n   theme_bw() + theme(panel.grid = element_blank())## Warning in cor(X, use = \"pairwise.complete.obs\"): the standard deviation is zero## Warning in cor(X, use = \"pairwise.complete.obs\"): the standard deviation is zero\n\n## Warning in cor(X, use = \"pairwise.complete.obs\"): the standard deviation is zero## Warning: Removed 150 rows containing missing values (geom_bar).\nggs_autocorrelation(ggs(fit, family=\"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = paste0(\"lambda[\",1:5,\"]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = paste0(\"tau[\",1:5,\"]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"psi[\",1:5,\"]\"),\n           \"phi\"),\n  prob = 0.8) +\n  plot_title\n# I prefer a posterior plot that includes prior and MLE\n# Expanded Posterior Plot\ncolnames(dat) <- paste0(\"x\",1:5)\nlav.mod <- '\n  xi =~ 1*x1 + x2 + x3 + x4 + x5\n  xi ~~ xi\n  x1 ~ 1\n  x2 ~ 1\n  x3 ~ 1\n  x4 ~ 1\n  x5 ~ 1\n'\nlav.fit <- lavaan::cfa(lav.mod, data=dat)\n\nMLE <- lavaan::parameterEstimates(lav.fit)\n\nprior_tau <- function(x){dnorm(x, 3, 10)}\nx.tau<- seq(1, 5, 0.01)\nprior.tau <- data.frame(tau=x.tau, dens.mtau = prior_tau(x.tau))\n\nprior_lambda <- function(x){dnorm(x, 1, 10)}\nx.lambda<- seq(0, 2, 0.01)\nprior.lambda <- data.frame(lambda=x.lambda, dens.lambda = prior_lambda(x.lambda))\n\nprior_sig <- function(x){dinvgamma(x, 5, 10)}\nx.sig<- seq(.01, 1, 0.01)\nprior.sig <- data.frame(sig=x.sig, dens.sig = prior_sig(x.sig))\n\nprior_sige <- function(x){dinvgamma(x, 1, 4)}\nx.sige<- seq(.1, 10, 0.1)\nprior.sige <- data.frame(sige=x.sige, dens.sige = prior_sige(x.sige))\n\nprior_ksi <- function(x){\n  mu <- 0\n  sig <- rinvgamma(1, 5, 10)\n  rnorm(x, mu, sig)\n}\nx.ksi<- seq(-5, 5, 0.01)\nprior.ksi <- data.frame(ksi=prior_ksi(10000))\n\ncols <- c(\"Posterior\"=\"#0072B2\", \"Prior\"=\"#E69F00\", \"MLE\"= \"black\")#\"#56B4E9\", \"#E69F00\" \"#CC79A7\"\n\n# get stan samples\nplot.data <- as.data.frame(plot.data)\n\n# make plotting pieces\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`lambda[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.lambda,\n            aes(x=lambda, y=dens.lambda, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[1, 4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.25, 1.5))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`lambda[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.lambda,\n            aes(x=lambda, y=dens.lambda, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[2, 4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.25, 1.5))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`lambda[3]`, color=\"Posterior\"))+\n  geom_line(data=prior.lambda,\n            aes(x=lambda, y=dens.lambda, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[3, 4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.25, 1.5))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np4 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`lambda[4]`, color=\"Posterior\"))+\n  geom_line(data=prior.lambda,\n            aes(x=lambda, y=dens.lambda, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[4, 4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.25, 1.5))+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np5 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`lambda[5]`, color=\"Posterior\"))+\n  geom_line(data=prior.lambda,\n            aes(x=lambda, y=dens.lambda, color=\"Prior\"))+\n  geom_vline(aes(xintercept=MLE[5, 4], color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  lims(x=c(0.25, 1.5))+theme_bw()+\n  theme(panel.grid = element_blank())\np1 + p2 + p3 + p4 + p5 + plot_layout(guides=\"collect\")## Warning: Removed 75 row(s) containing missing values (geom_path).## Warning: Removed 75 row(s) containing missing values (geom_path).\n## Removed 75 row(s) containing missing values (geom_path).\n## Removed 75 row(s) containing missing values (geom_path).\n## Removed 75 row(s) containing missing values (geom_path).\n# phi\np1 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`phi`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[6,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\n# psi\np2 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`psi[1]`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[12,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np3 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`psi[2]`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[13,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np4 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`psi[3]`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[14,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np5 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`psi[4]`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[15,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\np6 <- ggplot()+\n  geom_density(data=plot.data,\n               aes(x=`psi[5]`, color=\"Posterior\"))+\n  geom_line(data=prior.sig,\n            aes(x=sig, y=dens.sig, color=\"Prior\"))+\n  geom_vline(aes(xintercept=sqrt(MLE[16,4]), color=\"MLE\"))+\n  scale_color_manual(values=cols, name=NULL)+\n  theme_bw()+\n  theme(panel.grid = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(guides = \"collect\")"},{"path":"confirmatory-factor-analysis.html","id":"blavaan---single-latent-variable","chapter":"9 Confirmatory Factor Analysis","heading":"9.4 Blavaan - Single Latent Variable","text":"","code":"\n# model\nmodel_cfa_blavaan <- \"\n  f1 =~ 1*PI + AD + IGC + FI + FC\n\"\n\ndat <- as.matrix(read.table(\"code/CFA-Two-Latent-Variables/Data/IIS.dat\", header=T))\n\nfit <- blavaan::bcfa(model_cfa_blavaan, data=dat)## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 0 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 2.76 seconds (Warm-up)\n## Chain 1:                5.266 seconds (Sampling)\n## Chain 1:                8.026 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 0 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 2.882 seconds (Warm-up)\n## Chain 2:                5.3 seconds (Sampling)\n## Chain 2:                8.182 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 0 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 3: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 3: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 3: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 3: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 3: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 3: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 3: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 3: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 3: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 3: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 3: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 2.792 seconds (Warm-up)\n## Chain 3:                5.302 seconds (Sampling)\n## Chain 3:                8.094 seconds (Total)\n## Chain 3: \n## Computing posterior predictives...\nsummary(fit)## blavaan (0.4-3) results of 1000 samples after 500 adapt/burnin iterations\n## \n##   Number of observations                           500\n## \n##   Number of missing patterns                         1\n## \n##   Statistic                                 MargLogLik         PPP\n##   Value                                      -2196.532       0.000\n## \n## Latent Variables:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##   f1 =~                                                                        \n##     PI                1.000                                                    \n##     AD                0.847    0.056    0.743    0.964    1.002    normal(0,10)\n##     IGC               0.494    0.043    0.414    0.579    1.003    normal(0,10)\n##     FI                1.131    0.079    0.984    1.298    1.003    normal(0,10)\n##     FC                1.090    0.074    0.955    1.252    1.003    normal(0,10)\n## \n## Intercepts:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .PI                3.332    0.038    3.253    3.407    1.000    normal(0,32)\n##    .AD                3.896    0.028    3.840    3.950    1.000    normal(0,32)\n##    .IGC               4.595    0.022    4.554    4.638    1.000    normal(0,32)\n##    .FI                3.032    0.042    2.952    3.112    1.000    normal(0,32)\n##    .FC                3.711    0.037    3.637    3.783    1.000    normal(0,32)\n##     f1                0.000                                                    \n## \n## Variances:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .PI                0.353    0.026    0.305    0.407    1.001 gamma(1,.5)[sd]\n##    .AD                0.120    0.012    0.097    0.145    1.000 gamma(1,.5)[sd]\n##    .IGC               0.133    0.009    0.115    0.152    1.002 gamma(1,.5)[sd]\n##    .FI                0.363    0.030    0.307    0.422    1.001 gamma(1,.5)[sd]\n##    .FC                0.224    0.021    0.185    0.269    1.000 gamma(1,.5)[sd]\n##     f1                0.337    0.041    0.259    0.423    1.002 gamma(1,.5)[sd]\nplot(fit)"},{"path":"confirmatory-factor-analysis.html","id":"two-latent-variable-model","chapter":"9 Confirmatory Factor Analysis","heading":"9.5 Two Latent Variable Model","text":"consider model section 9.4 CFA model 2 latent variables 5 observed indicators.\ngraphical representation factor models get pretty complex pretty quickly, example reproduced version Figure 9.4, shown .\nFigure 9.4: DAG CFA model 2 latent variables\nHowever, authors noted, path diagram tradition conveying models also useful discussing describing model, give next.\nFigure 9.5: Path Diagram CFA model 2 latent variables\ncompleteness, included model specification diagram concretely connects DAG path diagram assumed distributions priors.\nFigure 9.6: Model specification diagram CFA model 2 latent factors\n","code":""},{"path":"confirmatory-factor-analysis.html","id":"jags---two-latent-variable","chapter":"9 Confirmatory Factor Analysis","heading":"9.6 JAGS - Two Latent Variable","text":"","code":"\n# model code\njags.model.cfa2 <- function(){\n\n  ###################\n  # Specify the factor analysis measurement model for the observables\n  ####################\n  for (i in 1:n){\n    \n    # expected value for each examinee for each observable\n    mu[i,1] <- tau[1] + lambda[1,1]*ksi[i,1]    \n    mu[i,2] <- tau[2] + lambda[2,1]*ksi[i,1]        \n    mu[i,3] <- tau[3] + lambda[3,1]*ksi[i,1]          \n    mu[i,4] <- tau[4] + lambda[4,2]*ksi[i,2]          \n    mu[i,5] <- tau[5] + lambda[5,2]*ksi[i,2]          \n    \n    for(j in 1:J){\n      x[i,j] ~ dnorm(mu[i,j], inv.psi[j])    # distribution for each observable\n    }\n  }\n  \n  \n  ######################################################################\n  # Specify the (prior) distribution for the latent variables\n  ######################################################################\n  for (i in 1:n){\n    ksi[i, 1:M] ~ dmnorm(kappa[], inv.phi[,])  # distribution for the latent variables\n  }\n  \n  \n  ######################################################################\n  # Specify the prior distribution for the parameters that govern the latent variables\n  ######################################################################\n  for(m in 1:M){\n    kappa[m] <- 0              # Means of latent variables\n  }\n  \n  inv.phi[1:M,1:M] ~ dwish(dxphi.0[ , ], d);    # prior for precision matrix for the latent variables\n  phi[1:M,1:M] <- inverse(inv.phi[ , ]);        # the covariance matrix for the latent vars\n  \n  phi.0[1,1] <- 1;                  \n  phi.0[1,2] <- .3;                 \n  phi.0[2,1] <- .3;\n  phi.0[2,2] <- 1;\n  d <- 2;                           \n                              \n  for (m in 1:M){                       \n       for (mm in 1:M){\n            dxphi.0[m,mm] <- d*phi.0[m,mm];\n       }\n  }\n  \n  \n  \n  ######################################################################\n  # Specify the prior distribution for the measurement model parameters\n  ######################################################################\n  for(j in 1:J){\n    tau[j] ~ dnorm(3, .1)        # Intercepts for observables\n    inv.psi[j] ~ dgamma(5, 10) # Precisions for observables\n    psi[j] <- 1/inv.psi[j]   # Variances for observables\n  }\n  \n  lambda[1,1] <- 1.0              # loading fixed to 1.0 \n  lambda[4,2] <- 1.0              # loading fixed to 1.0 \n  \n  for (j in 2:3){\n    lambda[j,1] ~ dnorm(1, .1)    # prior distribution for the remaining loadings\n  }\n  lambda[5,2] ~ dnorm(1, .1)      # prior distribution for the remaining loadings\n}\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  n = 500, J = 5, M =2,\n  x = as.matrix(dat)\n)\n\n\n# initial values\nstart_values <- list(\n  list(\"tau\"=c(1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01),\n       lambda= structure(\n         .Data= c( NA,  2.00E+00, 2.00E+00, NA, NA,\n                   NA,  NA,  NA,  NA, 2.00E+00),\n         .Dim=c(5, 2)),\n       inv.phi= structure(\n         .Data= c(1.00E+00, 0.00E+00, 0.00E+00, 1.00E+00),\n         .Dim=c(2, 2)),\n       inv.psi=c(1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00)),\n  list(tau=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00),\n       lambda= structure(\n         .Data= c( NA,  5.00E-01, 5.00E-01,  NA, NA, \n                   NA,  NA,  NA,  NA, 5.00E-01),\n         .Dim=c(5, 2)),\n       inv.phi= structure(\n         .Data= c(1.33E+00, -6.67E-01, -6.67E-01, 1.33E+00),\n         .Dim=c(2, 2)),\n       inv.psi=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00))\n,\n  list(tau=c(5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00, 5.00E+00),\n       lambda= structure(\n         .Data= c( NA,  1.00E+00, 1.00E+00,  NA, NA,\n                   NA,  NA,  NA,  NA, 1.00E+00),\n         .Dim=c(5, 2)),\n       inv.phi= structure(\n         .Data= c(1.96E+00, -1.37E+00, -1.37E+00, 1.96E+00),\n         .Dim=c(2, 2)),\n       inv.psi=c(5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01, 5.00E-01))\n)\n\n# vector of all parameters to save\n# exclude fixed lambda since it throws an error in\n# in the GRB plot\nparam_save <- c(\"tau\", \"lambda[2,1]\",\"lambda[3,1]\",\"lambda[5,2]\", \"phi\", \"psi\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.cfa2,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=5000,\n  n.burnin = 2500,\n  n.chains = 3,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 2500\n##    Unobserved stochastic nodes: 514\n##    Total graph size: 9035\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpKotSbY/model4ac475707c36.txt\", fit using jags,\n##  3 chains, each with 5000 iterations (first 2500 discarded)\n##  n.sims = 7500 iterations saved\n##              mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat n.eff\n## lambda[2,1]    0.782   0.117    0.678    0.738    0.771    0.809    0.892 1.028   440\n## lambda[3,1]    0.466   0.097    0.382    0.431    0.459    0.488    0.555 1.031   820\n## lambda[5,2]    0.929   0.150    0.817    0.880    0.915    0.953    1.037 1.005  7500\n## phi[1,1]       0.380   0.053    0.293    0.351    0.381    0.411    0.472 1.005   650\n## phi[2,1]       0.375   0.048    0.303    0.352    0.376    0.402    0.453 1.002  2600\n## phi[1,2]       0.375   0.048    0.303    0.352    0.376    0.402    0.453 1.002  2600\n## phi[2,2]       0.493   0.066    0.393    0.461    0.495    0.531    0.602 1.001  7500\n## psi[1]         0.368   0.035    0.310    0.346    0.365    0.386    0.432 1.003   850\n## psi[2]         0.173   0.174    0.144    0.161    0.170    0.179    0.199 1.017  7500\n## psi[3]         0.174   0.241    0.149    0.163    0.171    0.179    0.196 1.040  2100\n## psi[4]         0.343   0.125    0.284    0.318    0.338    0.359    0.405 1.005  7500\n## psi[5]         0.245   0.141    0.204    0.228    0.242    0.257    0.288 1.006  7500\n## tau[1]         3.334   0.038    3.260    3.308    3.334    3.360    3.410 1.001  7500\n## tau[2]         3.898   0.028    3.843    3.879    3.898    3.917    3.953 1.001  7500\n## tau[3]         4.596   0.023    4.551    4.581    4.596    4.611    4.640 1.001  7500\n## tau[4]         3.034   0.041    2.955    3.007    3.034    3.062    3.115 1.001  7500\n## tau[5]         3.713   0.036    3.641    3.689    3.713    3.738    3.784 1.001  7500\n## deviance    3189.610 134.312 3072.986 3145.423 3182.566 3223.278 3306.219 1.011  7500\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 9022.3 and DIC = 12211.9\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)\n# gelman-rubin-brook\ngelman.plot(jags.mcmc)\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"tau[\",1:5,\"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(\"lambda[2,1]\",\"lambda[3,1]\",\"lambda[5,2]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"psi[\", 1:5, \"]\"), \"phi[1,1]\", \"phi[1,2]\",\"phi[2,2]\"),\n  prob = 0.8) +\n  plot_title"},{"path":"confirmatory-factor-analysis.html","id":"stan---two-latent-variable","chapter":"9 Confirmatory Factor Analysis","heading":"9.7 Stan - Two Latent Variable","text":"","code":""},{"path":"confirmatory-factor-analysis.html","id":"inverse-wishart-prior","chapter":"9 Confirmatory Factor Analysis","heading":"9.7.1 Inverse-Wishart Prior","text":"Using Stan based nearly identical model structure presented text.","code":"\nmodel_cfa_2factor <- [1051 chars quoted with '\"']\n\n# data must be in a list\ndat <- as.matrix(read.table(\"code/CFA-Two-Latent-Variables/Data/IIS.dat\", header=T))\n\nmydata <- list(\n  N = 500, J = 5,\n  M = 2,\n  X = dat,\n  phi0 = matrix(c(1, .3, .3, 1), ncol=2)\n)\n\n# # initial values\nstart_values <- list(\n  list(\n    phi= structure(\n      .Data= c(1, 0.30, 0.30, 1),\n      .Dim=c(2, 2)),\n    tau = c(3, 3, 3, 3, 3),\n    lambda= c(1, 1, 1, 1, 1),\n    psi=c(.5, .5, .5, .5, .5)\n  ),\n  list(\n    phi= structure(\n      .Data= c(1, 0, 0, 1),\n      .Dim=c(2, 2)),\n    tau = c(5, 5, 5, 5, 5),\n    lambda= c(1, .7, .7, 1, .7),\n    psi=c(2, 2, 2, 2, 2)\n  ),\n  list(\n    phi= structure(\n      .Data= c(1, 0.10, 0.10, 1),\n      .Dim=c(2, 2)),\n    tau = c(1, 1, 1, 1, 1),\n    lambda= c(1, 1.3, 1.3, 1, 1.3),\n    psi=c(1, 1, 1, 1, 1)\n  )\n)\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_cfa_2factor, # model code to be compiled\n  data = mydata,          # my data\n  init = start_values,    # starting values\n  chains=3,\n  refresh = 0             # no progress shown\n)## Warning: There were 2983 divergent transitions after warmup. See\n## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n## to find out why this is a problem and how to eliminate them.## Warning: There were 17 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\n## https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded## Warning: Examine the pairs() plot to diagnose sampling problems## Warning: The largest R-hat is 3.78, indicating chains have not mixed.\n## Running the chains for more iterations may help. See\n## https://mc-stan.org/misc/warnings.html#r-hat## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\n## Running the chains for more iterations may help. See\n## https://mc-stan.org/misc/warnings.html#bulk-ess## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\n## Running the chains for more iterations may help. See\n## https://mc-stan.org/misc/warnings.html#tail-ess\n# first get a basic breakdown of the posteriors\nprint(fit, \n      pars =c(\"lambda\", \"tau\", \"psi\",\n              \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"))## Inference for Stan model: anon_model.\n## 3 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=3000.\n## \n##            mean se_mean   sd  2.5%   25%  50%  75% 97.5% n_eff       Rhat\n## lambda[1]  1.00    0.00 0.00  1.00  1.00 1.00 1.00  1.00     2       3.43\n## lambda[2]  1.00    0.20 0.24  0.70  0.70 1.00 1.30  1.30     2  688044.85\n## lambda[3]  1.00    0.20 0.24  0.70  0.70 1.00 1.30  1.30     2  571252.30\n## lambda[4]  1.00    0.00 0.00  1.00  1.00 1.00 1.00  1.00     2       8.82\n## lambda[5]  1.00    0.20 0.24  0.70  0.70 1.00 1.30  1.30     2  828681.14\n## tau[1]     3.00    1.33 1.63  1.00  1.00 3.00 5.00  5.00     2 6408079.17\n## tau[2]     3.00    1.33 1.63  1.00  1.00 3.00 5.00  5.00     2 4018815.25\n## tau[3]     3.00    1.33 1.63  1.00  1.00 3.00 5.00  5.00     2 4477733.07\n## tau[4]     3.00    1.33 1.63  1.00  1.00 3.00 5.00  5.00     2 5380093.58\n## tau[5]     3.00    1.33 1.63  1.00  1.00 3.00 5.00  5.00     2 2626323.63\n## psi[1]     1.17    0.51 0.62  0.50  0.50 1.00 2.00  2.00     2 1360074.81\n## psi[2]     1.17    0.51 0.62  0.50  0.50 1.00 2.00  2.00     2 1250198.67\n## psi[3]     1.17    0.51 0.62  0.50  0.50 1.00 2.00  2.00     2 1082592.54\n## psi[4]     1.17    0.51 0.62  0.50  0.50 1.00 2.00  2.00     2 1498131.27\n## psi[5]     1.17    0.51 0.62  0.50  0.50 1.00 2.00  2.00     2 1034527.28\n## phi[1,1]   1.00    0.00 0.00  1.00  1.00 1.00 1.00  1.00     3       1.55\n## phi[1,2]   0.13    0.10 0.12  0.00  0.00 0.10 0.30  0.30     2  388039.24\n## phi[2,1]   0.13    0.10 0.12  0.00  0.00 0.10 0.30  0.30     2  387623.00\n## phi[2,2]   1.00    0.00 0.00  1.00  1.00 1.00 1.00  1.00     2       9.66\n## ksi[1,1]   0.59    0.50 0.62 -0.28 -0.28 0.95 1.10  1.10     2 2255396.84\n## ksi[1,2]  -0.33    0.89 1.08 -1.84 -1.84 0.18 0.66  0.66     2 3056164.29\n## ksi[8,1]   0.13    0.69 0.85 -0.91 -0.91 0.12 1.18  1.18     2 2760193.16\n## ksi[8,2]   0.40    0.34 0.41 -0.10 -0.10 0.39 0.91  0.91     2  571386.66\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 23:30:13 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit,\n     pars =c(\"lambda\", \"tau\", \"psi\",\n              \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"))## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(\n  fit,\n  pars =c(\"lambda\", \"tau\", \"psi\",\n              \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"),\n  inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\nggs_grb(ggs(fit, family = c(\"lambda\"))) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\n# autocorrelation\nggs_autocorrelation(ggs(fit, family=\"lambda\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())"},{"path":"confirmatory-factor-analysis.html","id":"lkj-cholesky-parameterization","chapter":"9 Confirmatory Factor Analysis","heading":"9.7.2 LKJ Cholesky Parameterization","text":"massive problems , search people estimate CFA models Stan.\nfound people use LKJ Cholesky parameterization.helpful pages used help get work.Stan User’s Guide Factor Covaraince ParameterizationMichael DeWitt - Confirmatory Factor Analysis StanRick Farouni - Fitting Bayesian Factor Analysis Model Stan","code":"\nmodel_cfa2 <- [1409 chars quoted with '\"']\n\n# data must be in a list\ndat <- as.matrix(read.table(\"code/CFA-Two-Latent-Variables/Data/IIS.dat\", header=T))\n\nmydata <- list(\n  N = 500, J = 5,\n  M = 2,\n  X = dat\n)\n\n# Next, need to fit the model\n#   I have explicitly outlined some common parameters\nfit <- stan(\n  model_code = model_cfa2, # model code to be compiled\n  data = mydata,          # my data\n  #init = init_fun, #start_values,    # starting values\n  refresh = 0             # no progress shown\n)## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\n## Running the chains for more iterations may help. See\n## https://mc-stan.org/misc/warnings.html#bulk-ess\n# first get a basic breakdown of the posteriors\nprint(fit, \n      pars =c(\"lambda\", \"tau\", \"psi\",\n              \"R\", \"A\", \"A0\", \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"))## Inference for Stan model: anon_model.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## lambda[1]  1.00       0 0.00  1.00  1.00  1.00  1.00  1.00  3771 1.00\n## lambda[2]  0.87       0 0.06  0.77  0.83  0.87  0.91  0.99   563 1.00\n## lambda[3]  0.52       0 0.04  0.44  0.49  0.52  0.55  0.60   852 1.00\n## lambda[4]  1.00       0 0.00  1.00  1.00  1.00  1.00  1.00  4284 1.00\n## lambda[5]  0.96       0 0.06  0.86  0.93  0.96  1.00  1.08   774 1.00\n## tau[1]     3.33       0 0.04  3.26  3.31  3.33  3.36  3.41  1106 1.00\n## tau[2]     3.90       0 0.03  3.85  3.88  3.90  3.92  3.95   785 1.00\n## tau[3]     4.60       0 0.02  4.56  4.58  4.60  4.61  4.64  1382 1.00\n## tau[4]     3.04       0 0.04  2.95  3.01  3.03  3.06  3.11   905 1.00\n## tau[5]     3.71       0 0.04  3.65  3.69  3.71  3.74  3.79   809 1.00\n## psi[1]     0.61       0 0.02  0.56  0.59  0.61  0.62  0.65  2032 1.00\n## psi[2]     0.32       0 0.02  0.28  0.31  0.32  0.33  0.36  1043 1.00\n## psi[3]     0.36       0 0.01  0.33  0.35  0.36  0.36  0.38  3460 1.00\n## psi[4]     0.57       0 0.03  0.52  0.55  0.57  0.58  0.62  1730 1.00\n## psi[5]     0.42       0 0.03  0.37  0.41  0.42  0.44  0.47   952 1.00\n## R[1,1]     1.00     NaN 0.00  1.00  1.00  1.00  1.00  1.00   NaN  NaN\n## R[1,2]     0.86       0 0.03  0.80  0.84  0.86  0.88  0.91   485 1.00\n## R[2,1]     0.86       0 0.03  0.80  0.84  0.86  0.88  0.91   485 1.00\n## R[2,2]     1.00     NaN 0.00  1.00  1.00  1.00  1.00  1.00   NaN  NaN\n## A[1]       0.60       0 0.04  0.53  0.58  0.60  0.62  0.67   651 1.00\n## A[2]       0.71       0 0.04  0.64  0.68  0.70  0.73  0.78   985 1.00\n## A0[1,1]    0.60       0 0.04  0.53  0.58  0.60  0.62  0.67   651 1.00\n## A0[1,2]    0.00     NaN 0.00  0.00  0.00  0.00  0.00  0.00   NaN  NaN\n## A0[2,1]    0.61       0 0.04  0.53  0.58  0.61  0.63  0.68  1143 1.00\n## A0[2,2]    0.36       0 0.04  0.29  0.34  0.36  0.39  0.43   439 1.01\n## phi[1,1]   0.60       0 0.04  0.53  0.58  0.60  0.62  0.67   651 1.00\n## phi[1,2]   0.56       0 0.03  0.49  0.54  0.56  0.58  0.62  1003 1.00\n## phi[2,1]   0.56       0 0.03  0.49  0.54  0.56  0.58  0.62  1003 1.00\n## phi[2,2]   0.71       0 0.04  0.64  0.68  0.70  0.73  0.78   985 1.00\n## ksi[1,1]  -0.22       0 0.23 -0.69 -0.38 -0.22 -0.07  0.24  5222 1.00\n## ksi[1,2]  -0.37       0 0.28 -0.91 -0.56 -0.37 -0.18  0.17  5120 1.00\n## ksi[8,1]   0.91       0 0.23  0.44  0.76  0.91  1.06  1.36  3775 1.00\n## ksi[8,2]   0.87       0 0.28  0.35  0.68  0.87  1.06  1.42  5047 1.00\n## \n## Samples were drawn using NUTS(diag_e) at Sun Jun 19 23:01:08 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n# plot the posterior in a\n#  95% probability interval\n#  and 80% to contrast the dispersion\nplot(fit,pars =c(\"lambda\", \"tau\", \"psi\",\n                 \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"))## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)\n# traceplots\nrstan::traceplot(fit,\n      pars =c(\"lambda\", \"tau\", \"psi\",\n              \"phi\",\n              \"ksi[1, 1]\", \"ksi[1, 2]\",\n              \"ksi[8, 1]\", \"ksi[8, 2]\"), inc_warmup = TRUE)\n# Gelman-Rubin-Brooks Convergence Criterion\nggs_grb(ggs(fit, family = c(\"lambda\"))) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_grb(ggs(fit, family = \"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\n# autocorrelation\nggs_autocorrelation(ggs(fit, family=\"lambda\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"tau\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"psi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\nggs_autocorrelation(ggs(fit, family=\"phi\")) +\n   theme_bw() + theme(panel.grid = element_blank())\n# plot the posterior density\nplot.data <- as.matrix(fit)\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(\"lambda[2]\", \"lambda[3]\", \"lambda[5]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = paste0(\"tau[\",1:5,\"]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"psi[\",1:5,\"]\"),\n           \"phi[1,1]\", \"phi[1,2]\", \"phi[2,2]\"),\n  prob = 0.8) +\n  plot_title"},{"path":"confirmatory-factor-analysis.html","id":"blavaan---two-latent-variables","chapter":"9 Confirmatory Factor Analysis","heading":"9.8 Blavaan - Two Latent Variables","text":"","code":"\n# model\nmodel_cfa2_blavaan <- \"\n  f1 =~ 1*PI + AD + IGC \n  f2 =~ 1*FI + FC\n  f1 ~~ f2\n\"\n\ndat <- as.matrix(read.table(\"code/CFA-Two-Latent-Variables/Data/IIS.dat\", header=T))\n\nfit <- blavaan::bcfa(model_cfa2_blavaan, data=dat)## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 0 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 3.527 seconds (Warm-up)\n## Chain 1:                6.484 seconds (Sampling)\n## Chain 1:                10.011 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 0 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 3.806 seconds (Warm-up)\n## Chain 2:                6.57 seconds (Sampling)\n## Chain 2:                10.376 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 0 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 1500 [  0%]  (Warmup)\n## Chain 3: Iteration:  150 / 1500 [ 10%]  (Warmup)\n## Chain 3: Iteration:  300 / 1500 [ 20%]  (Warmup)\n## Chain 3: Iteration:  450 / 1500 [ 30%]  (Warmup)\n## Chain 3: Iteration:  501 / 1500 [ 33%]  (Sampling)\n## Chain 3: Iteration:  650 / 1500 [ 43%]  (Sampling)\n## Chain 3: Iteration:  800 / 1500 [ 53%]  (Sampling)\n## Chain 3: Iteration:  950 / 1500 [ 63%]  (Sampling)\n## Chain 3: Iteration: 1100 / 1500 [ 73%]  (Sampling)\n## Chain 3: Iteration: 1250 / 1500 [ 83%]  (Sampling)\n## Chain 3: Iteration: 1400 / 1500 [ 93%]  (Sampling)\n## Chain 3: Iteration: 1500 / 1500 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 3.375 seconds (Warm-up)\n## Chain 3:                6.44 seconds (Sampling)\n## Chain 3:                9.815 seconds (Total)\n## Chain 3: \n## Computing posterior predictives...\nsummary(fit)## blavaan (0.4-3) results of 1000 samples after 500 adapt/burnin iterations\n## \n##   Number of observations                           500\n## \n##   Number of missing patterns                         1\n## \n##   Statistic                                 MargLogLik         PPP\n##   Value                                      -2174.048       0.000\n## \n## Latent Variables:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##   f1 =~                                                                        \n##     PI                1.000                                                    \n##     AD                0.959    0.070    0.834    1.105    1.002    normal(0,10)\n##     IGC               0.562    0.046    0.477    0.660    1.001    normal(0,10)\n##   f2 =~                                                                        \n##     FI                1.000                                                    \n##     FC                1.006    0.062    0.893    1.135    1.001    normal(0,10)\n## \n## Covariances:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##   f1 ~~                                                                        \n##     f2                0.312    0.035    0.249    0.384    1.001     lkj_corr(1)\n## \n## Intercepts:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .PI                3.334    0.037    3.262    3.408    1.000    normal(0,32)\n##    .AD                3.898    0.027    3.844    3.949    1.000    normal(0,32)\n##    .IGC               4.596    0.021    4.556    4.637    1.000    normal(0,32)\n##    .FI                3.034    0.040    2.955    3.112    1.000    normal(0,32)\n##    .FC                3.713    0.036    3.642    3.782    1.000    normal(0,32)\n##     f1                0.000                                                    \n##     f2                0.000                                                    \n## \n## Variances:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .PI                0.381    0.030    0.325    0.445    1.002 gamma(1,.5)[sd]\n##    .AD                0.076    0.013    0.052    0.102    1.002 gamma(1,.5)[sd]\n##    .IGC               0.117    0.008    0.101    0.134    1.000 gamma(1,.5)[sd]\n##    .FI                0.322    0.030    0.266    0.385    1.002 gamma(1,.5)[sd]\n##    .FC                0.151    0.023    0.105    0.198    1.000 gamma(1,.5)[sd]\n##     f1                0.311    0.041    0.236    0.397    1.001 gamma(1,.5)[sd]\n##     f2                0.465    0.050    0.375    0.569    1.001 gamma(1,.5)[sd]\nplot(fit)"},{"path":"confirmatory-factor-analysis.html","id":"indeterminacy-in-one-factor-cfa","chapter":"9 Confirmatory Factor Analysis","heading":"9.9 Indeterminacy in One Factor CFA","text":"","code":"\n# model code\njags.model.cfa.ind <- function(){\n\n  ######################################################################\n  # Specify the factor analysis measurement model for the observables\n  ######################################################################\n  for (i in 1:n){\n    for(j in 1:J){\n      mu[i,j] <- tau[j] + ksi[i]*lambda[j]      # model implied expectation for each observable\n      x[i,j] ~ dnorm(mu[i,j], inv.psi[j])    # distribution for each observable\n    }\n  }\n  \n  \n  ######################################################################\n  # Specify the (prior) distribution for the latent variables\n  ######################################################################\n  for (i in 1:n){\n    ksi[i] ~ dnorm(kappa, inv.phi)  # distribution for the latent variables\n  }\n  \n  \n  ######################################################################\n  # Specify the prior distribution for the parameters that govern the latent variables\n  ######################################################################\n  kappa <- 0              # Mean of factor 1\n  inv.phi <-1   # Precision of factor 1\n  phi <- 1/inv.phi        # Variance of factor 1\n  \n  \n  ######################################################################\n  # Specify the prior distribution for the measurement model parameters\n  ######################################################################\n  for(j in 1:J){\n    tau[j] ~ dnorm(3, .1)        # Intercepts for observables\n    inv.psi[j] ~ dgamma(5, 10) # Precisions for observables\n    psi[j] <- 1/inv.psi[j]   # Variances for observables\n  }\n  \n  for (j in 1:J){\n    lambda[j] ~ dnorm(1, .1)    # prior distribution for the remaining loadings\n  }\n}\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  n = 500, J = 5,\n  x = as.matrix(dat)\n)\n\n\n# initial values\nstart_values <- list(\n  list(\"tau\"=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00),\n       \"lambda\"=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00),\n       \"inv.psi\"=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00)),\n  list(\"tau\"=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00),\n       \"lambda\"=c(-3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00, -3.00E+00),\n       \"inv.psi\"=c(2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00, 2.00E+00))\n)\n\n# vector of all parameters to save\n# exclude fixed lambda since it throws an error in\n# in the GRB plot\nparam_save <- c(\"tau[1]\", \"lambda[1]\", \"phi\", \"psi[1]\", \"ksi[8]\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.cfa.ind,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=5000,\n  n.burnin = 2500,\n  n.chains = 2,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 2500\n##    Unobserved stochastic nodes: 515\n##    Total graph size: 8029\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpKotSbY/model4ac44cdd3712.txt\", fit using jags,\n##  2 chains, each with 5000 iterations (first 2500 discarded)\n##  n.sims = 5000 iterations saved\n##            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%   Rhat n.eff\n## ksi[8]       0.004   1.402   -1.983   -1.320   -0.021    1.340    2.031  8.145     2\n## lambda[1]   -0.002   0.597   -0.650   -0.590   -0.004    0.588    0.651 20.033     2\n## phi          1.000   0.000    1.000    1.000    1.000    1.000    1.000  1.000     1\n## psi[1]       0.381   0.028    0.330    0.361    0.380    0.399    0.439  1.004   530\n## tau[1]       3.333   0.039    3.257    3.307    3.333    3.359    3.408  1.001  5000\n## deviance  3384.458  61.581 3302.836 3354.355 3382.730 3411.749 3470.650  1.002  1300\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 1895.3 and DIC = 5279.7\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\nR2jags::traceplot(jags.mcmc)"},{"path":"model-evaluation.html","id":"model-evaluation","chapter":"10 Model Evaluation","heading":"10 Model Evaluation","text":"goal evaluating model determine inferences suggested model reasonable based one’s content area knowledge.\ntext (BDA3) say evaluation reasonableness really form prior, \n“Gelman et al. (2013) argued tha analysts deem posterior distribution inferences unreasonable, really expressing additional information available included analysis.”\nknowledge posterior distribution look like can extremely helpful developing model takes shape (e.g., setting reasonable boundaries parameters).Three aspects/questions aim address part logic model checking:going data, possibly relation model?model say going ?going data compared model says going ?Model evaluation accomplished throughResidual analysis,Posterior predictive distributions, andModel comparisons.","code":""},{"path":"model-evaluation.html","id":"residual-analysis","chapter":"10 Model Evaluation","heading":"10.1 Residual Analysis","text":"residual key component analysis many statistical procedures.\n, describe important feature can used Bayesian psychometric analysis.\nfundamental level, residual one component data-model relationship, \n\\[DATA = MODEL + RESIDUALS.\\]\nview highlights three major pieces going use model evaluation process.\ncan rewrite focus residuals well.factor analysis (CFA particular), tend interested residuals covariances among variables factor model hypothesis observed variables interrelated.\nobserved data, capture relationship observed sample covariance matrix \\(\\mathbf{S}\\), variable \\(j\\) \\(k\\) usually computed \n\\[s_{jk} = \\frac{\\sum_{\\forall (x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)}}{n-1}.\\]\n, whole covariance matrix \\(\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\prime}\\mathbf{X}\\) \\(\\mathbf{X}\\) centered form.CFA, can get model implied covariance matrix using estimated parameters get \\(\\Sigma(\\theta)\\), \n\\[\\Sigma(\\theta) = \\Lambda\\Phi\\Lambda^{\\prime} + \\Psi.\\]\n, get residual matrix simple find difference scores (\\(\\mathbf{E}\\)) matrices\n\\[\\mathbf{E} = \\mathbf{S} - \\Sigma(\\theta).\\]\nAlthough, probably want rescale \\(\\mathbf{S}\\) \\(\\Sigma(\\theta)\\) correlation matrices scale covariances variances likely primary interest.\nmeans can use residuals correlations instead.Generating residual correlations accomplished using posterior predictive distribution.","code":""},{"path":"model-evaluation.html","id":"posterior-predictive-distributions","chapter":"10 Model Evaluation","heading":"10.2 Posterior Predictive Distributions","text":"posterior predictive distribution used heavily model evaluation.(look Bayes notes)Basically, posterior predictive distribution values observed data (\\(Y\\)) mostly likely given posterior distribution.","code":""},{"path":"model-evaluation.html","id":"example-of-posterior-predictive-distribution-of-correlations","chapter":"10 Model Evaluation","heading":"10.2.1 Example of posterior predictive distribution of correlations","text":"example, use correlations observed variable function interest.","code":"\n# model code\njags.model.cfa <- function(){\n\n#\n# Specify the factor analysis measurement\n# model for the observables\n#\n  for (i in 1:n){\n    for(j in 1:J){\n    # model implied expectation for each observable\n      mu[i,j] <- tau[j] + ksi[i]*lambda[j]\n      # distribution for each observable\n      x[i,j] ~ dnorm(mu[i,j], inv.psi[j])    \n    }\n  }\n  \n########################################\n  # Specify the (prior) distribution for \n  # the latent variables\n  ########################################\n  for (i in 1:n){\n    # distribution for the latent variables\n    ksi[i] ~ dnorm(kappa, inv.phi)  \n  }\n  \n  \n  ########################################\n  # Specify the prior distribution for the\n  # parameters that govern the latent variables\n  ########################################\n  kappa <- 0              # Mean of factor 1\n  inv.phi ~ dgamma(5, 10) # Precision of factor 1\n  phi <- 1/inv.phi        # Variance of factor 1\n  \n  \n  ########################################\n  # Specify the prior distribution for the\n  # measurement model parameters\n  ########################################\n  for(j in 1:J){\n    tau[j] ~ dnorm(3, .1)        # Intercepts for observables\n    inv.psi[j] ~ dgamma(5, 10) # Precisions for observables\n    psi[j] <- 1/inv.psi[j]   # Variances for observables\n  }\n  \n  lambda[1] <- 1.0              # loading fixed to 1.0 \n  for (j in 2:J){\n    lambda[j] ~ dnorm(1, .1)    # prior distribution for the remaining loadings\n  }\n\n}\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  n = 500, J = 5,\n  x = as.matrix(dat)\n)\n\n# vector of all parameters to save\nparam_save <- c(\"tau\", paste0(\"lambda[\",1:5,\"]\"), \"phi\", \"psi\")\n\n# fit model\nfit <- jags(\n  model.file=jags.model.cfa,\n  data=mydata,\n  parameters.to.save = param_save,\n  n.iter=15000,\n  n.burnin = 5000,\n  n.chains = 1, # for simplicity\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 2500\n##    Unobserved stochastic nodes: 515\n##    Total graph size: 8029\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model4544238f163c.txt\", fit using jags,\n##  1 chains, each with 15000 iterations (first 5000 discarded)\n##  n.sims = 10000 iterations saved\n##            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%\n## lambda[1]    1.000   0.000    1.000    1.000    1.000    1.000    1.000\n## lambda[2]    0.730   0.045    0.645    0.699    0.729    0.760    0.820\n## lambda[3]    0.420   0.037    0.349    0.395    0.420    0.445    0.493\n## lambda[4]    1.050   0.065    0.928    1.005    1.049    1.094    1.182\n## lambda[5]    0.985   0.059    0.873    0.944    0.983    1.024    1.104\n## phi          0.435   0.042    0.358    0.406    0.433    0.462    0.524\n## psi[1]       0.373   0.028    0.322    0.353    0.373    0.392    0.433\n## psi[2]       0.182   0.014    0.156    0.172    0.181    0.191    0.211\n## psi[3]       0.180   0.012    0.157    0.171    0.179    0.187    0.205\n## psi[4]       0.378   0.030    0.322    0.357    0.377    0.398    0.439\n## psi[5]       0.266   0.022    0.226    0.251    0.265    0.280    0.311\n## tau[1]       3.333   0.040    3.254    3.306    3.332    3.360    3.410\n## tau[2]       3.898   0.028    3.843    3.878    3.897    3.917    3.953\n## tau[3]       4.596   0.023    4.551    4.580    4.596    4.611    4.640\n## tau[4]       3.034   0.041    2.955    3.007    3.033    3.062    3.115\n## tau[5]       3.712   0.037    3.641    3.687    3.713    3.737    3.784\n## deviance  3379.879  42.773 3298.914 3350.714 3379.396 3407.878 3466.261\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 914.8 and DIC = 4294.7\n## DIC is an estimate of expected predictive error (lower deviance is better).\nplot(fit)\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\n\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"tau[\",1:5,\"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = paste0(\"lambda[\",1:5,\"]\"),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"psi[\", 1:5, \"]\"), \"phi\"),\n  prob = 0.8) +\n  plot_title\n# compute model implied covariance/correlations\n# for each iterations\nout.mat <- matrix(ncol=10,nrow=nrow(plot.data))\ncolnames(out.mat) <- c(\"r12\", \"r13\", \"r14\", \"r15\", \"r23\", \"r24\", \"r25\", \"r34\",\"r35\", \"r45\")\nplot.data1 <- cbind(plot.data,out.mat)\n\n# compute the model implied correlations for each iterations\ni <- 1\nfor(i in 1:nrow(plot.data1)){\n  x <- plot.data1[i,]\n  x <- unlist(x)\n  lambda <- matrix(x[4:8], ncol=1)\n  phi <- matrix(x[9], ncol=1)\n  psi <- diag(x[10:14], ncol=5, nrow=5)\n  micov <- lambda%*%phi%*%t(lambda)+psi\n  D <- diag(sqrt(diag(micov)), ncol=5, nrow=5)\n  Dinv <- solve(D)\n  micor <- Dinv%*%micov%*%Dinv\n  outr <- micor[lower.tri(micor)]\n  # combine\n  plot.data1[i,20:29] <- outr\n}\n\nobs.mat <- matrix(cor(dat)[lower.tri(cor(dat))],byrow=T,\n                  ncol=10,nrow=nrow(plot.data))\ncolnames(obs.mat) <- c(\"ObsR12\", \"ObsR13\", \"ObsR14\", \"ObsR15\", \"ObsR23\", \"ObsR24\", \"ObsR25\", \"ObsR34\",\"ObsR35\", \"ObsR45\")\nplot.data1 <- cbind(plot.data1,obs.mat)\n\n\ntheme_set(theme_classic())\nt1 <- grid::textGrob('PI')\nt2 <- grid::textGrob('AD')\nt3 <- grid::textGrob('IGC')\nt4 <- grid::textGrob('FI')\nt5 <- grid::textGrob('FC')\np12 <- ggplot(plot.data1) +\n  geom_density(aes(x=r12))+\n  geom_vline(aes(xintercept = ObsR12), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np13 <- ggplot(plot.data1) +\n  geom_density(aes(x=r13))+\n  geom_vline(aes(xintercept = ObsR13), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np14 <- ggplot(plot.data1) +\n  geom_density(aes(x=r14))+\n  geom_vline(aes(xintercept = ObsR14), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np15 <- ggplot(plot.data1) +\n  geom_density(aes(x=r15))+\n  geom_vline(aes(xintercept = ObsR15), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np23 <- ggplot(plot.data1) +\n  geom_density(aes(x=r23))+\n  geom_vline(aes(xintercept = ObsR23), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np24 <- ggplot(plot.data1) +\n  geom_density(aes(x=r24))+\n  geom_vline(aes(xintercept = ObsR24), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np25 <- ggplot(plot.data1) +\n  geom_density(aes(x=r25))+\n  geom_vline(aes(xintercept = ObsR25), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np34 <- ggplot(plot.data1) +\n  geom_density(aes(x=r34))+\n  geom_vline(aes(xintercept = ObsR34), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np35 <- ggplot(plot.data1) +\n  geom_density(aes(x=r35))+\n  geom_vline(aes(xintercept = ObsR35), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\np45 <- ggplot(plot.data1) +\n  geom_density(aes(x=r45))+\n  geom_vline(aes(xintercept = ObsR45), linetype=\"dashed\")+\n  lims(x=c(0.25, 0.75)) + \n  labs(x=NULL,y=NULL) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(), axis.line.y=element_blank())\n\nlayout <- '\nA####\nBC###\nDEF##\nGHIJ#\nKLMNO\n'\nwrap_plots(A=t1,C=t2,F=t3,J=t4,O=t5,\n           B=p12,D=p13,G=p14,K=p15,\n           E=p23,H=p24,L=p25,\n           I=p34,M=p35,\n           N=p45,design = layout)"},{"path":"model-evaluation.html","id":"ppd-srmr","chapter":"10 Model Evaluation","heading":"10.2.2 PPD SRMR","text":"","code":"\n# model code\njags.model.cfa <- function(){\n\n  ########################################\n  # Specify the factor analysis measurement\n  # model for the observables\n  ########################################\n  for (i in 1:n){\n    for(j in 1:J){\n      # model implied expectation for each observable\n      mu[i,j] <- tau[j] + ksi[i]*lambda[j]\n      # distribution for each observable\n      x[i,j] ~ dnorm(mu[i,j], inv.psi[j])  \n      # Posterior Predictive Distribution of x\n      #   needed for SRMR\n      #   set mean to 0\n      # x.ppd[i,j] ~ dnorm(0, inv.psi[j])  \n    }\n  }\n  \n  ########################################\n  # Specify the (prior) distribution for \n  # the latent variables\n  ########################################\n  for (i in 1:n){\n    # distribution for the latent variables\n    ksi[i] ~ dnorm(kappa, inv.phi)  \n  }\n  \n  \n  ########################################\n  # Specify the prior distribution for the\n  # parameters that govern the latent variables\n  ########################################\n  kappa <- 0              # Mean of factor 1\n  inv.phi ~ dgamma(5, 10) # Precision of factor 1\n  phi <- 1/inv.phi        # Variance of factor 1\n  \n  \n  ########################################\n  # Specify the prior distribution for the\n  # measurement model parameters\n  ########################################\n  for(j in 1:J){\n    tau[j] ~ dnorm(3, .1)        # Intercepts for observables\n    inv.psi[j] ~ dgamma(5, 10) # Precisions for observables\n    psi[j] <- 1/inv.psi[j]   # Variances for observables\n  }\n  \n  lambda[1] <- 1.0              # loading fixed to 1.0 \n  for (j in 2:J){\n    lambda[j] ~ dnorm(1, .1)    # prior distribution for the remaining loadings\n  }\n\n}\n# data must be in a list\ndat <- read.table(\"code/CFA-One-Latent-Variable/Data/IIS.dat\", header=T)\n\nmydata <- list(\n  n = 500, J = 5,\n  x = as.matrix(dat)\n)\n\n# vector of all parameters to save\nparam_save <- c(\"tau\", paste0(\"lambda[\",1:5,\"]\"), \"phi\", \"psi\")# \"Sigma\",\n\n# fit model\nfit <- jags(\n  model.file=jags.model.cfa,\n  data=mydata,\n  #inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=10000,\n  n.burnin = 5000,\n  n.chains = 1,\n  n.thin=1,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 2500\n##    Unobserved stochastic nodes: 515\n##    Total graph size: 8029\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model4544704352f.txt\", fit using jags,\n##  1 chains, each with 10000 iterations (first 5000 discarded)\n##  n.sims = 5000 iterations saved\n##            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%\n## lambda[1]    1.000   0.000    1.000    1.000    1.000    1.000    1.000\n## lambda[2]    0.730   0.044    0.647    0.699    0.729    0.759    0.820\n## lambda[3]    0.420   0.037    0.349    0.395    0.419    0.445    0.495\n## lambda[4]    1.052   0.067    0.925    1.006    1.051    1.096    1.188\n## lambda[5]    0.984   0.058    0.874    0.943    0.983    1.022    1.102\n## phi          0.435   0.042    0.360    0.405    0.433    0.463    0.520\n## psi[1]       0.373   0.028    0.322    0.354    0.372    0.392    0.432\n## psi[2]       0.182   0.014    0.157    0.173    0.182    0.191    0.211\n## psi[3]       0.180   0.012    0.157    0.171    0.179    0.188    0.205\n## psi[4]       0.377   0.030    0.322    0.357    0.376    0.396    0.439\n## psi[5]       0.266   0.022    0.225    0.251    0.266    0.281    0.311\n## tau[1]       3.333   0.041    3.253    3.306    3.333    3.360    3.412\n## tau[2]       3.897   0.029    3.840    3.877    3.898    3.917    3.954\n## tau[3]       4.596   0.023    4.552    4.581    4.596    4.611    4.640\n## tau[4]       3.033   0.042    2.952    3.005    3.034    3.062    3.115\n## tau[5]       3.712   0.037    3.638    3.686    3.713    3.737    3.784\n## deviance  3381.034  41.985 3299.407 3352.431 3380.812 3409.296 3463.769\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 881.4 and DIC = 4262.4\n## DIC is an estimate of expected predictive error (lower deviance is better).\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)[[1]]\n\n# function for estimating SRMR\nSRMR.function <- function(data.cov.matrix, mod.imp.cov.matrix){\n\n  J=nrow(data.cov.matrix)\n    temp <- matrix(NA, nrow=J, ncol=J)\n\n    for(j in 1:J){\n        for(jprime in 1:j){\n            temp[j, jprime] <- ((data.cov.matrix[j, jprime] - mod.imp.cov.matrix[j, jprime])/(data.cov.matrix[j, j] * data.cov.matrix[jprime, jprime]))^2\n        }\n    }\n\n    SRMR <- sqrt((2*sum(temp,na.rm=TRUE))/(J*(J+1)))\n    SRMR\n\n}\n\n# set up the parameters for \n#   (1) model implied covariance\n#   (2) PPD of x/covariance\niter <- nrow(jags.mcmc)\nsrmr.realized <- rep(NA, iter)\nsrmr.ppd      <- rep(NA, iter)\nsrmr.rpv      <- rep(NA, iter)\njags.mcmc <- cbind(jags.mcmc, srmr.realized, srmr.ppd, srmr.rpv)\n\nN <- 500; J <- 5; M <- 1\ncov.x <- cov(mydata$x)\ni <- 1\nfor(i in 1:iter){\n  # set up parameters\n  x <- jags.mcmc[i,]\n  lambda <- matrix(x[2:6], ncol=M, nrow=J)\n  phi    <- matrix(x[7], ncol=M, nrow=M)\n  psi    <- diag(x[8:12], ncol=J, nrow=J)\n  # estimate model implied covariance matrix\n  cov.imp <- lambda%*%phi%*%t(lambda) + psi\n  # get posterior predicted observed x\n  x.ppd   <- mvtnorm::rmvnorm(N, mean=rep(0, J), sigma=cov.imp)\n  # compute posterior predictied covariance matrix\n  cov.ppd <- cov(x.ppd)\n  # estimate SRMR values \n  jags.mcmc[i,18] <- SRMR.function(cov.x, cov.imp)# srmr realized\n  jags.mcmc[i,19] <- SRMR.function(cov.ppd, cov.imp)# srmr ppd\n  # posterior predicted p-value of realized SRMR being <= 0.08.\n  jags.mcmc[i,20] <- ifelse(jags.mcmc[i,18]<=0.08, 1, 0) \n}\n\nplot.dat <- as.data.frame(jags.mcmc)\n\np1 <- ggplot(plot.dat, aes(x=srmr.realized, y=srmr.ppd))+\n  geom_point()+\n  geom_abline(slope=1, intercept = 0)+\n  lims(x=c(0,1),y=c(0,1))+\n  labs(x=\"Realized SRMR\",y=\"Posterior Predicted SRMR\") +\n  theme_bw()+theme(panel.grid = element_blank())\np2 <- ggplot(plot.dat, aes(x=srmr.realized))+\n  geom_density()+\n  lims(x=c(0,1))+\n  labs(x=\"Realized SRMR\", y=NULL) + \n  annotate(\"text\", x = 0.75, y = 3,\n           label = paste0(\"Pr(SRMR <= 0.08)= \",\n                          round(mean(plot.dat$srmr.rpv), 2))) +\n  theme_bw()+theme(panel.grid = element_blank(),\n                   axis.text.y = element_blank(),\n                   axis.ticks.y = element_blank())\np1 + p2"},{"path":"model-evaluation.html","id":"model-comparison","chapter":"10 Model Evaluation","heading":"10.3 Model Comparison","text":"Bayes Factors (BF)Conditional predictive ordinate (CPO)Information criteriaEntropy","code":""},{"path":"item-response-theory.html","id":"item-response-theory","chapter":"11 Item Response Theory","heading":"11 Item Response Theory","text":"Item response theory (IRT) builds models item (stimuli measures collected) based two broad classes modelsModels dichotomous (binary, 0/1) items andModels polytomous (multi-category) items.","code":""},{"path":"item-response-theory.html","id":"irt-models-for-dichotomous-data","chapter":"11 Item Response Theory","heading":"11.1 IRT Models for Dichotomous Data","text":"First, conventional dichotomous observed variables, IRT model can generally specified follows.Let \\(x_{ij}\\) observed value respondent \\(\\) observable (item) \\(j\\).\n\\(x\\) binary, observed value can \\(0\\) \\(1\\).\ncommon IRT models binary observed variables can expressed version \\[p(x_{ij} = 1 \\mid \\theta_i, d_j, a_j, c_j) = c_j + (1-c_j)F(a_j \\theta_i + d_j),\\],\\(\\theta_i\\) magnitude latent variable individual \\(\\) possesses. educational measurement, \\(\\theta_i\\) commonly represents proficiency higher \\(\\theta_i\\) means individual trait,\\(d_j\\) item location difficulty parameter. \\(d_j\\) commonly transformed \\(d_j = -a_jb_j\\) location parameter easier interpret relation latent trait \\(\\theta_i\\),\\(a_j\\) item slope discrimination parameter,\\(c_j\\) item lower asymptote guessing parameter,\\(F(.)\\) link function specified determines form transformation latent trait item response probability. link common chosen either logistic link normal-ogive link.Common IRT models the1-PL, one-parameter logistic model, uses one measurement parameter \\(d_j\\) per item,2-PL, two-parameter logistic model, uses two measurement model parameters \\(a_j\\) \\(d_j\\) per item,3-PL, three-parameter logistic model, uses three parameters shown .models also possible binary item response formats omitted .describes functional form used model individual may greater lesser likelihood endorsing item (\\(1\\) measure).\nuse model basis defining conditional probability response given values parameters.\nconditional probability commonly used part marginal maximum likelihood (MML) approach finding parameters values measurement model maximize likelihood.\nHowever, given values latent variable \\(\\theta_i\\) also unknown, distribution \\(\\theta_i\\) marginalized likelihood function.However, Bayesian formulation, can side-step issues use prior distributions.\nStarting general form likelihood function\n\\[p(\\mathbf{x}\\mid \\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = \\prod_{=1}^np(\\mathbf{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\]\n\n\\[x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j \\sim \\mathrm{Bernoulli}[p(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j)].\\]Developing joint prior distribution \\(p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega})\\) straightforward given high dimensional aspect components.\n, common assumption distribution latent variables (\\(\\boldsymbol{\\theta}\\)) independent distribution measurement model parameters (\\(\\boldsymbol{\\omega}\\)).\n, can separate problem independent priors\n\\[p(\\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = p(\\boldsymbol{\\theta})p(\\boldsymbol{\\omega}).\\]latent variables, prior distributuion generally build assuming individuals also independent.\nindependence observations leads joint prior product priors common distribution,\n\\[p(\\boldsymbol{\\theta}) = \\prod_{=1}^np(\\theta_i\\mid \\boldsymbol{\\theta}_p),\\]\n\\(\\boldsymbol{\\theta}_p\\) hyperparameters governing common prior latent variable distribution.\ncommon choice \\(\\theta_i \\sim \\mathrm{Normal}(\\mu_{\\theta} = 0, \\sigma^2_{\\theta}=1)\\) distribution arbitrary.measurement model parameters, bit complex specification generally needed.\nOne simple approach invoke exchangeability assumption among items among item parameters.\nessentially make priors independent simplify specification product univariate priors measurement model parameters\n\\[p(\\boldsymbol{\\omega}) = \\prod_{j=1}^Jp(\\boldsymbol{\\omega}_j)=\\prod_{j=1}^Jp(d_j)p(a_j)p(c_j).\\]\nlocation parameter (\\(d_j\\)), common prior distribution unbounded normal distribution.\n, location can take value within range latent variable also technically unbounded let\n\\[d_j \\sim \\mathrm{Normal}(\\mu_{d},\\sigma^2_d).\\]\nchoice hyperparameters can guided prior research set common relative diffuse value items \\(\\mu_{d}=0,\\sigma^2_d=10\\).discrimination parameter governs strength relationship latent variable probability endorsing item.\nsimilar flavor factor loading CFA.\nissue specifying prior discrimination parameter indeterminacy respect orientation latent variable.\nCFA, resolved orientation indeterminacy issue fixing one factor loading 1.\nIRT, can constraining possible values discrimination parameters strictly positive.\nforces item meaning higher values latent variable directly (least proportionally) increase probability endorsing item.\nachieve using prior \n\\[a_j \\sim \\mathrm{Normal}^{+}(\\mu_a,\\sigma^2_a).\\]\nterm \\(\\mathrm{Normal}^{+}(.)\\) means normal distribution truncated 0 positive values possible.Lastly, guessing parameter \\(c_j\\) takes values \\([0,1]\\).\ncommon choice parameters bounded 0 1 Beta prior, \n\\[c_j \\sim \\mathrm{Beta}(\\alpha_c, \\beta_c).\\]\nhyperparameters \\(\\alpha_c\\) \\(\\beta_c\\) determine shape beta prior affect likelihood magnitude guessing parameters.","code":""},{"path":"item-response-theory.html","id":"pl-lsat-example","chapter":"11 Item Response Theory","heading":"11.2 3-PL LSAT Example","text":"Law School Admission Test (LSAT) example (p. 263-271), data 1000 examinees responding five items just subset LSAT.\nhypothesize one underlying latent variable measured items.\nguessing also plausible.\nfull 3-PL model use can described equation \n\\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{}, \\boldsymbol{c} \\mid \\mathbf{x}) \\propto \\prod_{=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)p(\\theta_i)p(d_j)p(a_j)p(c_j),\\]\n\n\\[\\begin{align*}\nx_{ij}\\mid\\theta_i\\mid\\theta_i, d_j, a_j, c_j &\\sim \\mathrm{Bernoulli}[p(\\theta_i\\mid\\theta_i, d_j, a_j, c_j)],\\ \\mathrm{}\\ =1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\\np(\\theta_i\\mid\\theta_i, d_j, a_j, c_j) &= c_j + (1-c_j)\\Phi(a_j\\theta_j + d_j),\\ \\mathrm{}\\ =1, \\cdots, 100,\\ j = 1, \\cdots, 5;\\\\\n\\theta_i &\\sim \\mathrm{Normal}(0,1),\\ \\mathrm{}\\ = 1, \\cdots, 1000;\\\\\nd_j &\\sim \\mathrm{Normal}(0, 2),\\ \\mathrm{}\\ j=1, \\cdots, 5;\\\\\na_j &\\sim \\mathrm{Normal}^{+}(1, 2),\\ \\mathrm{}\\ j=1, \\cdots, 5;\\\\\nc_j &\\sim \\mathrm{Beta}(5, 17),\\ \\mathrm{}\\ j=1, \\cdots, 5.\n\\end{align*}\\]model can illustrated DAG shown .\nFigure 11.1: DAG 3-PL IRT model LSAT Example\npath diagram IRT essentially identical path diagram CFA model.\nfact highlights important feature IRT/CFA major conceptual difference approaches define link latent variable observed items.\nFigure 11.2: Path diagram 3-PL IRT model\ncompleteness, included model specification diagram concretely connects DAG path diagram assumed distributions priors.\nFigure 11.3: Model specification diagram 3-PL IRT model\n","code":""},{"path":"item-response-theory.html","id":"lsat-example---jags","chapter":"11 Item Response Theory","heading":"11.3 LSAT Example - JAGS","text":"","code":"\njags.model.lsat <- function(){\n\n#########################################\n# Specify the item response measurement model for the observables\n#########################################\nfor (i in 1:n){\n  for(j in 1:J){\n    P[i,j] <- c[j]+(1-c[j])*phi(a[j]*theta[i]+d[j])       # 3P-NO expression\n    x[i,j] ~ dbern(P[i,j])                  # distribution for each observable\n  }\n}\n\n\n##########################################\n# Specify the (prior) distribution for the latent variables\n##########################################\nfor (i in 1:n){\n  theta[i] ~ dnorm(0, 1)  # distribution for the latent variables\n}\n\n\n##########################################\n# Specify the prior distribution for the measurement model parameters\n##########################################\nfor(j in 1:J){\n  d[j] ~ dnorm(0, .5)          # Locations for observables\n  a[j] ~ dnorm(1, .5); I(0,)    # Discriminations for observables\n  c[j] ~ dbeta(5,17)           # Lower asymptotes for observables\n  \n}\n\n\n} # closes the model\n\n# initial values\nstart_values <- list(\n  list(\"d\"=c(1.00, 1.00, 1.00, 1.00, 1.00),\n       \"a\"=c(1.00, 1.00, 1.00, 1.00, 1.00),\n       \"c\"=c(0.20, 0.20, 0.20, 0.20, 0.20)),\n  list(\"d\"=c(-3.00, -3.00, -3.00, -3.00, -3.00),\n       \"a\"=c(3.00, 3.00, 3.00, 3.00, 3.00),\n       \"c\"=c(0.50, 0.50, 0.50, 0.50, 0.50)),\n  list(\"d\"=c(3.00, 3.00, 3.00, 3.00, 3.00),\n       \"a\"=c(0.1, 0.1, 0.1, 0.1, 0.1),\n       \"c\"=c(0.05, 0.05, 0.05, 0.05, 0.05))\n)\n\n# vector of all parameters to save\nparam_save <- c(\"a\", \"c\", \"d\", \"theta\")\n\n# dataset\ndat <- read.table(\"data/LSAT.dat\", header=T)\n\nmydata <- list(\n  n = nrow(dat), J = ncol(dat),\n  x = as.matrix(dat)\n)\n\n# fit model\nfit <- jags(\n  model.file=jags.model.lsat,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=20000,\n  n.iter=26000,\n  n.burnin = 6000,\n  n.chains = 3,\n  progress.bar = \"none\")## Error in jags(model.file = jags.model.lsat, data = mydata, inits = start_values, : formal argument \"n.iter\" matched by multiple actual arguments\nprint(fit)## Error in print(fit): object 'fit' not found\nround(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% \"theta\", ], 3)## Error in eval(expr, envir, enclos): object 'fit' not found\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)## Error in as.mcmc(fit): object 'fit' not found\n# the below two plots are too big to be useful given the 1000 observations.\n#R2jags::traceplot(jags.mcmc)\n\n# gelman-rubin-brook\n#gelman.plot(jags.mcmc)\n\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))## Error in as.data.frame(jags.mcmc[[1]]): object 'jags.mcmc' not found\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))## Error in as.matrix(jags.mcmc, chains = T, iters = T): object 'jags.mcmc' not found\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)## Error in eval(expr, envir, enclos): object 'a' not found\nbayesplot::mcmc_acf(plot.data,pars = c(paste0(\"d[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nbayesplot::mcmc_trace(plot.data,pars = c(paste0(\"d[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nggmcmc::ggs_grb(ggs(jags.mcmc), family=\"d\")## Error in ggs(jags.mcmc): object 'jags.mcmc' not found\nmcmc_areas(plot.data, pars = c(paste0(\"d[\",1:5,\"]\")), prob = 0.8)## Error in posterior::is_draws(x): object 'plot.data' not found\nbayesplot::mcmc_acf(plot.data,pars = c(paste0(\"a[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nbayesplot::mcmc_trace(plot.data,pars = c(paste0(\"a[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nggmcmc::ggs_grb(ggs(jags.mcmc), family=\"a\")## Error in ggs(jags.mcmc): object 'jags.mcmc' not found\nmcmc_areas( plot.data,pars = c(paste0(\"a[\", 1:5, \"]\")), prob = 0.8)## Error in posterior::is_draws(x): object 'plot.data' not found\nbayesplot::mcmc_acf(plot.data,pars = c(paste0(\"c[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nbayesplot::mcmc_trace(plot.data,pars = c(paste0(\"c[\", 1:5, \"]\")))## Error in posterior::is_draws(x): object 'plot.data' not found\nggmcmc::ggs_grb(ggs(jags.mcmc), family=\"c\")## Error in ggs(jags.mcmc): object 'jags.mcmc' not found\nmcmc_areas(plot.data, pars = c(paste0(\"c[\", 1:5, \"]\")), prob = 0.8)## Error in posterior::is_draws(x): object 'plot.data' not found"},{"path":"item-response-theory.html","id":"posterior-predicted-distributions","chapter":"11 Item Response Theory","heading":"11.3.1 Posterior Predicted Distributions","text":", want compare observed expected posterior predicted distributions.Statistical functions interest (1) standardized model-based covariance (SMBC) (2) standardized generalized discrepancy measure (SGDDM).(1), SMBC \n\\[SMBC_{jj^\\prime}=\\frac{\\frac{1}{n}\\sum_{=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}{\\sqrt{\\frac{1}{n}\\sum_{=1}^n(x_{ij} - E(x_{ij} \\mid \\theta_i,\\boldsymbol{\\omega}_j))^2}\\sqrt{\\frac{1}{n}\\sum_{=1}^n(x_{ij^\\prime} - E(x_{ij^\\prime} \\mid \\theta_i,\\boldsymbol{\\omega}_j^\\prime))}}\\]R, functions can used compute qualtities.Next, use functions among basic data wrangling construct full posterior predictive distribution analysis probe resulting posterior.Next, generate plots help summarize describe posterior predictor distributions statistics.","code":"\ncalculate.SGDDM <- function(data.matrix, expected.value.matrix){\n    \n    J.local = ncol(data.matrix)\n\n    SMBC.matrix <- calculate.SMBC.matrix(data.matrix, expected.value.matrix)\n    \n    SGDDM = sum(abs((lower.tri(SMBC.matrix, diag=FALSE))*SMBC.matrix))/((J.local*(J.local-1))/2)\n\n    SGDDM\n\n} # closes calculate.SGDDM\n\ncalculate.SMBC.matrix <- function(data.matrix, expected.value.matrix){\n    \n    N.local <- nrow(data.matrix)\n\n    MBC.matrix <- (t(data.matrix-expected.value.matrix) %*% (data.matrix-expected.value.matrix))/N.local\n\n    MBStddevs.matrix <- diag(sqrt(diag(MBC.matrix)))\n\n    #SMBC.matrix <- solve(MBStddevs.matrix) %*% MBC.matrix %*% solve(MBStddevs.matrix)\n\n\n    J.local <- ncol(data.matrix)\n\n    SMBC.matrix <- matrix(NA, nrow=J.local, ncol=J.local)\n\n    for(j in 1:J.local){\n        for(jj in 1:J.local){\n            SMBC.matrix[j,jj] <- MBC.matrix[j,jj]/(MBStddevs.matrix[j,j]*MBStddevs.matrix[jj,jj])\n        }\n    }\n\n    SMBC.matrix \n\n} # closes calculate.MBC.matrix\n# Data wrangle the results/posterior draws for use\ndatv1 <- plot.data %>%\n  pivot_longer(\n    cols = `a[1]`:`a[5]`,\n    values_to = \"a\",\n    names_to = \"item\"\n  ) %>%\n  mutate(item = substr(item, 3,3)) %>%\n  select(chain, iter, item, a)## Error in pivot_longer(., cols = `a[1]`:`a[5]`, values_to = \"a\", names_to = \"item\"): object 'plot.data' not found\ndatv2 <- plot.data %>%\n  pivot_longer(\n    cols = `c[1]`:`c[5]`,\n    values_to = \"c\",\n    names_to = \"item\"\n  ) %>%\n  mutate(item = substr(item, 3,3)) %>%\n  select(chain, iter, item, c)## Error in pivot_longer(., cols = `c[1]`:`c[5]`, values_to = \"c\", names_to = \"item\"): object 'plot.data' not found\ndatv3 <- plot.data %>%\n  pivot_longer(\n    cols = `d[1]`:`d[5]`,\n    values_to = \"d\",\n    names_to = \"item\"\n  ) %>%\n  mutate(item = substr(item, 3,3)) %>%\n  select(chain, iter, item, d)## Error in pivot_longer(., cols = `d[1]`:`d[5]`, values_to = \"d\", names_to = \"item\"): object 'plot.data' not found\ndatv4 <- plot.data %>%\n  pivot_longer(\n    cols = `theta[1]`:`theta[999]`,\n    values_to = \"theta\",\n    names_to = \"person\"\n  ) %>%\n  select(chain, iter, person, theta)## Error in pivot_longer(., cols = `theta[1]`:`theta[999]`, values_to = \"theta\", : object 'plot.data' not found\ndat_long <- full_join(datv1, datv2)## Error in full_join(datv1, datv2): object 'datv1' not found\ndat_long <- full_join(dat_long, datv3)## Error in full_join(dat_long, datv3): object 'dat_long' not found\ndat_long <- full_join(dat_long, datv4)## Error in full_join(dat_long, datv4): object 'dat_long' not found\ndat1 <- dat\ndat1$person <- paste0(\"theta[\",1:nrow(dat), \"]\")\ndatvl <- dat1 %>%\n  pivot_longer(\n    cols=contains(\"item\"),\n    names_to = \"item\",\n    values_to = \"x\"\n  ) %>%\n  mutate(\n    item = substr(item, 6, 100)\n  )\n\ndat_long <- left_join(dat_long, datvl)## Error in left_join(dat_long, datvl): object 'dat_long' not found\n# compute expected prob\nilogit <- function(x){exp(x)/(1+exp(x))}\ndat_long <- dat_long %>%\n  as_tibble()%>%\n  mutate(\n    x.exp = c + (1-c)*ilogit(a*(theta - d)),\n    x.dif = x - x.exp\n  )## Error in as_tibble(.): object 'dat_long' not found\ndat_long$x.ppd <- apply(\n  dat_long, 1, \n  FUN=function(x){\n    rbern(1, as.numeric(x[10]))\n  }\n  )## Error in apply(dat_long, 1, FUN = function(x) {: object 'dat_long' not found\nitermin <- min(dat_long$iter) # used for subseting## Error in eval(expr, envir, enclos): object 'dat_long' not found\n# figure 11.4\nd <- dat_long %>%\n  group_by(chain, iter, person) %>%\n  summarise(raw.score = sum(x),\n            raw.score.ppd = sum(x.ppd))## Error in group_by(., chain, iter, person): object 'dat_long' not found\ndi <- d[ d$chain ==1 & d$iter == itermin, ] %>%\ndi <- d %>%\n  filter(chain==1, iter==6001) %>%\n  group_by(raw.score) %>%\n  summarise(count = n())## Error in filter(., chain == 1, iter == 6001): object 'd' not found\ndii <- d %>%\n  group_by(chain, iter, raw.score.ppd)%>%\n  summarise(raw.score = n())## Error in group_by(., chain, iter, raw.score.ppd): object 'd' not found\n# overall fit of observed scores\nggplot()+\n  geom_boxplot(data=dii, aes(y=raw.score, x= raw.score.ppd, group=raw.score.ppd))+\n  geom_point(data=di, aes(x=raw.score, y=count), color=\"red\", size=2)+\n  labs(x=\"Raw Score\", y=\"Number of Examinees\")+\n  scale_x_continuous(breaks=0:5)+\n  theme_classic()## Error in fortify(data): object 'dii' not found\n# by item\nd <- dat_long %>%\n  group_by(chain, iter, person) %>%\n  mutate(raw.score = sum(x),\n         raw.score.ppd = sum(x.ppd))## Error in group_by(., chain, iter, person): object 'dat_long' not found\ndi <- d[ d$chain ==1 & d$iter == itermin, ] %>%\ndi <- d %>%\n  filter(chain==1, iter==6001) %>%\n  group_by(raw.score, item) %>%\n  summarise(p.correct = mean(x))## Error in filter(., chain == 1, iter == 6001): object 'd' not found\ndii <- d %>%\n  group_by(chain, iter, raw.score.ppd, item)%>%\n  summarise(p.correct = mean(x.ppd))## Error in group_by(., chain, iter, raw.score.ppd, item): object 'd' not found\nggplot()+\n  geom_boxplot(data=dii,\n               aes(y= p.correct,\n                   x= raw.score.ppd,\n                   group=raw.score.ppd))+\n  geom_point(data=di,\n             aes(x=raw.score, y=p.correct),\n             color=\"red\", size=2)+\n  facet_wrap(.~item)+\n  labs(x=\"Raw Score\", y=\"Number of Examinees\")+\n  theme_classic()## Error in fortify(data): object 'dii' not found\n# computing standardized model summary statistics\n# objects for results\nJ <- 5\nn.chain <- 3\nn.iters <- length(unique(dat_long$iter))## Error in unique(dat_long$iter): object 'dat_long' not found\nn.iters <- length(unique(long_dat$iter))## Error in unique(long_dat$iter): object 'long_dat' not found\nn.iters.PPMC <- n.iters*n.chain## Error in eval(expr, envir, enclos): object 'n.iters' not found\nrealized.SMBC.array <- array(NA, c(n.iters.PPMC, J, J))## Error in array(NA, c(n.iters.PPMC, J, J)): object 'n.iters.PPMC' not found\npostpred.SMBC.array <- array(NA, c(n.iters.PPMC, J, J))## Error in array(NA, c(n.iters.PPMC, J, J)): object 'n.iters.PPMC' not found\nrealized.SGDDM.vector <- array(NA, c(n.iters.PPMC))## Error in array(NA, c(n.iters.PPMC)): object 'n.iters.PPMC' not found\npostpred.SGDDM.vector <- array(NA, c(n.iters.PPMC))## Error in array(NA, c(n.iters.PPMC)): object 'n.iters.PPMC' not found\nii <- i <- c <- 1\n# iteration condiitons\niter.cond <- unique(dat_long$iter)## Error in unique(dat_long$iter): object 'dat_long' not found\nXobs <- as.matrix(dat[,-6])\n\nfor(i in 1:length(iter.cond)){\n  for(c in 1:3){\n  cc <- iter.cond[i]\n  Xexp <- dat_long[dat_long$chain==c & dat_long$iter==cc , ] %>%\n    pivot_wider(\n      id_cols = person,\n      names_from = \"item\",\n      values_from = \"x.exp\",\n      names_prefix = \"item\"\n    ) %>%\n    ungroup()%>%\n    select(item1:item5)%>%\n    as.matrix()\n  Xppd <- dat_long[dat_long$chain==c & dat_long$iter==cc , ] %>%\n    pivot_wider(\n      id_cols = person,\n      names_from = \"item\",\n      values_from = \"x.ppd\",\n      names_prefix = \"item\"\n    ) %>%\n    ungroup()%>%\n    select(item1:item5)%>%\n    as.matrix()\n\n  # compute realized values\n  realized.SMBC.array[ii, ,] <- calculate.SMBC.matrix(Xobs, Xexp)\n  realized.SGDDM.vector[ii] <-  calculate.SGDDM(Xobs, Xexp)\n  # compute PPD values\n  postpred.SMBC.array[ii, ,] <- calculate.SMBC.matrix(Xppd, Xexp)\n  postpred.SGDDM.vector[ii] <-  calculate.SGDDM(Xppd, Xexp)\n    ii <- ii + 1\n  }\n}## Error in eval(expr, envir, enclos): object 'iter.cond' not found\nplot.dat.ppd <- data.frame(\n  real = realized.SGDDM.vector,\n  ppd = postpred.SGDDM.vector\n)## Error in data.frame(real = realized.SGDDM.vector, ppd = postpred.SGDDM.vector): object 'realized.SGDDM.vector' not found\nggplot(plot.dat.ppd, aes(x=real, y=ppd))+\n  geom_point()+\n  geom_abline(intercept = 0, slope=1)+\n  lims(x=c(0,0.5), y=c(0, 0.5))## Error in ggplot(plot.dat.ppd, aes(x = real, y = ppd)): object 'plot.dat.ppd' not found\n# transform smbc into plotable format\nddim <- dim(postpred.SMBC.array)## Error in eval(expr, envir, enclos): object 'postpred.SMBC.array' not found\nplot.dat.ppd <- as.data.frame(matrix(0, nrow=ddim[1]*ddim[2]*ddim[3], ncol=4))## Error in matrix(0, nrow = ddim[1] * ddim[2] * ddim[3], ncol = 4): object 'ddim' not found\ncolnames(plot.dat.ppd) <- c(\"itemj\", \"itemjj\", \"real\", \"ppd\")## Error in colnames(plot.dat.ppd) <- c(\"itemj\", \"itemjj\", \"real\", \"ppd\"): object 'plot.dat.ppd' not found\nii <- i <- j <- jj <- 1\n\nfor(i in 1:ddim[1]){\n  for(j in 1:ddim[2]){\n    for(jj in 1:ddim[3]){\n      plot.dat.ppd[ii, 1] <- j\n      plot.dat.ppd[ii, 2] <- jj\n      plot.dat.ppd[ii, 3] <- realized.SMBC.array[i, j, jj]\n      plot.dat.ppd[ii, 4] <- postpred.SMBC.array[i, j, jj]\n      ii <- ii + 1\n    }\n  }\n}## Error in eval(expr, envir, enclos): object 'ddim' not found\nplot.dat.ppd <- plot.dat.ppd %>%\n  filter(itemj < itemjj) %>%\n  mutate(\n    cov = paste0(\"cov(\", itemj, \", \", itemjj,\")\")\n  )## Error in filter(., itemj < itemjj): object 'plot.dat.ppd' not found\nggplot(plot.dat.ppd, aes(x=real, y=ppd))+\n  geom_point(alpha=0.25)+\n  geom_density2d(adjust=2)+\n  geom_abline(intercept = 0, slope=1)+\n  facet_wrap(.~cov)+\n  lims(x=c(-1,1), y=c(-1,1))+\n  theme_classic()## Error in ggplot(plot.dat.ppd, aes(x = real, y = ppd)): object 'plot.dat.ppd' not found"},{"path":"item-response-theory.html","id":"lsat-example---stan","chapter":"11 Item Response Theory","heading":"11.4 LSAT Example - Stan","text":"-","code":""},{"path":"item-response-theory.html","id":"irt-models-for-polytomous-data","chapter":"11 Item Response Theory","heading":"11.5 IRT Models for Polytomous Data","text":"commonly used IRT model polytomous items graded response model (GRM).\none way describing model.\nLet \\(x_{ij}\\) observed response item \\(j\\) examinee \\(\\) may take values 1, 2, …, \\(K_j\\), \\(K_j\\) number possible responses/outcomes item \\(j\\).\nmany applications, number response options constant across items, though need case.\nGRM using conditional probability statements probability response specific category obtaining probability category difference two conditional probabilities.\n\n\\[P(x_{ij} = k \\mid \\theta_i, \\boldsymbol{d}_j,a_j) = P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j) - P(x_{ij \\geq k+1 \\mid \\theta_i, d_{j(k+1)},a_j),\\]\n\\(\\boldsymbol{d}_j\\) collection location/threshold parameters item \\(j\\).\nGRM takes structure similar 2-PL one category\n\\[P(x_{ij} \\geq k \\mid \\theta_i, d_{jk},a_j)=F(a_j\\theta_i + d_{jk}).\\]conditional probability observed responses may modeled similarly used dichotomous responses important differences.\nconditional distribution data \n\\[p(\\boldsymbol{x}\\mid \\boldsymbol{\\theta},\\boldsymbol{\\omega}) = \\prod_{=1}^np(\\boldsymbol{x}_i\\mid \\theta_i, \\boldsymbol{\\omega}) = \\prod_{=1}^n\\prod_{j=1}^Jp(x_{ij}\\mid \\theta_i, \\boldsymbol{\\omega}_j),\\]\n\\(x_{ij}\\) specified categorical random variable (multinomial).\ncategorical random variable generalization Bernoulli distribution defined collection category response probabilities\n\\[x_{ij} \\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)).\\]helps form likelihood observed data.\nNext, prior distribution described structure necessarily obvious.First, prior latent ability follows logic dichotomous model.\nemploy exchangeability assumption specify independent priors respondent normally distribution prior.Next, measurement model parameters’ priors described.\ncan assume exchangeability arrive common independent prior across items, assume priors location discrimination parameters independent.\nassumptions may tenable theory, practically useful.\npriors discrimination stay dichotomous model.\npriors location parameters bit involved.location parameters, first location parameter \\(d_{j1}\\) specifies probability responding 1 greater certainty gave response.\nTherefore, probability 1.\nset \\(d_{j1} = -\\inf\\) set normal prior \\(d_{j2}\\sim \\mathrm{Normal}(\\mu_{d2},\\sigma^2_{d2})\\).\npriors remaining location parameters (\\(d_{3}-d_{k}\\)) can specified truncated normal distributions.\n, location next threshold constrained larger previous threshold formally\n\\[d_{jk} \\sim \\mathrm{Normal}^{>d_{j(k-1)}}(\\mu_{d_k},\\sigma^2_{d_k},\\ \\mathrm{}\\ k=3, ...,K_j.\\]posterior distribution GRM can parameterized follows.\nmodel described general can accommodate varying number thresholds per item constrained 1 latent factor.\\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{}\\mid \\mathbf{x}) \\propto \\prod_{=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j)p(\\theta_i)p(a_j)\\prod_{k=2}^{K_j}p(d_{jk}),\\]\n\n\\[\\begin{align*}\nx_{ij}\\mid\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &\\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\\n\\mathbf{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= \\left(P(x_{ij}=1\\mid\\theta_i, \\boldsymbol{d}_j, a_j), \\cdots, P(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j)\\right),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\\nP(x_{ij}=k\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= P(x_{ij}\\geq k\\mid\\theta_i,d_{jk}, a_j) - P(x_{ij}\\geq k+1\\mid\\theta_i, d_{j(k+1)}, a_j),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k = 1,\\cdots,K_j-1;\\\\\nP(x_{ij}=K_j\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= P(x_{ij}\\geq K_j\\mid\\theta_i,d_{jK_j}, a_j),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\\nP(x_{ij}\\geq k\\mid\\theta_i, d_{jk}, a_j) &= F(a_j\\theta_j + d_{jk}),\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J,\\ k=2,\\cdots,K_j;\\\\\nP(x_{ij}\\geq 1\\mid\\theta_i, d_{j1}, a_j) &= 1,\\ \\mathrm{}\\ =1, \\cdots, n,\\ j = 1, \\cdots, J;\\\\\n\\theta_i \\mid \\mu_{\\theta}, \\sigma^2_{\\theta} &\\sim \\mathrm{Normal}(\\mu_{\\theta}, \\sigma^2_{\\theta}),\\ \\mathrm{}\\ = 1, \\cdots, n;\\\\\na_j \\mid \\mu_{}, \\sigma^2_{} &\\sim \\mathrm{Normal}^{+}(\\mu_{}, \\sigma^2_{}),\\ \\mathrm{}\\ j=1, \\cdots, J;\\\\\nd_{j2}\\mid\\mu_{j2}, \\sigma^2_{j2} &\\sim \\mathrm{Normal}(\\mu_{j2}, \\sigma^2_{j2} ),\\ \\mathrm{}\\ j=1, \\cdots, J;\\ \\mathrm{}\\\\\nd_{jk}\\mid\\mu_{d_{jk}},\\sigma^2_{d_{jk}} &\\sim \\mathrm{Normal}^{>d_{j(k-1)}}(\\mu_{d_{jk}},\\sigma^2_{d_{jk}}),\\ \\mathrm{}\\ j=1, \\cdots, J,\\ k=3, ...,K_j.\n\\end{align*}\\]","code":""},{"path":"item-response-theory.html","id":"grm-peer-interactions-example","chapter":"11 Item Response Theory","heading":"11.6 GRM Peer Interactions Example","text":"book uses example Peer Interactions 500 responses seven items. responses coded 1 5 agreement Likert-type scale.\nDAG GRM corresponding data shown .\nFigure 11.4: DAG Peer Interactions GRM analysis\npath diagram version substantially simpler identical path diagram 3-PL factor analysis diagrams.\nHighlighting similarity substantive modeling polytomous items dichotomous items.\nFigure 11.5: Path diagram Peer Interactions GRM analysis\ncompleteness, included model specification diagram concretely connects DAG path diagram assumed distributions priors.\nFigure 11.6: Model specification diagram Peer Interactions GRM analysis\n","code":""},{"path":"item-response-theory.html","id":"example-specific-model-specification","chapter":"11 Item Response Theory","heading":"11.6.1 Example Specific Model Specification","text":"fitting GRM Peer Interactions data, can precise prior likelihood structure.\nbreakdown model specific example.\nEverything structurally identical previous page specific values chosen hyperparameters.\\[p(\\boldsymbol{\\theta}, \\boldsymbol{d}, \\boldsymbol{}\\mid \\mathbf{x}) \\propto \\prod_{=1}^n\\prod_{j=1}^Jp(\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j)p(\\theta_i)p(a_j)\\prod_{k=2}^{K_j}p(d_{jk}),\\]\n\n\\[\\begin{align*}\nx_{ij}\\mid\\theta_i\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &\\sim \\mathrm{Categorical}(\\boldsymbol{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{\\omega}_j)),\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\\n\\mathbf{P}(x_{ij}\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= \\left(P(x_{ij}=1\\mid\\theta_i, \\boldsymbol{d}_j, a_j), \\cdots, P(x_{ij}=5\\mid\\theta_i, \\boldsymbol{d}_j, a_j)\\right),\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\\nP(x_{ij}=k\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= P(x_{ij}\\geq k\\mid\\theta_i,d_{jk}, a_j) - P(x_{ij}\\geq k+1\\mid\\theta_i, d_{j(k+1)}, a_j),\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7,\\ k = 1,\\cdots,4;\\\\\nP(x_{ij}=5\\mid\\theta_i, \\boldsymbol{d}_j, a_j) &= P(x_{ij}\\geq 5\\mid\\theta_i,d_{j5}, a_j),\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\\nP(x_{ij}\\geq k\\mid\\theta_i, d_{jk}, a_j) &= F(a_j\\theta_j + d_{jk}) = \\frac{\\exp\\left(a_j\\theta_i +d_{jk}\\right)}{1+\\exp\\left(a_j\\theta_i +d_{jk}\\right)},\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7,\\ k=2,\\cdots,5;\\\\\nP(x_{ij}\\geq 1\\mid\\theta_i, d_{j1}, a_j) &= 1,\\ \\mathrm{}\\ =1, \\cdots, 500,\\ j = 1, \\cdots, 7;\\\\\n\\theta_i \\mid \\mu_{\\theta}, \\sigma^2_{\\theta} &\\sim \\mathrm{Normal}(0, 1),\\ \\mathrm{}\\ = 1, \\cdots, 500;\\\\\na_j \\mid \\mu_{}, \\sigma^2_{} &\\sim \\mathrm{Normal}^{+}(0,2),\\ \\mathrm{}\\ j=1, \\cdots, 7;\\\\\nd_{j2}\\mid\\mu_{d_{j2}}, \\sigma^2_{d_{j2}} &\\sim \\mathrm{Normal}(2,2),\\ \\mathrm{}\\ j=1, \\cdots, 7;\\\\\nd_{j3}\\mid\\mu_{d_{j3}},\\sigma^2_{d_{j3}} &\\sim \\mathrm{Normal}^{>d_{j2}}(1, 2),\\ \\mathrm{}\\ j=1, \\cdots, 7;\\\\\nd_{j4}\\mid\\mu_{d_{j4}},\\sigma^2_{d_{j4}} &\\sim \\mathrm{Normal}^{>d_{j3}}(-1, 2),\\ \\mathrm{}\\ j=1, \\cdots, 7; \\mathrm{}\\\\\nd_{j5}\\mid\\mu_{d_{j5}},\\sigma^2_{d_{j5}} &\\sim \\mathrm{Normal}^{>d_{j4}}(-2, 2),\\ \\mathrm{}\\ j=1, \\cdots, 7.\n\\end{align*}\\]","code":""},{"path":"item-response-theory.html","id":"pi-example---jags","chapter":"11 Item Response Theory","heading":"11.7 PI Example - JAGS","text":"implementation, change d[j,3] ~ dnorm(1, .5)(d[j,4],d[j,2]) d[j,3] ~ dnorm(1, .5);(,d[j,2]) (1) R dumb doesn’t realize JAGS function; (2) JAGS allow directed cycle.\ndirected cycle DAG range values d[j,3] fixed within (d[j,4],d[j,2]) permissible. need simply constrain thresholds decreasing smaller previous threshold.\n’m sure underlying technical reason error, found adding semi-colon fixes issue defining model R function.","code":"\njags.model.peer.int <- function(){\n\n  #######################################\n  # Specify the item response measurement model for the observables\n  #######################################\n  for (i in 1:n){\n    for(j in 1:J){\n  \n      ###################################\n      # Specify the probabilities of a value being greater than or equal to each category\n      ###################################\n      for(k in 2:(K[j])){\n        # P(greater than or equal to category k > 1)\n        logit(P.gte[i,j,k]) <- a[j]*theta[i]+d[j,k]\n      }\n      # P(greater than or equal to category 1)\n      P.gte[i,j,1] <- 1\n  \n  \n      ###################################\n      # Specify the probabilities of a value being equal to each category\n      ###################################\n      for(k in 1:(K[j]-1)){\n        # P(greater equal to category k < K)\n        P[i,j,k] <- P.gte[i,j,k]-P.gte[i,j,k+1]\n      }\n      # P(greater equal to category K)\n      P[i,j,K[j]] <- P.gte[i,j,K[j]]\n      \n      ###################################\n      # Specify the distribution for each observable\n      ###################################\n      x[i,j] ~ dcat(P[i,j,1:K[j]])\n    }\n  }\n  \n  \n  #######################################\n  # Specify the (prior) distribution for the latent variables\n  #######################################\n  for (i in 1:n){\n    theta[i] ~ dnorm(0, 1)  # distribution for the latent variables\n  }\n  \n  \n  #######################################\n  # Specify the prior distribution for the measurement model parameters\n  #######################################\n  for(j in 1:J){\n    \n    d[j,2] ~ dnorm(2, .5)                   # Locations for k = 2\n    d[j,3] ~ dnorm(1, .5);I(,d[j,2])   # Locations for k = 3\n    d[j,4] ~ dnorm(-1, .5);I(,d[j,3])  # Locations for k = 4\n    d[j,5] ~ dnorm(-2, .5);I(,d[j,4])        # Locations for k = 5\n    a[j] ~ dnorm(1, .5); I(0,)    # Discriminations for observables\n  \n  }\n\n} # closes the model\n\n# initial values\nstart_values <- list(\n  list(\n    d= matrix(c(NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00,\n               NA, 3.00E+00, 1.00E+00, 0.00E+00, -1.00E+00),\n      ncol=5, nrow=7, byrow=T),\n    a=c(1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01, 1.00E-01)),\n  list(\n    d= matrix(c(NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00,\n               NA, 2.00E+00, 0.00E+00, -1.00E+00, -2.00E+00),\n      ncol=5, nrow=7, byrow=T),\n    a=c(3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00, 3.00E+00)),\n  list(\n    d= matrix(c(NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00,\n               NA, 1.00E+00, -1.00E+00, -2.00E+00, -3.00E+00),\n              ncol=5, nrow=7, byrow=T),\n    a=c(1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00, 1.00E+00))\n)\n\n# vector of all parameters to save\nparam_save <- c(\"a\", \"d\", \"theta\")\n\n# dataset\ndat <- read.table(\"data/PI.dat\", header=T)\n\nmydata <- list(\n  n = nrow(dat), J = ncol(dat),\n  K = rep(5, ncol(dat)),\n  x = as.matrix(dat)\n)\n\n# fit model\nfit <- jags(\n  model.file=jags.model.peer.int,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=2000,\n  n.burnin = 1000,\n  n.chains = 3)## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 3500\n##    Unobserved stochastic nodes: 535\n##    Total graph size: 53050\n## \n## Initializing model\n## \n## \n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n## \n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |**************************************************| 100%\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model454427434e8.txt\", fit using jags,\n##  3 chains, each with 2000 iterations (first 1000 discarded)\n##  n.sims = 3000 iterations saved\n##             mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat n.eff\n## a[1]          2.136   0.164    1.845    2.024    2.126    2.239    2.481 1.006   670\n## a[2]          3.521   0.260    3.016    3.346    3.518    3.696    4.032 1.008   270\n## a[3]          3.957   0.303    3.356    3.758    3.959    4.149    4.550 1.004   530\n## a[4]          3.576   0.252    3.104    3.401    3.571    3.747    4.088 1.008   260\n## a[5]          2.444   0.171    2.124    2.323    2.437    2.563    2.786 1.009   270\n## a[6]          1.866   0.151    1.587    1.761    1.861    1.965    2.171 1.011   200\n## a[7]          1.495   0.121    1.255    1.413    1.492    1.577    1.730 1.004  2300\n## d[1,2]        4.952   0.328    4.343    4.735    4.949    5.156    5.639 1.002  1000\n## d[2,2]        5.185   0.355    4.529    4.936    5.169    5.420    5.915 1.007   330\n## d[3,2]        6.513   0.482    5.575    6.188    6.506    6.830    7.485 1.003  1400\n## d[4,2]        6.385   0.435    5.593    6.086    6.373    6.676    7.251 1.004   650\n## d[5,2]        4.768   0.312    4.184    4.562    4.759    4.973    5.403 1.013   200\n## d[6,2]        3.833   0.258    3.362    3.653    3.825    4.007    4.361 1.013   160\n## d[7,2]        4.274   0.286    3.723    4.080    4.265    4.460    4.852 1.001  2400\n## d[1,3]        2.943   0.211    2.536    2.800    2.939    3.085    3.370 1.003   970\n## d[2,3]        1.169   0.201    0.756    1.046    1.174    1.300    1.563 1.020   240\n## d[3,3]        3.323   0.296    2.753    3.120    3.329    3.508    3.930 1.002  2300\n## d[4,3]        2.703   0.242    2.261    2.536    2.695    2.859    3.194 1.009   270\n## d[5,3]        2.672   0.203    2.268    2.540    2.676    2.805    3.073 1.013   200\n## d[6,3]        1.949   0.164    1.626    1.837    1.951    2.059    2.275 1.012   200\n## d[7,3]        2.443   0.167    2.120    2.330    2.446    2.557    2.762 1.004   710\n## d[1,4]        0.645   0.146    0.362    0.547    0.644    0.741    0.934 1.006   360\n## d[2,4]       -1.785   0.222   -2.239   -1.929   -1.780   -1.627   -1.370 1.020   110\n## d[3,4]       -1.325   0.233   -1.837   -1.467   -1.312   -1.164   -0.903 1.024    90\n## d[4,4]       -0.806   0.206   -1.253   -0.933   -0.797   -0.672   -0.425 1.019   160\n## d[5,4]       -0.356   0.156   -0.670   -0.455   -0.353   -0.249   -0.059 1.011   190\n## d[6,4]       -0.490   0.131   -0.767   -0.571   -0.488   -0.403   -0.240 1.007   650\n## d[7,4]        0.263   0.116    0.038    0.186    0.266    0.341    0.482 1.008   290\n## d[1,5]       -2.511   0.186   -2.887   -2.636   -2.507   -2.379   -2.157 1.007   320\n## d[2,5]       -5.140   0.368   -5.858   -5.374   -5.137   -4.885   -4.446 1.011   200\n## d[3,5]       -5.562   0.419   -6.453   -5.830   -5.553   -5.272   -4.757 1.013   170\n## d[4,5]       -4.855   0.348   -5.562   -5.083   -4.832   -4.625   -4.196 1.017   220\n## d[5,5]       -3.605   0.243   -4.092   -3.762   -3.600   -3.436   -3.153 1.003  3000\n## d[6,5]       -3.210   0.214   -3.631   -3.355   -3.205   -3.061   -2.806 1.009   530\n## d[7,5]       -2.748   0.179   -3.112   -2.865   -2.740   -2.625   -2.413 1.004   650\n## theta[1]      0.855   0.285    0.283    0.669    0.853    1.050    1.390 1.001  3000\n## theta[2]     -0.976   0.268   -1.499   -1.152   -0.970   -0.802   -0.457 1.002  1000\n## theta[3]     -1.040   0.252   -1.546   -1.205   -1.041   -0.871   -0.539 1.002  1100\n## theta[4]     -0.257   0.258   -0.769   -0.428   -0.252   -0.083    0.242 1.001  3000\n## theta[5]      0.113   0.274   -0.412   -0.070    0.124    0.293    0.636 1.001  3000\n## theta[6]      0.107   0.248   -0.364   -0.062    0.107    0.272    0.598 1.004   570\n## theta[7]      1.470   0.330    0.861    1.244    1.456    1.683    2.147 1.005   600\n## theta[8]      0.787   0.258    0.292    0.608    0.788    0.964    1.297 1.001  3000\n## theta[9]      0.004   0.253   -0.495   -0.166    0.008    0.173    0.503 1.004   580\n## theta[10]     0.371   0.267   -0.130    0.194    0.367    0.543    0.896 1.001  3000\n## theta[11]    -0.758   0.259   -1.266   -0.938   -0.754   -0.585   -0.231 1.001  2100\n## theta[12]    -0.195   0.253   -0.697   -0.365   -0.194   -0.028    0.300 1.003   830\n## theta[13]     1.036   0.267    0.521    0.857    1.036    1.210    1.562 1.003   720\n## theta[14]     0.006   0.261   -0.516   -0.164    0.009    0.183    0.520 1.002  1600\n## theta[15]    -0.698   0.267   -1.219   -0.880   -0.704   -0.518   -0.176 1.003   810\n## theta[16]    -0.449   0.292   -1.001   -0.654   -0.451   -0.253    0.124 1.001  3000\n## theta[17]     2.349   0.463    1.582    2.021    2.299    2.619    3.407 1.001  3000\n## theta[18]     0.232   0.260   -0.271    0.062    0.236    0.404    0.744 1.002  2000\n## theta[19]     0.045   0.260   -0.457   -0.133    0.046    0.216    0.564 1.001  3000\n## theta[20]     0.240   0.263   -0.282    0.069    0.242    0.415    0.749 1.002  1300\n## theta[21]     0.370   0.259   -0.139    0.198    0.370    0.543    0.883 1.001  3000\n## theta[22]    -1.004   0.267   -1.518   -1.189   -1.003   -0.829   -0.479 1.001  2500\n## theta[23]     0.698   0.265    0.195    0.520    0.699    0.869    1.231 1.003   900\n## theta[24]     0.059   0.263   -0.468   -0.116    0.060    0.237    0.562 1.002  3000\n## theta[25]     2.356   0.452    1.589    2.028    2.316    2.637    3.354 1.002  2900\n## theta[26]     0.549   0.303   -0.038    0.342    0.539    0.758    1.137 1.001  3000\n## theta[27]    -0.753   0.269   -1.275   -0.924   -0.759   -0.581   -0.192 1.002  1300\n## theta[28]     0.214   0.260   -0.298    0.044    0.217    0.392    0.728 1.004   600\n## theta[29]     1.074   0.290    0.525    0.876    1.063    1.267    1.662 1.003  3000\n## theta[30]     0.373   0.263   -0.146    0.201    0.371    0.547    0.890 1.001  3000\n## theta[31]     0.489   0.258   -0.019    0.311    0.484    0.659    1.002 1.002  1800\n## theta[32]     0.438   0.261   -0.069    0.261    0.437    0.608    0.958 1.001  3000\n## theta[33]     0.439   0.268   -0.102    0.256    0.441    0.617    0.973 1.001  3000\n## theta[34]     0.137   0.261   -0.377   -0.037    0.141    0.309    0.652 1.002  3000\n## theta[35]    -1.317   0.268   -1.848   -1.494   -1.319   -1.143   -0.770 1.003   890\n## theta[36]     0.783   0.256    0.282    0.615    0.779    0.956    1.284 1.005   430\n## theta[37]    -0.516   0.260   -1.010   -0.694   -0.524   -0.347    0.022 1.004   610\n## theta[38]    -0.727   0.250   -1.222   -0.887   -0.725   -0.562   -0.240 1.002  1900\n## theta[39]    -0.461   0.296   -1.048   -0.659   -0.460   -0.262    0.124 1.004  1400\n## theta[40]    -0.128   0.272   -0.669   -0.310   -0.123    0.058    0.392 1.006   370\n## theta[41]     0.228   0.278   -0.308    0.041    0.231    0.411    0.784 1.001  3000\n## theta[42]    -2.225   0.354   -2.982   -2.452   -2.204   -1.975   -1.596 1.001  3000\n## theta[43]     2.103   0.389    1.447    1.833    2.075    2.338    3.007 1.001  3000\n## theta[44]    -0.396   0.303   -0.988   -0.606   -0.389   -0.195    0.189 1.001  3000\n## theta[45]     2.361   0.481    1.556    2.028    2.303    2.647    3.514 1.001  3000\n## theta[46]     0.796   0.262    0.281    0.625    0.790    0.966    1.331 1.001  3000\n## theta[47]    -0.845   0.274   -1.381   -1.032   -0.841   -0.664   -0.312 1.002  1200\n## theta[48]    -0.168   0.266   -0.701   -0.340   -0.160    0.015    0.322 1.002  2100\n## theta[49]     0.178   0.270   -0.339    0.006    0.171    0.353    0.719 1.002  1100\n## theta[50]    -0.582   0.285   -1.128   -0.775   -0.589   -0.389   -0.028 1.001  3000\n## theta[51]    -0.377   0.262   -0.900   -0.554   -0.374   -0.199    0.130 1.005   440\n## theta[52]    -0.998   0.260   -1.508   -1.177   -1.005   -0.828   -0.476 1.001  3000\n## theta[53]     0.444   0.266   -0.082    0.266    0.446    0.628    0.951 1.001  3000\n## theta[54]     0.380   0.272   -0.164    0.203    0.378    0.559    0.916 1.002  1900\n## theta[55]    -0.829   0.271   -1.362   -1.010   -0.826   -0.651   -0.308 1.001  3000\n## theta[56]     0.782   0.262    0.276    0.607    0.781    0.958    1.292 1.003   870\n## theta[57]     1.393   0.275    0.852    1.207    1.389    1.568    1.954 1.001  3000\n## theta[58]    -0.840   0.243   -1.302   -1.017   -0.845   -0.669   -0.371 1.001  2500\n## theta[59]     0.111   0.253   -0.381   -0.052    0.108    0.279    0.617 1.007   310\n## theta[60]     0.357   0.261   -0.156    0.183    0.355    0.530    0.878 1.003  1300\n## theta[61]    -1.806   0.316   -2.490   -2.013   -1.792   -1.589   -1.235 1.001  3000\n## theta[62]    -0.416   0.257   -0.905   -0.588   -0.419   -0.249    0.113 1.002  1000\n## theta[63]     2.365   0.489    1.533    2.018    2.312    2.674    3.411 1.003   770\n## theta[64]    -0.926   0.254   -1.434   -1.097   -0.921   -0.748   -0.444 1.001  3000\n## theta[65]     0.387   0.264   -0.119    0.206    0.382    0.562    0.918 1.002  1800\n## theta[66]     0.777   0.253    0.279    0.606    0.773    0.946    1.281 1.029   720\n## theta[67]     0.344   0.250   -0.146    0.179    0.339    0.505    0.848 1.001  3000\n## theta[68]     1.622   0.285    1.076    1.433    1.614    1.807    2.207 1.006   550\n## theta[69]     1.091   0.268    0.554    0.912    1.089    1.272    1.618 1.003   960\n## theta[70]     1.622   0.297    1.066    1.421    1.611    1.810    2.259 1.001  2000\n## theta[71]    -0.003   0.247   -0.493   -0.178    0.002    0.169    0.478 1.002  1600\n## theta[72]    -1.028   0.254   -1.538   -1.193   -1.029   -0.853   -0.564 1.001  3000\n## theta[73]     0.158   0.259   -0.359   -0.010    0.161    0.332    0.663 1.001  3000\n## theta[74]    -2.300   0.384   -3.130   -2.533   -2.272   -2.035   -1.665 1.004  3000\n## theta[75]    -0.108   0.284   -0.659   -0.297   -0.106    0.082    0.448 1.001  2300\n## theta[76]    -1.098   0.254   -1.596   -1.268   -1.094   -0.928   -0.609 1.003   870\n##  [ reached getOption(\"max.print\") -- omitted 425 rows ]\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 692.5 and DIC = 7012.1\n## DIC is an estimate of expected predictive error (lower deviance is better).\nround(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% \"theta\", ], 3)##              mean     sd     2.5%      25%      50%      75%    97.5%  Rhat n.eff\n## a[1]        2.136  0.164    1.845    2.024    2.126    2.239    2.481 1.006   670\n## a[2]        3.521  0.260    3.016    3.346    3.518    3.696    4.032 1.008   270\n## a[3]        3.957  0.303    3.356    3.758    3.959    4.149    4.550 1.004   530\n## a[4]        3.576  0.252    3.104    3.401    3.571    3.747    4.088 1.008   260\n## a[5]        2.444  0.171    2.124    2.323    2.437    2.563    2.786 1.009   270\n## a[6]        1.866  0.151    1.587    1.761    1.861    1.965    2.171 1.011   200\n## a[7]        1.495  0.121    1.255    1.413    1.492    1.577    1.730 1.004  2300\n## d[1,2]      4.952  0.328    4.343    4.735    4.949    5.156    5.639 1.002  1000\n## d[2,2]      5.185  0.355    4.529    4.936    5.169    5.420    5.915 1.007   330\n## d[3,2]      6.513  0.482    5.575    6.188    6.506    6.830    7.485 1.003  1400\n## d[4,2]      6.385  0.435    5.593    6.086    6.373    6.676    7.251 1.004   650\n## d[5,2]      4.768  0.312    4.184    4.562    4.759    4.973    5.403 1.013   200\n## d[6,2]      3.833  0.258    3.362    3.653    3.825    4.007    4.361 1.013   160\n## d[7,2]      4.274  0.286    3.723    4.080    4.265    4.460    4.852 1.001  2400\n## d[1,3]      2.943  0.211    2.536    2.800    2.939    3.085    3.370 1.003   970\n## d[2,3]      1.169  0.201    0.756    1.046    1.174    1.300    1.563 1.020   240\n## d[3,3]      3.323  0.296    2.753    3.120    3.329    3.508    3.930 1.002  2300\n## d[4,3]      2.703  0.242    2.261    2.536    2.695    2.859    3.194 1.009   270\n## d[5,3]      2.672  0.203    2.268    2.540    2.676    2.805    3.073 1.013   200\n## d[6,3]      1.949  0.164    1.626    1.837    1.951    2.059    2.275 1.012   200\n## d[7,3]      2.443  0.167    2.120    2.330    2.446    2.557    2.762 1.004   710\n## d[1,4]      0.645  0.146    0.362    0.547    0.644    0.741    0.934 1.006   360\n## d[2,4]     -1.785  0.222   -2.239   -1.929   -1.780   -1.627   -1.370 1.020   110\n## d[3,4]     -1.325  0.233   -1.837   -1.467   -1.312   -1.164   -0.903 1.024    90\n## d[4,4]     -0.806  0.206   -1.253   -0.933   -0.797   -0.672   -0.425 1.019   160\n## d[5,4]     -0.356  0.156   -0.670   -0.455   -0.353   -0.249   -0.059 1.011   190\n## d[6,4]     -0.490  0.131   -0.767   -0.571   -0.488   -0.403   -0.240 1.007   650\n## d[7,4]      0.263  0.116    0.038    0.186    0.266    0.341    0.482 1.008   290\n## d[1,5]     -2.511  0.186   -2.887   -2.636   -2.507   -2.379   -2.157 1.007   320\n## d[2,5]     -5.140  0.368   -5.858   -5.374   -5.137   -4.885   -4.446 1.011   200\n## d[3,5]     -5.562  0.419   -6.453   -5.830   -5.553   -5.272   -4.757 1.013   170\n## d[4,5]     -4.855  0.348   -5.562   -5.083   -4.832   -4.625   -4.196 1.017   220\n## d[5,5]     -3.605  0.243   -4.092   -3.762   -3.600   -3.436   -3.153 1.003  3000\n## d[6,5]     -3.210  0.214   -3.631   -3.355   -3.205   -3.061   -2.806 1.009   530\n## d[7,5]     -2.748  0.179   -3.112   -2.865   -2.740   -2.625   -2.413 1.004   650\n## deviance 6319.569 37.286 6251.475 6293.382 6318.824 6344.230 6396.701 1.005   460\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\"with medians and 80% intervals\")\nbayesplot::mcmc_areas(plot.data, regex_pars = \"d\", prob = 0.8) +  plot_title + lims(x=c(-10, 10))## Scale for 'x' is already present. Adding another scale for 'x', which will replace the existing\n## scale.## Warning: Removed 1 rows containing missing values (geom_segment).\nbayesplot::mcmc_areas(\n  plot.data,\n  pars = c(paste0(\"a[\", 1:7, \"]\")),\n  prob = 0.8) +\n  plot_title\nbayesplot::mcmc_acf(plot.data,pars = c(paste0(\"a[\", 1:7, \"]\")))\nbayesplot::mcmc_trace(plot.data,pars = c(paste0(\"a[\", 1:7, \"]\")))\nggmcmc::ggs_grb(ggs(jags.mcmc), family=\"d\")\nggmcmc::ggs_grb(ggs(jags.mcmc), family=\"a\")\nggmcmc::ggs_autocorrelation(ggs(jags.mcmc), family=\"d\")"},{"path":"item-response-theory.html","id":"pi-example---stan","chapter":"11 Item Response Theory","heading":"11.8 PI Example - Stan","text":"-","code":""},{"path":"item-response-theory.html","id":"latent-response-formulation","chapter":"11 Item Response Theory","heading":"11.9 Latent Response Formulation","text":"Connecting IRT models factor analytic perspective can helpful modeling standpoint.\nEspecially one’s model multidimensional leading structural equation models.\nuseful connection can made introducing extra variable(s) model represent latent response variable underlying observed categorical response variable.\ncan think latent response variables asa latent continuous variable hypothesized underlie observed categorical variable discretized due data collection difficulty measurement; ora latent continuous variable hypothesized underlie observed categorical variable discretized due data collection difficulty measurement; orwhen natural interpretation appropriate, can think latent response variable propensity measure given response. Although perfect interpretation, use latent response formulation eases computational machinery allows nice connection IRT CFA models.natural interpretation appropriate, can think latent response variable propensity measure given response. Although perfect interpretation, use latent response formulation eases computational machinery allows nice connection IRT CFA models.Next, latent response formulation shown set dichotomous outcomes.\nmodel conceptually 2-PL/2-PNO (2 parameter normal ogive) model essentially probit model.\nmodel can defined \\[x^{\\ast}_{ij} = a_j\\theta_i+d_j+\\varepsilon_{ij},\\], \\(j\\), \\(\\varepsilon_{ij} \\sim \\mathrm{Normal}(0, \\sigma^2_{\\varepsilon_j})\\). can jointly expressed \\[x^{\\ast}_{ij} \\sim \\mathrm{Normal}(a_j\\theta_i+d_j, \\sigma^2_{\\varepsilon_j}).\\]probability observed response modeled probability latent response variable person \\(\\) item \\(j\\) greater equal threshold \\(\\gamma_j\\).","code":""},{"path":"item-response-theory.html","id":"lsat-example-revisted","chapter":"11 Item Response Theory","heading":"11.9.1 LSAT Example Revisted","text":"","code":"\njags.model.lsat <- function(){\n\nfor (i in 1:n){\n  for(j in 1:J){\n    # latent response variable\n    xstar[i,j] ~ dnorm(a[j]*theta[i]+d[j], 1)\n    P[i,j] <- c[j]+(1-c[j])*phi(xstar[i,j]) # 3P-NO expression\n    x[i,j] ~ dbern(P[i,j])                  # distribution for each observable\n  }\n}\n\nfor (i in 1:n){\n  theta[i] ~ dnorm(0, 1)  # distribution for the latent variables\n}\n\nfor(j in 1:J){\n  d[j] ~ dnorm(0, .5)          # Locations for observables\n  a[j] ~ dnorm(1, .5); I(0,)    # Discriminations for observables\n  c[j] ~ dbeta(5,17)           # Lower asymptotes for observables\n  \n}\n\n\n} # closes the model\n\n# initial values\nstart_values <- list(\n  list(\"d\"=c(1.00, 1.00, 1.00, 1.00, 1.00),\n       \"a\"=c(1.00, 1.00, 1.00, 1.00, 1.00),\n       \"c\"=c(0.20, 0.20, 0.20, 0.20, 0.20)),\n  list(\"d\"=c(-3.00, -3.00, -3.00, -3.00, -3.00),\n       \"a\"=c(3.00, 3.00, 3.00, 3.00, 3.00),\n       \"c\"=c(0.50, 0.50, 0.50, 0.50, 0.50)),\n  list(\"d\"=c(3.00, 3.00, 3.00, 3.00, 3.00),\n       \"a\"=c(0.1, 0.1, 0.1, 0.1, 0.1),\n       \"c\"=c(0.05, 0.05, 0.05, 0.05, 0.05))\n)\n\n# vector of all parameters to save\nparam_save <- c(\"a\", \"d\", \"c\", \"theta\")\n\n# dataset\ndat <- read.table(\"data/LSAT.dat\", header=T)\n\nmydata <- list(\n  n = nrow(dat),\n  J = ncol(dat),\n  x = as.matrix(dat)\n)\n\n# fit model\nfit <- jags(\n  model.file=jags.model.lsat,\n  data=mydata,\n  inits=start_values,\n  parameters.to.save = param_save,\n  n.iter=500,\n  n.burnin = 200,\n  n.chains = 3,\n  progress.bar = \"none\")## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 5000\n##    Unobserved stochastic nodes: 6015\n##    Total graph size: 36027\n## \n## Initializing model\nprint(fit)## Inference for Bugs model at \"C:/Users/noahp/AppData/Local/Temp/RtmpQR8sbs/model454417575fb3.txt\", fit using jags,\n##  3 chains, each with 500 iterations (first 200 discarded)\n##  n.sims = 900 iterations saved\n##              mu.vect sd.vect     2.5%      25%      50%      75%    97.5%   Rhat n.eff\n## a[1]           0.577   0.198    0.285    0.436    0.544    0.678    1.045  1.652     6\n## a[2]           1.939   1.567    0.383    0.665    1.051    3.534    4.703  3.996     3\n## a[3]           1.568   0.660    0.710    0.929    1.355    2.234    2.572  2.317     4\n## a[4]           0.600   0.198    0.278    0.462    0.560    0.726    1.038  1.360    10\n## a[5]           0.847   0.608    0.090    0.359    0.575    1.440    1.965  2.599     4\n## c[1]           0.453   0.324    0.095    0.194    0.283    0.894    0.915  3.948     3\n## c[2]           0.348   0.165    0.115    0.214    0.297    0.539    0.607  3.008     4\n## c[3]           0.293   0.153    0.098    0.170    0.235    0.474    0.538  4.308     3\n## c[4]           0.402   0.245    0.103    0.195    0.294    0.718    0.752  3.600     4\n## c[5]           0.417   0.305    0.089    0.176    0.256    0.829    0.858  4.082     3\n## d[1]           0.562   1.927   -2.420   -2.019    1.843    1.959    2.144 20.801     3\n## d[2]          -0.353   1.203   -2.471   -1.766    0.360    0.525    0.760  9.579     3\n## d[3]          -1.043   1.054   -3.088   -2.063   -0.548   -0.197    0.058  6.905     3\n## d[4]          -0.230   1.388   -2.375   -2.069    0.648    0.824    1.000 15.453     3\n## d[5]           0.028   2.055   -3.286   -2.602    1.392    1.514    1.643 13.313     3\n## theta[1]      -1.308   0.838   -2.869   -1.944   -1.321   -0.713    0.414  1.185    15\n## theta[2]      -1.124   0.942   -3.075   -1.731   -1.142   -0.495    0.663  1.623     7\n## theta[3]      -1.304   0.885   -3.028   -1.914   -1.308   -0.729    0.485  1.229    13\n## theta[4]      -1.043   0.790   -2.712   -1.543   -0.952   -0.486    0.304  1.248    12\n## theta[5]      -1.157   0.922   -3.024   -1.803   -1.099   -0.494    0.481  1.276    11\n## theta[6]      -1.097   0.712   -2.471   -1.580   -1.118   -0.586    0.295  1.061    37\n## theta[7]      -1.025   0.887   -2.935   -1.593   -0.984   -0.326    0.435  1.374     9\n## theta[8]      -1.005   0.846   -2.812   -1.557   -0.901   -0.426    0.514  1.329    10\n## theta[9]      -0.940   0.760   -2.434   -1.460   -0.895   -0.420    0.484  1.271    11\n## theta[10]     -1.158   0.761   -2.647   -1.688   -1.140   -0.572    0.181  1.053    45\n## theta[11]     -0.950   0.849   -2.720   -1.451   -0.976   -0.398    0.731  1.180    15\n## theta[12]     -0.828   0.828   -2.502   -1.422   -0.816   -0.188    0.610  1.115    22\n## theta[13]     -0.982   0.755   -2.531   -1.515   -0.977   -0.458    0.426  1.010   900\n## theta[14]     -0.812   0.773   -2.363   -1.375   -0.795   -0.255    0.605  1.102    25\n## theta[15]     -0.790   0.825   -2.487   -1.343   -0.722   -0.237    0.728  1.230    13\n## theta[16]     -0.737   0.742   -2.070   -1.290   -0.740   -0.183    0.637  1.147    18\n## theta[17]     -0.686   0.705   -1.971   -1.192   -0.718   -0.220    0.736  1.149    19\n## theta[18]     -0.735   0.769   -2.236   -1.282   -0.745   -0.194    0.786  1.236    13\n## theta[19]     -0.722   0.825   -2.455   -1.256   -0.696   -0.137    0.687  1.270    11\n## theta[20]     -0.731   0.784   -2.271   -1.276   -0.677   -0.142    0.711  1.303    10\n## theta[21]     -0.769   0.847   -2.494   -1.331   -0.742   -0.157    0.795  1.234    13\n## theta[22]     -0.762   0.724   -2.394   -1.248   -0.705   -0.265    0.567  1.219    13\n## theta[23]     -0.981   0.884   -2.570   -1.663   -1.022   -0.354    0.694  1.001   900\n## theta[24]     -0.850   0.938   -2.710   -1.534   -0.831   -0.100    0.714  1.107    24\n## theta[25]     -0.504   0.857   -2.318   -1.040   -0.419    0.095    0.954  1.042    88\n## theta[26]     -0.627   0.836   -2.248   -1.203   -0.596   -0.028    0.884  1.057    48\n## theta[27]     -0.336   0.726   -1.901   -0.804   -0.287    0.146    1.060  1.040    82\n## theta[28]     -0.451   0.851   -2.100   -1.043   -0.400    0.147    1.131  1.001   900\n## theta[29]     -0.298   0.799   -2.076   -0.824   -0.268    0.230    1.178  1.146    19\n## theta[30]     -0.522   0.785   -2.256   -0.971   -0.430   -0.002    0.859  1.019   900\n## theta[31]     -0.306   0.780   -1.806   -0.852   -0.292    0.248    1.174  1.028   140\n## theta[32]     -0.919   0.882   -2.686   -1.504   -0.861   -0.327    0.676  1.209    14\n## theta[33]     -0.433   1.002   -2.037   -1.126   -0.572    0.025    2.160  1.338    11\n## theta[34]     -0.537   0.987   -2.328   -1.262   -0.597    0.141    1.429  1.847     6\n## theta[35]     -0.417   0.854   -2.085   -0.966   -0.436    0.172    1.256  1.240    13\n## theta[36]     -0.548   0.963   -2.371   -1.212   -0.548    0.067    1.430  1.596     7\n## theta[37]     -0.609   0.976   -2.327   -1.305   -0.695    0.065    1.325  1.470     8\n## theta[38]     -0.625   0.878   -2.214   -1.262   -0.584   -0.092    1.208  1.055    63\n## theta[39]     -0.527   1.010   -2.441   -1.169   -0.585    0.114    1.716  1.595     7\n## theta[40]     -0.657   1.053   -2.599   -1.347   -0.690   -0.064    1.677  1.169    23\n## theta[41]     -0.202   0.940   -1.998   -0.759   -0.232    0.353    1.795  1.293    11\n## theta[42]     -0.132   0.850   -1.726   -0.750   -0.186    0.438    1.550  1.451     8\n## theta[43]     -0.308   0.979   -2.107   -0.918   -0.386    0.255    1.981  1.117    45\n## theta[44]     -0.493   0.809   -2.095   -1.034   -0.489    0.055    1.100  1.013   900\n## theta[45]     -0.247   1.016   -2.496   -0.829   -0.229    0.336    1.758  1.272    13\n## theta[46]     -0.407   0.852   -2.005   -1.003   -0.376    0.201    1.224  1.163    17\n## theta[47]     -0.235   1.045   -1.924   -0.967   -0.346    0.439    1.948  1.366    10\n## theta[48]     -0.419   0.831   -2.152   -0.892   -0.366    0.111    1.166  1.048   870\n## theta[49]     -0.411   0.814   -2.040   -0.993   -0.425    0.126    1.214  1.228    13\n## theta[50]     -0.692   0.886   -2.646   -1.220   -0.614   -0.113    0.984  1.179    16\n## theta[51]     -0.283   0.850   -1.850   -0.885   -0.376    0.296    1.424  1.152    18\n## theta[52]     -0.396   0.841   -1.996   -0.975   -0.353    0.171    1.209  1.136    19\n## theta[53]     -0.621   0.718   -2.011   -1.106   -0.608   -0.124    0.744  1.021   110\n## theta[54]     -0.347   0.802   -1.969   -0.869   -0.362    0.212    1.152  1.061    42\n## theta[55]     -0.453   0.790   -2.062   -0.974   -0.457    0.086    1.113  1.025   340\n## theta[56]     -0.227   0.968   -1.961   -0.896   -0.247    0.455    1.733  1.243    13\n## theta[57]     -0.094   0.851   -1.744   -0.675   -0.106    0.453    1.616  1.031    70\n## theta[58]     -0.236   0.934   -2.433   -0.772   -0.127    0.441    1.290  1.036    78\n## theta[59]     -0.065   1.046   -2.133   -0.760   -0.066    0.644    1.895  1.035   110\n## theta[60]     -0.050   0.934   -1.826   -0.762    0.010    0.594    1.738  1.056    40\n## theta[61]      0.071   0.875   -2.021   -0.426    0.179    0.627    1.659  1.148    19\n## theta[62]      0.190   0.759   -1.280   -0.310    0.207    0.681    1.655  1.143    19\n## theta[63]      0.424   0.839   -1.405   -0.069    0.471    0.965    1.969  1.010   280\n## theta[64]      0.298   0.806   -1.413   -0.228    0.353    0.858    1.778  1.014   310\n## theta[65]      0.441   1.022   -1.649   -0.221    0.512    1.134    2.385  1.340    10\n## theta[66]      0.076   1.031   -2.033   -0.610    0.192    0.761    2.058  1.140    23\n## theta[67]      0.581   0.933   -1.319   -0.032    0.611    1.213    2.459  1.160    17\n## theta[68]      0.244   1.001   -2.200   -0.304    0.369    0.892    1.952  1.036   120\n## theta[69]      0.123   0.784   -1.605   -0.300    0.183    0.649    1.463  1.022   110\n## theta[70]      0.051   0.733   -1.401   -0.459    0.005    0.546    1.503  1.077    31\n## theta[71]      0.207   0.821   -1.512   -0.313    0.215    0.812    1.713  1.033    65\n## theta[72]      0.148   0.832   -1.604   -0.423    0.183    0.698    1.726  1.066    38\n## theta[73]      0.394   1.131   -1.731   -0.361    0.308    1.164    2.614  1.377    10\n## theta[74]     -0.013   0.780   -1.446   -0.546   -0.015    0.487    1.616  1.078    36\n## theta[75]      0.455   0.817   -1.267   -0.079    0.520    1.031    1.862  1.056    51\n## theta[76]      0.237   1.007   -1.745   -0.462    0.263    0.925    2.219  1.138    31\n## theta[77]     -1.296   0.858   -2.849   -1.913   -1.305   -0.646    0.389  1.032    75\n## theta[78]     -0.879   0.774   -2.427   -1.404   -0.867   -0.290    0.428  1.280    11\n## theta[79]     -0.976   0.818   -2.547   -1.509   -0.998   -0.398    0.623  1.049    47\n## theta[80]     -0.916   0.814   -2.520   -1.477   -0.886   -0.399    0.653  1.439     8\n## theta[81]     -1.186   0.928   -2.848   -1.844   -1.263   -0.481    0.579  1.056    63\n## theta[82]     -1.012   0.871   -2.763   -1.587   -1.000   -0.366    0.516  1.135    20\n## theta[83]     -0.893   0.830   -2.430   -1.498   -0.948   -0.292    0.734  1.324    10\n## theta[84]     -1.013   0.799   -2.621   -1.565   -1.008   -0.437    0.408  1.123    22\n## theta[85]     -1.151   0.766   -2.736   -1.618   -1.107   -0.600    0.167  1.096    28\n## theta[86]     -1.130   0.776   -2.783   -1.635   -1.075   -0.565    0.160  1.236    13\n## theta[87]     -0.786   0.781   -2.528   -1.290   -0.719   -0.197    0.491  1.166    17\n## theta[88]     -0.872   0.806   -2.663   -1.378   -0.820   -0.311    0.518  1.095    26\n## theta[89]     -0.849   0.736   -2.369   -1.361   -0.810   -0.349    0.557  1.060    38\n## theta[90]     -0.876   0.743   -2.525   -1.325   -0.818   -0.343    0.434  1.161    17\n## theta[91]     -0.907   0.763   -2.322   -1.490   -0.892   -0.356    0.556  1.022   120\n## theta[92]     -0.647   0.729   -2.069   -1.134   -0.661   -0.137    0.814  1.232    13\n## theta[93]     -0.752   0.675   -2.164   -1.212   -0.729   -0.283    0.502  1.099    25\n## theta[94]     -0.937   0.766   -2.479   -1.493   -0.923   -0.386    0.484  1.048    47\n## theta[95]     -0.757   0.769   -2.332   -1.284   -0.717   -0.189    0.637  1.151    18\n## theta[96]     -0.846   0.773   -2.453   -1.355   -0.816   -0.306    0.511  1.053    44\n##  [ reached getOption(\"max.print\") -- omitted 905 rows ]\n## \n## For each parameter, n.eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n## \n## DIC info (using the rule, pD = var(deviance)/2)\n## pD = 5736.9 and DIC = 9694.9\n## DIC is an estimate of expected predictive error (lower deviance is better).\nround(fit$BUGSoutput$summary[ !rownames(fit$BUGSoutput$summary) %like% \"theta\", ], 3)##              mean      sd     2.5%      25%      50%      75%    97.5%   Rhat n.eff\n## a[1]        0.577   0.198    0.285    0.436    0.544    0.678    1.045  1.652     6\n## a[2]        1.939   1.567    0.383    0.665    1.051    3.534    4.703  3.996     3\n## a[3]        1.568   0.660    0.710    0.929    1.355    2.234    2.572  2.317     4\n## a[4]        0.600   0.198    0.278    0.462    0.560    0.726    1.038  1.360    10\n## a[5]        0.847   0.608    0.090    0.359    0.575    1.440    1.965  2.599     4\n## c[1]        0.453   0.324    0.095    0.194    0.283    0.894    0.915  3.948     3\n## c[2]        0.348   0.165    0.115    0.214    0.297    0.539    0.607  3.008     4\n## c[3]        0.293   0.153    0.098    0.170    0.235    0.474    0.538  4.308     3\n## c[4]        0.402   0.245    0.103    0.195    0.294    0.718    0.752  3.600     4\n## c[5]        0.417   0.305    0.089    0.176    0.256    0.829    0.858  4.082     3\n## d[1]        0.562   1.927   -2.420   -2.019    1.843    1.959    2.144 20.801     3\n## d[2]       -0.353   1.203   -2.471   -1.766    0.360    0.525    0.760  9.579     3\n## d[3]       -1.043   1.054   -3.088   -2.063   -0.548   -0.197    0.058  6.905     3\n## d[4]       -0.230   1.388   -2.375   -2.069    0.648    0.824    1.000 15.453     3\n## d[5]        0.028   2.055   -3.286   -2.602    1.392    1.514    1.643 13.313     3\n## deviance 3957.959 550.269 3380.214 3542.102 3666.316 4645.067 4825.453  8.457     3\n# extract posteriors for all chains\njags.mcmc <- as.mcmc(fit)\n# the below two plots are too big to be useful given the 1000 observations.\n#R2jags::traceplot(jags.mcmc)\n\n# gelman-rubin-brook\n#gelman.plot(jags.mcmc)\n\n# convert to single data.frame for density plot\na <- colnames(as.data.frame(jags.mcmc[[1]]))\nplot.data <- data.frame(as.matrix(jags.mcmc, chains=T, iters = T))\ncolnames(plot.data) <- c(\"chain\", \"iter\", a)\n\n\nplot_title <- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"d[\",1:5,\"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"a[\", 1:5, \"]\")),\n  prob = 0.8) +\n  plot_title\nmcmc_areas(\n  plot.data,\n  pars = c(paste0(\"c[\", 1:5, \"]\")),\n  prob = 0.8) +\n  plot_title"},{"path":"item-response-theory.html","id":"final-notes","chapter":"11 Item Response Theory","heading":"11.10 Final Notes","text":"fully Bayesian approach psychometric modeling helps highlight major similarities factor analytic frameworks item response theory perspective.","code":""},{"path":"missing-data-modeling.html","id":"missing-data-modeling","chapter":"12 Missing Data Modeling","heading":"12 Missing Data Modeling","text":"-","code":""},{"path":"latent-class-analysis.html","id":"latent-class-analysis","chapter":"13 Latent Class Analysis","heading":"13 Latent Class Analysis","text":"Latent class analysis (LCA) takes different approach modeling latent variables discussed previous chapters (especially CFA IRT).\nmajor distinguishing aspect LCA latent variable hypothesized discrete random variable opposed continuous random variable.\nshift perspective beneficial researchers/analysts wishing think categorically discuss groups observations/people/units opposed dimension possible differences.LCA often used exploratory nature try identify number “latent classes” unobserved groups.\nHowever, text, Levy Mislevy discuss describe “confirmatory” approach LCA.\nmajor distinction specify number classes known, treat chapter resource estimate model within Bayesian framework.extend work, added section model selection provided resources interested reader.\n, general, topic model selection Bayesian LCA (Bayesian SEM generally) still ongoing area research.\nMany views exist conduct model selection () try present array options proposed.","code":""},{"path":"latent-class-analysis.html","id":"lca-model-specification","chapter":"13 Latent Class Analysis","heading":"13.1 LCA Model Specification","text":"LCA finite mixture model.\nmixture model generally statistical model combines distributions describe different subsets aspects data.\nexample, LCA mixture model posits class specific item/indicator response probabilities.\nDifferent classes different distribution expected response average.\nunit observation (e.g., person) assumed belong 1 1 class.\nindividuals can belong one class, different classes observations make different subsets data used analysis.\nOne result LCA model ability identify likelihood individual belongs particular class sometimes called probabilistic clustering.Let \\(x_{ij}\\) observed value respondent \\(=1, \\ldots, N\\) observable (item) \\(j=1, \\ldots, J\\).\n\\(x\\) binary, observed value can \\(0\\) \\(1\\).\nlatent class variable commonly denoted \\(C\\) represent total number latent classes.\nLet \\(\\theta_i\\) represent class individual \\(\\), \\(\\theta_i \\\\lbrace 1, \\ldots, C\\rbrace\\).\nSimilar IRT, LCA models probability observed response.\nHowever, LCA estimate value directly value depends latent class membership instead value indirectly estimated item difficulties discrimination parameters.\nmodel observed response 1 \n\\[p(x_{ij} = 1) = \\sum_{c=1}^Cp(x_{ij}=1\\mid\\theta_i=c,\\pi_j)\\gamma_c,\\]\n,\\(p(x_{ij}=1\\mid\\theta_i=c,\\pi_i)\\) class specific conditional response probability item \\(\\),\\(\\pi_j\\) represents item parameters, case simply conditional probability, can expanded set item parameters (e.g., IRT-like),\\(\\gamma_c\\) represents class mixing weight class size parameter. size class helps identify likely individual \\(C\\) classes information.Additionally, class mixing weights always sums \\(1\\), \\[\\sum_{c=1}^C\\gamma_c = \\sum_{c=1}^Cp(\\theta_i=c) = 1.\\]Another way interpreting mixing weight class proportion.\nframing can sometimes make easier see indicator response probability weighted average response probabilities \\(C\\) classes.\nlarger class class influences expected response probabilities.","code":""},{"path":"latent-class-analysis.html","id":"lca-model-indeterminacy","chapter":"13 Latent Class Analysis","heading":"13.1.1 LCA Model Indeterminacy","text":"One common aspect LCA can bit headache times carefully considered issue known label switching.\nLCA models discrete latent variables inherent labels, CFA/IRT model continuous latent variables inherent scale/metric.","code":""},{"path":"latent-class-analysis.html","id":"model-likelihood","chapter":"13 Latent Class Analysis","heading":"13.1.2 Model Likelihood","text":"likelihood function LCA follows similar development likelihood function CFA IRT.\nassume individuals independent (\\(\\)’s exchangeable).\nassume responses item conditionally (locally) independent (\\(j\\)’s exchangeable).\njoint probability conditional latent variable \\(C\\) \\[p(\\mathbf{x}_i \\vert \\mathbf{\\theta}, \\mathbf{\\pi}) = \\prod_{=1}^Np(\\mathbf{x}_i \\vert \\theta_i=c, \\mathbf{\\pi}) = \\prod_{=1}^N\\prod_{j=1}^Jp(x_{ij} \\vert \\theta_i=c, \\pi_j).\\]marginal probability observed data sum possible latent classes becomes\\[\\begin{align*}\np(\\mathbf{x}_i \\vert \\mathbf{\\pi}, \\mathbf{\\gamma}) &= \\prod_{=1}^N\\sum_{c=1}^C p(\\mathbf{x}_i \\vert \\theta_i=c, \\mathbf{\\pi})p(\\theta_i = c \\vert \\gamma)\\\\\n&= \\prod_{=1}^N\\left(\\sum_{c=1}^C \\left(\\prod_{j=1}^J p(x_{ij} \\vert \\theta_i=c, \\pi_j)\\right)p(\\theta_i = c \\vert \\gamma)\\right)\n\\end{align*}\n\\]","code":""},{"path":"latent-class-analysis.html","id":"bayesian-lca-model-specification","chapter":"13 Latent Class Analysis","heading":"13.2 Bayesian LCA Model Specification","text":"Bayesian formulation LCA model, construction carried pieces similar previous chapters.\nFigure 13.1: DAG general latent class analysis model\n","code":""},{"path":"latent-class-analysis.html","id":"distribution-of-observed-indicators","chapter":"13 Latent Class Analysis","heading":"13.2.1 Distribution of Observed Indicators","text":"First, distribution observed variables specified product independent probabilities.\n, observed data distribution categorical(.).\n\\[\\begin{align*}\np(\\mathbf{x}\\vert \\mathbf{\\theta}, \\mathbf{\\pi})&= \\prod_{=1}^N p(\\mathbf{x}_i\\vert \\theta_i, \\mathbf{\\pi}) = \\prod_{=1}^N \\prod_{j=1}^J p(x_{ij}\\vert \\theta_i, \\mathbf{\\pi}_j),\\\\\nx_{ij} &\\sim \\mathrm{Categorical}(\\pi_{cj})\\\\\n\\end{align*}\\]\nmodel holds observables taking values coded \\(1,...,K\\) categorical indicators.\nOne major changes IRT model latent variables categorical, resulting different distribution latent variable.\nDirichlet distribution commonly used prior categorical latent variables.\nDirichlet distribution generalization Beta distribution two district outcomes.\ndistribution models categorical data likelihood propensity one categoricals.","code":""},{"path":"latent-class-analysis.html","id":"prior-distributions","chapter":"13 Latent Class Analysis","heading":"13.2.2 Prior Distributions","text":"LCA model, described , contains two major types parameters.\n, (1) latent class status respondent (2) class specific category proportions.\ntwo parameter types vectors respondents items, respectively.\njoint prior distribution can generally described \n\\[p(\\theta, \\pi) = p(\\theta)p(\\pi),\\]\ndue assuming independence two types parameters.\nIndependence logical assumption aligns IRT CFA traditions latent variable values independent measurement model parameters.prior latent variables represents prior discrete groups respondents assumed belong .\ncommon prior placed respondents\n\\[p(\\theta)=\\prod_{=1}^np(\\theta_i\\vert\\mathbf{\\theta}_p),\\]\n\\(\\mathbf\\theta_p\\) represents hyperpriors defining conditions categorical latent variables.\nvector parameters \\(\\mathbf\\theta_p = \\mathbf\\gamma = (\\gamma_1, \\gamma_2, ..., \\gamma_C)\\), \\(C\\) number latent groups.\nStated another way,\n\\[\\theta_i | \\mathbf\\gamma \\sim \\mathrm{Categorical}(\\mathbf\\gamma).\\]hyperprior categorical latent variable also commonly given prior distribution.\n\\(\\mathbf\\gamma\\) parameters represent class proportions prior parameters useful class proportions known.\nprior \n\\[\\mathbf\\gamma \\sim \\mathrm{Dirichlet}(\\mathbf\\alpha_\\gamma),\\]\n\\(\\mathbf\\alpha_\\gamma = (\\alpha_{\\gamma 1}, \\alpha_{\\gamma 2}, ..., \\alpha_{\\gamma C}).\\)\nDirichlet distribution generalization Beta distribution two categories.\nallows useful representation probabilities well known statistical sampling properties.priors measurement model parameters (\\(\\mathbf\\pi\\)) commonly defined item level instead jointly items:\n\\[p(\\mathbf\\pi) = \\prod_{j=1}^Jp(\\mathbf\\pi_j\\vert\\mathbf\\alpha_\\pi),\\]\n\\(\\mathbf\\alpha_\\pi\\) defines hyperpriors measurement model parameters.\nmeasurement model commonly utilizes either dichotomous categorical indicators leads use either Beta distribution Dirichlet distribution priors.","code":""},{"path":"latent-class-analysis.html","id":"full-model-specification","chapter":"13 Latent Class Analysis","heading":"13.2.3 Full Model Specification","text":"full model specification can shown follows:\\[\n\\begin{align*}\np(\\mathbf\\theta, \\mathbf\\gamma, \\mathbf\\pi) &\\propto p(\\mathbf{x} \\vert \\mathbf\\theta, \\mathbf\\gamma, \\mathbf\\pi) p(\\mathbf\\theta, \\mathbf\\gamma, \\mathbf\\pi)\\\\\n&= p(\\mathbf{x} \\vert \\mathbf\\theta, \\mathbf\\gamma, \\mathbf\\pi) p(\\mathbf\\theta) p(\\mathbf\\gamma) p(\\mathbf\\pi)\\\\\n&= \\prod_{=1}^N \\prod_{j=1}^J p(x_{ij}\\vert \\theta_i, \\mathbf{\\pi}_j)p(\\theta_i \\vert \\mathbf\\gamma)p(\\mathbf\\gamma) \\prod_{c=1}^Cp(\\mathbf\\pi_{cj})\\\\\n(x_{ij} \\vert \\theta_i=c, \\mathbf{\\pi}_j)\n\\end{align*}\n\\]","code":""},{"path":"latent-class-analysis.html","id":"extending-lca","chapter":"13 Latent Class Analysis","heading":"13.3 Extending LCA","text":"LCA, mixing distributions can generalized include complex components factor models model response process within class.\nexample, can factor mixture models different CFA models hold different subsets population.","code":""},{"path":"bayesian-networks.html","id":"bayesian-networks","chapter":"14 Bayesian Networks","heading":"14 Bayesian Networks","text":"-","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
